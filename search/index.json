[{"content":"NeurIPS, \u0026lsquo;22 https://arxiv.org/abs/2207.06010\nSummary Self-supervised pre-training alone does not provide statistically significant improvements over non-pre-trained methods on downstream tasks. Data splits, hand-crafted rich features, or hyperparameters can bring significant improvements. Preliminaries Pre-train objectives\nSelf-supervised Node prediction Context prediction Motif prediction Contrastive learning Supervised Related tasks with label (e.g. ChEMBL dataset) Graph features\nBasic Feature set used in Hu et al.\nRich\nFeature set used in Rong et al. This is the superset of basic features. In downstream tasks, additional 2d normalized rdNormalizedDescriptors are used (not in pre-training).\nDownstream splits\nScaffold\nSorts the molecule according to the scaffold, then partition the sorted list into train/valid/test splits. → Deterministic\nMolecules of each set are most different ones.\nBalanced scaffold\nIntroduces the randomness in the sorting and splitting stages of Scaffold split.\nGNN architecture\nGIN GraphSAGE Pre-train dataset\nZINC15 (self-supervised)\n2 million molecules. Pre-processed following Hu et al.\nSAVI (self-supervised)\n1 billion drug-like molecules synthesized by computer simulated reactions.\nChEMBL (supervised)\n500k drugable molecules with 1,310 prediction target labels from bio-activity assays.\nResults Key Takeaways When pre-training might help? Related supervised pre-training dataset. But not always feasible. If the rich features are absent. If the downstream split distributions are substantially different. When the gain dimishes? If using rich features. If don’t have the highly relevant supervisions. If the downstream split is balanced. If the self-supervised learning dataset lacks diversity. Why pre-training may not help in some cases? Some of the pre-training methods (e.g. node label prediction) might be too easy\n→ Transfer less knowledge.\nSo…\nUse rich features Use balanced scaffold Use related supervised pre-training dataset Use difficult pre-training task (for self-supervised pre-training) and use high-quality negative samples. ","date":"2022-11-22T00:00:00Z","image":"https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/0_hube49b7519c97672414e9b0ddbe5479a4_229014_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/","title":"Does GNN Pre-training Help Molecular Representation?"},{"content":"NeurIPS, \u0026lsquo;22,\nhttps://openreview.net/forum?id=SgZ-glWWUlq\nSummary Authors proposed a framework called D-SLA that aims to learn the exact discrepancy between the original and the perturbed graphs. Three major components Learn to distinguish whether each graph is the original graph or the perturbed one. Capture the amount of discrepancy for each perturbed graph (using edit distance) Learn relative discrepancy with other graphs Preliminaries Graph Neural Networks (GNN) Aggregate the features from its neighbors Combining the aggregated message Variants of Update \u0026amp; Aggregate functions\nGraph Convolution Network (GCN)\nGeneral convolution operation + Mean aggregation\nGraphSAGE\nConcatenate representations of neighbors with its own representation when updating\nGraph Attention Network (GAT)\nConsiders the relative importance among neighboring nodes when aggregation\nGraph Isomorphism Network (GIN)\nSum aggregation\nSelf-supervised learning for graphs (GSL) Aims to learn a good representation of the graphs in an unsupervised manner.\n→ Transfer this knowledge to downstream tasks.\nMost prevalent framework for GSL\nPredictive learning (PL)\nAims to learn contextual relationships by predicting sub-graphical features (nodes, edges, subgraphs)\npredict the attributes of masked nodes predict the presence of an edge or a path predict the generative sequence, contextual property, and motifs But predictive learning may not capture the global structures and/or semantics of graphs.\nContrastive learning (CL)\nAims to capture global level information.\nEarly CL learn the similarity between the entire graph and its substructure. Others include attribute masking, edge perturbation, and subgraph sampling. Recent CL adversarial methods generate positive examples either by adaptively removing the edges or by adjusting the attributes. But CL may not distinguish two topologically similar graphs yet having completely different properties.\nMinimize $\\mathcal{L}_{CL} = - \\log \\frac{f_{\\text{sim}} (h_{\\mathcal{G}_i}, h_{\\mathcal{G}_j})}{\\sum_{\\mathcal{G}\u0026rsquo;, \\mathcal{G\u0026rsquo; \\neq \\mathcal{G}_0}}f_{\\text{sim}}(h_{\\mathcal{G}_i}, h_{\\mathcal{G}\u0026rsquo;})}$\n$\\mathcal{G}_0$: original graph $\\mathcal{G}_i, \\mathcal{G}_j$: perturbed graphs $\\mathcal{G}\u0026rsquo;$: other graph in the same batch with the $\\mathcal{G}_0$, a.k.a. negative graph positive pair: $(\\mathcal{G}_i, \\mathcal{G}_j)$; negative pair: $(\\mathcal{G}_i, \\mathcal{G}\u0026rsquo;)$ $f_\\text{sim}$: similarity function between two graphs → $L_2$ distance or cosine similarity → similarity of positive pair $\\uparrow$, similarity of negative pair $\\downarrow$\nDiscrepancy Learning Discriminate original vs perturbed\nPerturbed graph could be semantically incorrect!\n→ Embed perturbed graph apart from original.\n$\\mathcal{L}_{GD} = - \\log \\Big (\\frac{e^{S_0}}{e^{S_0} + \\sum_{i \\geq 1}e^{S_i}} \\Big ) \\text{ with } S = f_S(h_{\\mathcal{G}})$\nIntuitively,\nlarge value of $e^{S_0}$ for the original graph small value of $e^{S_i}$ for the perturbed graphs How to perturb?\nAim at perturbed graph to be semantically incorrect\nRemove or add a small number of edges\nManipulate the edge set by removing existing edges + adding new edges on $\\mathcal{X}_\\mathcal{E}$\nMask node attributes\nRandomly mask the node attributes on $\\mathcal{X}_\\mathcal{V}$ for both original and perturbed graphs\n(to make it more difficult to distinguish between them)\n$\\mathcal{G}_0 = (\\mathcal{V}, \\mathcal{E}, \\tilde{\\mathcal{X}^0_{\\mathcal{V}}}, \\mathcal{X}_{\\mathcal{E}}), \\tilde{\\mathcal{X}^0_{\\mathcal{V}}} \\sim \\texttt{M}(\\mathcal{G})$\n$\\mathcal{G}_i = (\\mathcal{V}, \\mathcal{E}^i, \\tilde{\\mathcal{X}^i_{\\mathcal{V}}}, \\mathcal{X}^i_{\\mathcal{E}}), \\tilde{\\mathcal{X}^i_{\\mathcal{V}}} \\sim \\texttt{M}(\\mathcal{G}), (\\mathcal{E}^i, \\mathcal{X}^i_{\\mathcal{E}}) \\sim \\texttt{P}(\\mathcal{G})$\nPersonal opinion\nThe real usage of discriminator loss will be to push original \u0026amp; perturbed graph apart, while applying edit distance loss. Discrepancy with Edit distance\nHow dissimilar?\nUsually, we need to measure the graph distance, such as edit distance.\nEdit distance: number of insertion, deletion, and substitution operations for nodes \u0026amp; edges to transform one graph from another. → NP hard!\nBut we know the exact number of perturbations for each graphs\n→ use it as distance.\n$\\mathcal{L}_{edit} = \\sum_{i, j} \\Big ( \\frac{d_i}{e_i} - \\frac{d_j}{e_j}\\Big )^2 \\text{ with } d_i = f_{\\text{diff}}(h_{\\mathcal{G}_0}, h_{\\mathcal{G}_i})$\n$f_{\\text{diff}}$ measures the embedding level differences between graphs with L2 norm.\n$e_i$: edit distance (number of perturbations)\nThe trivial solution for the edit distance loss is $d_i = d_j = 0$. But because of the discriminator loss, this is not possible.\nRelative discrepancy learning with other graphs\nAssumption:\nDistance between original and negative graphs in the same batch is larger than the distance between the original and perturbed graphs with some amount of margin.\nFormally,\n$\\mathcal{L}_{margin} = \\sum_{i, j} \\max (0, \\alpha + d_i - d\u0026rsquo;_j)$\n$d_i$: distance between original and its perturbed graphs\n$d\u0026rsquo;_j$: distance between original and negative graphs\nIntuitively, $\\alpha + d_i \u0026lt; d\u0026rsquo;_j$ !\nOverall loss $\\mathcal{L} = \\mathcal{L}_{GD} + \\lambda_1 \\mathcal{L}_{edit} + \\lambda_2 \\mathcal{L}_{margin}$\nResults ","date":"2022-09-21T00:00:00Z","image":"https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/0_hu1da554d6c291b842a8ea287e9b40bdf6_233064_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/","title":"Graph Self-supervised Learning with Accurate Discrepancy Learning"},{"content":"NeurIPS Poster, \u0026lsquo;21,\nhttps://arxiv.org/abs/2112.04624\nSummary Used physicist network (PhysNet) and chemist network (ChemNet) simultaneously, and each network shares information to solve individual tasks. PhysNet: Neural physical engine. Mimics molecular dynamics to predict conformation. ChemNet: Message passing network for chemical \u0026amp; biomedical property prediction. Molecule without 3D conformation can be inferred during test time. Preliminaries Molecular representation learning:\nEmbedding molecules into latent space for downstream tasks.\nNeural Physical Engines\nNeural networks are capable of learning annotated potentials and forces in particle systems.\nHamNet proposed a neural physical engine that operated on a generalized space, where positions and momentums of atoms were defined as high-dimensional vectors.\nMulti-task learning\nSharing representations for different but related tasks.\nModel fusion\nMerging different models on identical tasks to improve performance.\nNotation Graph $\\mathcal{M} = (\\mathcal{V}, \\mathcal{E}, n, m, \\mathbf{X}^v, \\mathbf{X}^e)$\n$\\mathcal{V}$: set of $n$ atoms $\\mathcal{E}$: set of $m$ chemical bonds $\\mathbf{X}^v \\in \\mathbb{R}^{n \\times d_v} = (x^v_1, \u0026hellip;, x^v_n)^\\top$: matrix of atomic features $\\mathbf{X}^e \\in \\mathbb{R}^{m \\times d_e} = (x^e_1, \u0026hellip;, x^e_m)^\\top$: matrix of bond features Model Figure 1. PhysChem Architecture\nInitializer\nInput: atomic features, bond features (from RDKit) Layer: fully connected layers Output: bond states, atom states for ChemNet\n$v^{(0)}_i = \\text{FC}(x^v_i), i\\in \\mathcal{V}$\n$e^{(0)}_{i,j} = \\text{FC}(x^e_{i,j}), (i, j)\\in \\mathcal{E}$ atom positions, atomic momenta for PhysNet\nBond strength adjacency matrix\n$$A(i,j)=\\begin{cases}0, \u0026amp; \\text{if $(i,j) \\notin \\mathcal{E}$} \\\\ \\text{FC}_{\\text{sigmoid}}(x^e_{i,j}), \u0026amp; \\text{if $(i,j) \\in \\mathcal{E}$} \\end{cases}$$ $\\tilde{V} = \\text{GCN}(A, V^{(0)})$\n${ (q^{(0)}_i \\oplus p^{(0)}_i)} = \\text{LSTM}({\\tilde{v}_i}), i \\in \\mathcal{V}$ PhysNet\nPhysNet is inspired by HamNet.\nHamNet showed that neural networks can simulate molecular dynamics for conformation prediction. Directly parameterize the forces between each pair of atoms. Consider the effects of chemical interactions(e.g. bond types) by cooperating with ChemNet’s bond states. Introduces torsion forces. Output: 3D conformation ChemNet\nChemNet modifies MPNN(message passing neural network) for molecular representation learning. Output: Molecule representation Loss $L_{\\text{phys}}$: Conn-k loss for Conformation prediction (PhysNet)\n$k$-hop connectivity loss\n$L_{\\text{Conn}-k}(\\hat{\\mathbf{R}}, \\mathbf{R}) = |\\frac{1}{n} \\hat{\\mathbf{C}}^{(k)} \\odot (\\hat{\\mathbf{D}} - \\mathbf{D}) \\odot (\\hat{\\mathbf{D}} - \\mathbf{D}) |_{F}$\n$\\odot$: element-wise product\n$| \\cdot |$: Frobenius norm\n$(\\hat{\\mathbf{D}} - \\mathbf{D})$ : distance matrix of the real and predicted conformations $(\\hat{\\mathbf{R}} - \\mathbf{R})$\n$\\hat{\\mathbf{C}}^{(k)}$: normalized $k$-hop connectivity matrix\n$L_{\\text{chem}}$: MAE or Cross entropy loss for Property prediction (ChemNet)\nTotal loss\n$L_{\\text{total}} = \\lambda L_{\\text{phys}} + L_{\\text{chem}}$\nCheckpoints\nIs Conn-k loss generally used in other conformation prediction models?\nNo! But seems related to local distance loss.\nIs triplet descriptor generally used in other models?\nNo!\n","date":"2022-07-29T00:00:00Z","image":"https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/physchem_1_hu614a3ffe516a8cd557a8bbd359513341_952945_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/","title":"PhysChem: Deep Molecular Representation Learning via Fusing Physical and Chemical Information"},{"content":"Oncogene, \u0026lsquo;20\nhttps://www.nature.com/articles/s41388-020-1316-2\nSummary In-silico approach based on CMap to identify drug candidates for lung cancer metastasis Revealed the underlying mechanisms of undruggable target (GALNT14) and targeted the downstream transcription factor Repositioned drug: BTZ (Bortezomib) Integrated multiple independent expression signatures from cancer patients (TCGA), genetic perturbations(knock-down or overexpression), and drug treatment (CMap) Background GALNT14: a putative driver of lung cancer metastasis, leading to poor survival \u0026amp; has poor druggability. Bortezomib: drug used for multiple myeloma and mantle cell lymphoma CMap: a collection of genome wide expression profiles of cell lines treated with \u0026gt; 20,000 chemicals Main Results Figure 1. GALNT14 as a putative molecular target for lung cancer metastasis.\n1a. TCGA Lung adenocarcinoma cohort의 516명 lung cancer 환자의 transcriptome data. 1b. relapse-free survival / DEG 분석에서 7개의 gene들이 검출되었고, 그 7개의 gene의 expression이 높은 group, 낮은 group으로 분류. 1c. metastasis와 tumor signature가 high-expression group에서 enrich 되었음. 1d. GALNT14만 단독으로 보아도 metastasis와 tumor 에서 enrich 되어 있음을 알 수 있음. 1e, 1f. GALNT14이 각각 metastatic potential과 tumorigenic potential이 있다는 in vivo 실험 결과. 1g. Metastatic lung cancer cell이 non-metastatic cancer보다 GALNT14 depletion에 더 vulnerable. 1h. GALNT14이 survival에 분명한 negative correlation을 보임.\n이것으로 미루어보아, GALNT14이 lung cancer metastasis의 promising molecular target이라는 것을 알 수 있음. Figure 5. In vivo validation of the anti-metastatic effect of BTZ. BTZ의 anti-migration, anti-invasion effect를 in vitro level에서 확인한 뒤 in vivo에서 cancer metastasis efficacy를 확인한 실험 결과\n5a. 쥐의 꼬리 정맥으로 H460 lung cancer cell을 주입하여 local metastasis를 유도하고 control 군, BTZ 처리군, CFZ 처리군으로 구분하였음. 5b. BTZ, CFZ의 proteasome inhibition을 확인하기 위해 혈액에서 proteasome activity를 측정한 결과. 상당히 줄어들었음을 알 수 있음. P.C. 는 positive control 5c. Body weight 정보. 항암제 처리로 인해 다른 조직 등에 dramatic한 영향은 없었음. 5d. Lung cancer로 metastasis 유무 사진. Vehicle과 CFZ는 상당부분 Metastasis가 일어난 것을 볼 수 있지만 BTZ는 6개 중 1개만 미약하게 metastasis 발생. 5e. H\u0026amp;E staining 후의 lung image 5f. tumor nodule size의 average. BTZ는 매우 작음. 5g. tumor nodule의 수 분포. BTZ 매우 적음. 5h. proteasome activity 차이 Discussion Unlike other studies that used CMap, they focused exclusively on a target gene related to a pertinent phenotype and identified BTZ as a drug candidate with novel anti-metastatic effects. In pathway level, the most enriched pathway was TGF￼ signaling, and they also identified the GALNT14-TGF￼ signature, which has invasive properties that are attenuated by BTZ. They integrated multiple independent expression signatures from cancer patients(TCGA), genetic perturbations(knock-down or overexpression), and drug treatment(CMap). ","date":"2022-07-22T00:00:00Z","image":"https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_0_hub7b0114b78cd33a49a5f6e1bdf51e130_146004_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/","title":"CMap based Drug Repositioning of BTZ to reverse the metastatic effect of GALNT14 in lung cancer"}]