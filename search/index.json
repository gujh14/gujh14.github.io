[{"content":"M2D2 m2d2.io\nM2D2 is an abbreviation for Molecular Modeling \u0026amp; Drug Discovery. This is a community is co-organized by Valence Discovery and Mila - Quebec AI Institute. New presentation comes up every Tuesday via Zoom. Authors of recent papers introduce their work themselves and answers questions. Most of the sessions are recorded and uploaded on Youtube. Most papers have been accepted at top conferences. Highly recommend to watch those videos. Hugging Face huggingface.co/blog/intro-graphml\nThis blog article is a tutorial about machine learning techniques for Graphs. Blog is not tailor-made for molecules, but substantially introduced. ","date":"2023-03-01T00:00:00Z","permalink":"https://gujh14.github.io/p/useful-sources-for-molecule-learning-in-progress/","title":"Useful Sources for Molecule Learning (in Progress)"},{"content":"ICLR, \u0026lsquo;23\nDiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking\nThis article is one of the first research work that formulated molecular docking as a generative problem.\nShowed very interesting results with decent performance gain.\nIf you are interested in molecular docking and diffusion models, this is definitely a must-read paper!\nIt is highly recommended to watch youtube video explained by the authors.\nSummary Molecular docking as a generative problem, not regression!\nProblem of learning a distribution over ligand poses conditioned on the target protein structure $p(\\mathbf{x} | \\mathbf{y})$ Used “Diffusion process” for generation\nTwo separate model\nScore model: $s(\\mathbf{x}, \\mathbf{y}, t)$\nPredicts score based on ligand pose $\\mathbf{x}$, protein structure $\\mathbf{y}$, and timestep $t$\nConfidence model: $d(\\mathbf{x}, \\mathbf{y})$\nPredicts whether the ligand pose has RMSD below 2Å compared to ground truth ligand pose\nDiffusion on Product space $\\mathbb{P}$\nReduced degrees of freedom $3n \\rightarrow (m+6)$ Preliminaries Molecular Docking Definition:\nPredicting the position, orientation, and conformation of a ligand when bound to a target protein\nTwo types of tasks\nKnown-pocket docking Given: position of the binding pocket Blind docking More general setting: no prior knowledge about binding pocket Previous works: Search-based / Regression-based Search based docking methods\nTraditional methods\nConsist of parameterized physics-based scoring function and a search algorithm\nScoring function\nInput: 3D structures Output: estimate of the quality/likelihood of the given pose Search algorithm\nStochastically modifies the ligand pose (position, orientation, torsion angles) Goal: finding the global optimum of the scoring function. ML has been applied to parameterize the scoring function.\nBut very computationally expensive (large search space) Example\nRegression based methods\nRecent deep learning method\nSignificant speedup compared to search based methods\nNo improvements in accuracy\nExample\nEquiBind\nTried to tackle the blind docking task as a regression problem by directly predicting pocket keypoints on both ligand and protein and aligning them. TANKBind\nImproved over this by independently predicting a docking pose for each possible pocket and then ranking them. E3Bind\nUsed ligand-constrained \u0026amp; protein-constrained update layer to embed ligand atoms and iteratively updated coordinates. Docking objective Standard evaluation metric: $\\mathcal{L}_{\\epsilon} = \\sum_{x, y} I_{\\text{RMSD}(y, \\hat{y}(x))\u0026lt;\\epsilon}$:\nproportion of predictions with $\\text{RMSD} \u0026lt; \\epsilon$ → Not differentiable!\nInstead, we use $\\text{argmin}_{\\hat{y}} \\lim_{\\epsilon \\rightarrow 0} \\mathcal{L}_\\epsilon$ as objective function.\nRegression is suitable for docking only if it is unimodal. Docking has significant aleatoric (irreducible) \u0026amp; epistemic (reducible) uncertainty Regression methods will minimize $\\sum \\|y - \\hat{y}\\|^2_2$ → will produce weighted mean of multiple modes On the other hand, generative model will populate all/most modes! Regression (EquiBind) model set conformer in the middle of the modes. Generative samples can populate conformer in most modes. Much less steric clashes for generative models DiffDock Overview Two-step approach Score model: Reverse diffusion over translation, rotation, and torsion Confidence model: Predict whether or not each ligand pose is $\\text{RMSD} \u0026lt; 2\\text{Å}$ compared to ground truth ligand pose Score model Ligand pose: $\\mathbb{R}^{3n}$ ($n$: number of atoms)\nBut molecular docking needs far less degrees of freedom.\nReduced degree of freedom: $(m+6)$\nLocal structure: Fixed (rigid) after conformer generation with RDKit EmbedMolecule(mol)\nBond length, angles, small rings Position (translation): $\\mathbb{R}^3$ - 3D vector\nOrientation (rotation): $SO(3)$ - three Euler angle vector\nTorsion angles: $\\mathbb{T}^m$ ($m$: number of rotatable bonds)\nCan perform diffusion on product space $\\mathbb{P}: \\mathbb{R}^3 \\times SO(3) \\times \\mathbb{T}^m$\nFor a given seed conformation $\\mathbf{c}$, the map $A(\\cdot, \\mathbf{c}): \\mathbb{P} \\rightarrow \\mathcal{M}_\\mathbf{c}$ is a bijection! Confidence Model Generative model can sample an arbitrary number of poses, but researchers are interested in one or a fixed number of them. Confidence predictions are very useful for downstream tasks. Confidence model $d(\\mathbf{x}, \\mathbf{y})$ $\\mathbf{x}$: pose of a ligand $\\mathbf{y}$: target protein structure Samples are ranked by score and the score of the best is used as overall confidence score. Training \u0026amp; Inference Ran the trained diffusion model to obtain a set of candidate poses for every training example and generate binary labels: each pose has RMSD below $2 \\text{Å}$ or not. Then the confidence model is trained with cross entropy loss to predict the binary label for each pose. During inference, diffusion model is run to generate $N$ poses in parallel, and passed to the confidence model that ranks them based on its confidence that they have RMSD below $2\\text{Å}$. DiffDock Workflow DiffDock Results Standard benchmark: PDBBind\n19k experimentally determined structures of small molecules + proteins\nBaselines: search-based \u0026amp; deep learning\nPrediction correctness\nOutperform search-based, deep learning, and pocket prediction + search-based methods\nRuntime\n3 times faster than the most accurate baseline\nGeneralization to unseen receptors\nAble to generalize: outperform classical method\nReverse diffusion process GIF\nConfidence score quality\nHigh selective accuracy: valuable information for practitioners\nPersonal opinions It is impressive that the authors formulated molecular docking as a generative problem, conditioned on protein structure. But it is not an end-to-end approach. And there are some discrepancy between the inputs and output of the confidence model. The input is the predicted ligand pose $\\hat{\\mathbf{x}}$ and protein structure $\\mathbf{y}$, but the output is “whether the RMSE is below 2Å between predicted ligand pose $\\hat{\\mathbf{x}}$ and ground truth ligand pose $\\mathbf{x}$”. There are quite a room to improve the performance, but it requires heavy workloads of GPUs. I’m skeptical about the generalizability of this model since there are almost no physics informed inductive bias in the model. Reference Article\nDiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking\nYoutube\nDiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking\nBlog\nGenerative Modeling by Estimating Gradients of the Data Distribution | Yang Song\nWhat are Diffusion Models?\n","date":"2023-02-27T00:00:00Z","image":"https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail_hu35359f9550b3436da89f85bd47cc7f53_260405_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/","title":"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking"},{"content":"LoG conference, \u0026lsquo;22,\n\u0026ldquo;LoG conference Day 4 Youtube\u0026rdquo;\nLecturer: Djork-Arné Clevert 2022.05.19~09.22: Bayer Pharma, Head of Machine Learning Research (All work presented today were published during Bayer.) 2022.10.22~: Pfizer, Head of Machine Learning Research Summary Most ML methods are focused on early drug discovery part. Major applications of ML in drug discovery include: ADMET modeling Representation learning Conditional de novo hit design Inverse molecule modeling Drug Discovery vs Drug Development Drug discovery: the early part (Hit identification ~ Pre-clinical phase) Drug development: the later part (Clinical trials phase 1 ~ Phase 4) Most ML research is focused on the Drug discovery part, since there is a larger quantity of data available that is more convenient to input into a computer. Background bioactivity / ADMET modeling Bioactivity modeling have been used since the 1960s.\n[Research article] Modeling Physico-Chemical ADMET Endpoints with Multitask Graph Convolutional Networks (Molecules, 2019)\nMultitask GCN for modeling physico-chemical properties. Performed a molecular property prediction with GCN in multi-task setting. This is an early work in this field using GNNs to predict properties. [Research article] Improving Molecular GCNs Explainability with Orthonormality and Sparsity (ICML, 2021)\nProposed two regularization techniques to improve the accuracy and explainability.\nBatch Representation Orthonormalization (BRO)\n→ encourages graph convolution operations to generate orthonormal node embeddings.\nGini regularization\n→ applied to the weights of the output layer and constrains the number of dimensions the model can use to make predictions.\nExplainability results\n[Research article] Representation Learning on Biomolecular Structures using Equivariant Graph Attention (LoG, 2022)\nLet’s not focus only on invariant feature, but on equivariant feature.\nEQGAT operates with Cartesian coordinates to incorporate directionality and is implemented with a novel attention mechanism, acting as a content and spatial dependent filter when propagating information between nodes.\nPerformed well on large biomolecule dataset (ATOM3D), and it is efficient.\nThe Diversity of Data in Drug Discovery There are many types of data in molecule domain, including various spectrometry data, graph, sequence, image, 3D point clouds, … Molecular Representations for Drug Discovery [Research article] Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations (Chemical Science, 2019)\nTo learn molecule representation, authors made a autoencoder model that translates any SMILES into canonical SMILES. It was good for predicting downstream tasks.\nPerformance results\n[Research article] Efficient multi-objective molecular optimization in a continuous latent space (Chemical Science, 2019)\nThis method takes a starting compound as input and proposes new molecules with more desirable (predicted) properties. Objective function combines multiple in silico prediction models, defined desirability ranges and substructure constraints. Conditional Molecular de novo Hit Design [Research article] De novo generation of hit-like molecules from gene expression signatures using artificial intelligence (Nature Communications, 2020)\nA generative model (GAN) that bridges systems biology and molecular design, conditioning a generative adversarial network with transcriptomic data. When the model is provided with the desired state of gene expression signature, it is able to design active-like molecules for desired targets without any previous target annotation. [Research article] Cell morphology-guided de novo hit design by conditioning GANs on phenotypic image features (Digital Discovery, 2023)\nUtilized cellular morphology (cell painting morphological profiles) to directly guide the de novo design of small molecules. Authors used conditional GAN as a generative model. Inverse Molecular problems Is it possible to inverse fingerprint, molecular depiction, or resonance spectrum into a molecule structure?\n[Research article] Neuraldecipher – reverse-engineering extended-connectivity fingerprints (ECFPs) to their molecular structures (Chemical Science, 2020)\nSince ECFP representation is made with hash function, they are often non-invertible. Neuraldecipher is a neural net model that predicts a compact vector representation of compounds, given ECFPs. Then utilize another pre-trained model to retrieve the molecular structure as SMILES representation. This model were able to correctly deduce up to 69% of molecular structures. [Research article] Img2Mol – accurate SMILES recognition from molecular graphical depictions (Chemical Science, 2021)\nThis model use CNN for molecule depictions and a pre-trained decoder that translates the latent representation into the SMILES representation of the molecules. Img2Mol was able to correctly translate up to 88% of the molecular depictions into SMILES. Learning Graph level representation [Research article] Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning (NeurIPS, 2021, Poster)\nGraph representation is highly complexed, which can be represented by $(\\#\\text{ nodes})!$ equivalent adjacency matrices. This model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. Showed promising results in graph classification, generation, clustering, interpolation. Personal Opinion It was interesting to see what kinds of research is being performed in big pharmas. Big pharmas seem to be more interested in applying ML methods on small problems than creating state-of-the-arts techniques. Inverse molecular modeling seems interesting for me. References https://www.youtube.com/live/ouuxi5uW-JA?feature=share\u0026t=8477 ","date":"2023-02-04T00:00:00Z","image":"https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/thumbnail_hu0d4bc15913e42a594fe5b478b080476a_169216_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/","title":"Graph Representation Learning for Drug Discovery"},{"content":"LoG, \u0026lsquo;22\n\u0026ldquo;Neighborhood-aware Scalable Temporal Network Representation Learning\u0026rdquo;\nSummary There are few models to infer network-scientific properties on time-varying temporal network. Traditional GNNs cannot deal with structural features, and most of the existing works used distance between nodes on static networks. Authors proposed dictionary-type node representation and neighborhood cache as a scalable way to represent temporal networks, and achieved SOTA performance on many link prediction tasks. Temporal Network Temporal network is an abstraction of complex interactive systems.\nNetwork structures evolve over time.\nExamples: User-item network Social media network Email network Engineering control networks Mobility networks Goal:\nPredicting how temporal networks evolve → link prediction in temporal networks\nApplication:\nrecommendation, anomaly detection, …\nTemporal networks in network science Network science: how network structures evolves reflects the fundamental laws of these complex systems Triadic closure in social network: people with common friends tend to know each other. Feed-forward control in biological/engineering control systems: positive stimuli are followed with negative stimuli. Issues of Previous Approaches (Effectiveness) GNNs cannot capture structural features that involve multiple nodes of interest, such as triadic closure.\nAt time step $t_3$, it is hard for traditional GNN to distinguish nodes $w$ and $v$, since their computation graphs are same.\n→ Temporal network representation learning, if following traditional GNN-type computation, will fail to learn such information.\nWorks over static graph\nThere were some works to deal with this issues on static graphs.\nSEAL (Zhang et al. 2018) Distance encoding (Li et al. 2020) Labeling trick (Zhang et al. 2021) SUREL (Yin et al. 2022) ELPH (Chamberlain et al. 2022) Most of the idea is to build structural feature (usually shortest path distance) and use it as extra feature.\nHow can we apply this idea to temporal networks, in an efficient and scalable way?\nRecent work over temporal networks\nCAWN (Wang et al. 2021): but high computation overhead.\nFor each queried node pair, random walks need to be sampled. The relative positional encoding needs to be computed online. Neighborhood Aware Temporal Network (NAT) Key ideas\nDictionary-type node representations Constructs structural feature efficiently Avoids online neighbor sampling Neighborhood Caches (N-caches) Maintains dictionary representations with parallel hashing scalably Dictionary representations Do not use long-vector representations\nEach node u is represented as a dictionary\nKeys: Down-sampled neighbors in 0-hop (self), 1-hop, 2-hop, … Values: Short vector representations (2~8 dim) representation for node $a$ as a neighbor of node $u$ → Captures joint neighborhood structural features between $u$ and $v$\n→ NAT combines structural feature construction and traditional vector representations\nNAT architecture Architecture Experiments Performed on both inductive \u0026amp; transductive setting.\nPerformance Computation \u0026amp; Scalability Takeaways Structural features from a joint neighborhood of multiple nodes are crucial to predict temporal network evolution Dictionary-type representations can combine structural feature construction with traditional vector representations. Dictionary-type representations allow online construction of such structural features in an efficient way. Reference Neighborhood-aware Scalable Temporal Network Representation Learning\nLearning on Graphs Conference 2022 - Day 1 Livestream\n","date":"2023-01-15T00:00:00Z","image":"https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail_hu242d3a243b2ed8387d2472cbaa72b75a_149908_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/","title":"Neighborhood-aware Scalable Temporal Network Representation Learning"},{"content":"LoG, 22 \u0026ldquo;A Generalist Neural Algorithmic Learner\u0026rdquo;\nSummary Neural network, especially GNN, can learn traditional computer science algorithms in CLRS book. A generalist neural algorithmic learner is necessary if the algorithm is not obvious. Chunking mechanism was important to stabilize multi-task learning. Starting point: A benchmark to train neural computer scientists The CLRS Algorithmic Reasoning Benchmark\nhttps://github.com/deepmind/clrs\nCan we train a neural network to execute classical CS algorithms? A differentiable computer scientist could then apply its \u0026ldquo;knowledge\u0026rdquo; to natural inputs. We will also ponder: can it learn multiple algorithms at the same time? Typically the problem is modeled with a recurrent architecture: LSTMs as in, e.g., Differentiable Neural Computers Transformers, as in the Universal Transformer ConvNets GNNs - our approach Introduction to Algorithms: CLRS\nRepresentation\nAll algorithms have been boiled down to a common graph representation\nEach algorithm is specified by a fixed number of \u0026ldquo;probes\u0026rdquo;.\nFor example, the spec of insertion sort consists of the following 6 probes:\n'pos': (Stage. INPUT, Location.NODE, Type.SCALAR) → the id of each node 'key': (Stage.INPUT, Location. NODE, Type.SCALAR) → the values to sort 'pred': (Stage.OUTPUT, Location. NODE, Type.POINTER) → the final node order 'pred h': (Stage. HINT, Location. NODE, Type. POINTER) → the node order along execution 'i': (Stage.HINT, Location.NODE, Type.MASK_ONE) → index for insertion 'j': (Stage.HINT, Location.NODE, Type.MASK_ONE) → index tracking \u0026ldquo;sorted up to\u0026rdquo; A probe can be input, output or hint.\nThe inputs and outputs are fixed during algorithm execution, the hints change during execution\n→ they specify the algorithm (e.g., all sorting algorithms have the same inputs and outputs, differing only in their hints).\nRepresentation: Encoding pos: Positional ID (ID of node) is encoded as vector by encoder.\nkey: The value to be processed is also encoded as vector and added to pos.\npred_h: pointer used as hint is encoded and used as edge embedding.\ni, j: index needed for insertion are also added.\nRepresentation: Decoding Processing step is agnostic to the algorithm. Processing parameter is shared.\nTraining Hint: used only when training (not testing)\nWhen training, hint loss is also added along with output loss.\nDetails Trained on unlimited samples of size (number of nodes) \u0026lt; 16\nThe training distribution doesn\u0026rsquo;t cover all possible inputs though (e.g., we use only Erdös-Rényi graphs)\nTested on samples of size 64.\nThe length of the trajectory is given → both at train \u0026amp; test time.\nEarly stopping based on in-distribution scores.\nBut.. why even care about building a generalist? → It is all about problem solving!\nHow do we solve problems?\nExample: Route recommendation\nDetails With Neural Algorithmic Reasoning, we break the blue bottleneck!\nA generalist processor would break the red one!\nIf the model have a shared latent space where all the \u0026ldquo;key\u0026rdquo; algorithms would be executed\u0026hellip; No longer need to decide upfront which algorithm to use! The algorithm (combo) can be softly selected, learned by backprop through the encoder. To get a generalist, first we need a good specialist However, training a generalist is not as easy as simple training over all 30 algos in CLRS-30!\nInitial runs of this kind led to NaNs.\nPrior results, e.g. NE++ (Xhonneux et al., NeurIPS'21) imply this can be successful only if the algorithms being learnt together are highly related (e.g. Prim + Dijkstra)..\nKey limitation:\nTasks with high learning instabilities cause breakages for all other.\n→ Set out to improve single-task stability first!\nBucket list of improvements Key improvements include:\nRemoving teacher forcing\nTraining data augmentation (e.g. sampling multiple sizes below 16)\nSoft hint propagation (e.g. do not apply $\\argmax$ to the hints; compute $\\text{softmax}$ instead)\nStatic hint elimination (if a hint provably never changes, convert it to an input)\nEncoder initialization (Xavier) + gradient clipping\nRandomized positional embeddings\nPermutation decoders using the Sinkhorn operator\nGating mechanisms in the processor\nTriplet reasoning\n$t_{ijk} = \\psi_t (h_i, h_j, h_k, e_{ij}, e_{ik}, e_{kj}, g)$\n$h_{ij} = \\phi_t(\\max_k t_{ijk})$\nResults Final step to the generalist: Chunking The chunking mechanism was important for multi-task learning!\nThis not only helps protect against OOM issues, it also improves learning stability!\nThe idea is conceptually simple (though tricky to implement)\nThe length of the trajectory is set to 16. Shorter samples are not padded, but concatenated by next sample. i.e. if trajectory doesn\u0026rsquo;t fully fit in the chunk, this is OK-can restart from a midpoint hint.\nInitialization of the hidden state should not matter, since CLRS-30 tasks are Markovian!\nSingle generalist that matches the thirty specialists Chunking helps significantly Reference Learning on Graphs Conference 2022 - Day 1 Livestream\nA Generalist Neural Algorithmic Learner\n","date":"2023-01-13T00:00:00Z","image":"https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/thumbnail_hudd1318377aff45244e3590338f52153c_112022_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/","title":"A Generalist Neural Algorithmic Learner"},{"content":"NeurIPS, \u0026lsquo;22\nDoes GNN Pre-training Help Molecular Representation?\nSummary Self-supervised pre-training alone does not provide statistically significant improvements over non-pre-trained methods on downstream tasks. Data splits, hand-crafted rich features, or hyperparameters can bring significant improvements. Preliminaries Pre-train objectives\nSelf-supervised Node prediction Context prediction Motif prediction Contrastive learning Supervised Related tasks with label (e.g. ChEMBL dataset) Graph features\nBasic Feature set used in Hu et al.\nRich\nFeature set used in Rong et al. This is the superset of basic features. In downstream tasks, additional 2d normalized rdNormalizedDescriptors are used (not in pre-training).\nDownstream splits\nScaffold\nSorts the molecule according to the scaffold, then partition the sorted list into train/valid/test splits. → Deterministic\nMolecules of each set are most different ones.\nBalanced scaffold\nIntroduces the randomness in the sorting and splitting stages of Scaffold split.\nGNN architecture\nGIN GraphSAGE Pre-train dataset\nZINC15 (self-supervised)\n2 million molecules. Pre-processed following Hu et al.\nSAVI (self-supervised)\n1 billion drug-like molecules synthesized by computer simulated reactions.\nChEMBL (supervised)\n500k drugable molecules with 1,310 prediction target labels from bio-activity assays.\nResults Key Takeaways When pre-training might help? Related supervised pre-training dataset. But not always feasible. If the rich features are absent. If the downstream split distributions are substantially different. When the gain dimishes? If using rich features. If don’t have the highly relevant supervisions. If the downstream split is balanced. If the self-supervised learning dataset lacks diversity. Why pre-training may not help in some cases? Some of the pre-training methods (e.g. node label prediction) might be too easy\n→ Transfer less knowledge.\nSo…\nUse rich features Use balanced scaffold Use related supervised pre-training dataset Use difficult pre-training task (for self-supervised pre-training) and use high-quality negative samples. ","date":"2022-11-22T00:00:00Z","image":"https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/thumbnail_hube49b7519c97672414e9b0ddbe5479a4_229014_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/","title":"Does GNN Pre-training Help Molecular Representation?"},{"content":"NeurIPS, \u0026lsquo;22,\nGraph Self-supervised Learning with Accurate Discrepancy Learning\nSummary Authors proposed a framework called D-SLA that aims to learn the exact discrepancy between the original and the perturbed graphs. Three major components Learn to distinguish whether each graph is the original graph or the perturbed one. Capture the amount of discrepancy for each perturbed graph (using edit distance) Learn relative discrepancy with other graphs Preliminaries Graph Neural Networks (GNN) Aggregate the features from its neighbors Combining the aggregated message Variants of Update \u0026amp; Aggregate functions\nGraph Convolution Network (GCN)\nGeneral convolution operation + Mean aggregation\nGraphSAGE\nConcatenate representations of neighbors with its own representation when updating\nGraph Attention Network (GAT)\nConsiders the relative importance among neighboring nodes when aggregation\nGraph Isomorphism Network (GIN)\nSum aggregation\nSelf-supervised learning for graphs (GSL) Aims to learn a good representation of the graphs in an unsupervised manner.\n→ Transfer this knowledge to downstream tasks.\nMost prevalent framework for GSL\nPredictive learning (PL)\nAims to learn contextual relationships by predicting sub-graphical features (nodes, edges, subgraphs)\npredict the attributes of masked nodes predict the presence of an edge or a path predict the generative sequence, contextual property, and motifs But predictive learning may not capture the global structures and/or semantics of graphs.\nContrastive learning (CL)\nAims to capture global level information.\nEarly CL learn the similarity between the entire graph and its substructure. Others include attribute masking, edge perturbation, and subgraph sampling. Recent CL adversarial methods generate positive examples either by adaptively removing the edges or by adjusting the attributes. But CL may not distinguish two topologically similar graphs yet having completely different properties.\nMinimize $\\mathcal{L}_{CL} = - \\log \\frac{f_{\\text{sim}} (h_{\\mathcal{G}_i}, h_{\\mathcal{G}_j})}{\\sum_{\\mathcal{G}\u0026rsquo;, \\mathcal{G\u0026rsquo; \\neq \\mathcal{G}_0}}f_{\\text{sim}}(h_{\\mathcal{G}_i}, h_{\\mathcal{G}\u0026rsquo;})}$\n$\\mathcal{G}_0$: original graph $\\mathcal{G}_i, \\mathcal{G}_j$: perturbed graphs $\\mathcal{G}\u0026rsquo;$: other graph in the same batch with the $\\mathcal{G}_0$, a.k.a. negative graph positive pair: $(\\mathcal{G}_i, \\mathcal{G}_j)$; negative pair: $(\\mathcal{G}_i, \\mathcal{G}\u0026rsquo;)$ $f_\\text{sim}$: similarity function between two graphs → $L_2$ distance or cosine similarity → similarity of positive pair $\\uparrow$, similarity of negative pair $\\downarrow$\nDiscrepancy Learning Discriminate original vs perturbed\nPerturbed graph could be semantically incorrect!\n→ Embed perturbed graph apart from original.\n$\\mathcal{L}_{GD} = - \\log \\Big (\\frac{e^{S_0}}{e^{S_0} + \\sum_{i \\geq 1}e^{S_i}} \\Big ) \\text{ with } S = f_S(h_{\\mathcal{G}})$\nIntuitively,\nlarge value of $e^{S_0}$ for the original graph small value of $e^{S_i}$ for the perturbed graphs How to perturb?\nAim at perturbed graph to be semantically incorrect\nRemove or add a small number of edges\nManipulate the edge set by removing existing edges + adding new edges on $\\mathcal{X}_\\mathcal{E}$\nMask node attributes\nRandomly mask the node attributes on $\\mathcal{X}_\\mathcal{V}$ for both original and perturbed graphs\n(to make it more difficult to distinguish between them)\n$\\mathcal{G}_0 = (\\mathcal{V}, \\mathcal{E}, \\tilde{\\mathcal{X}^0_{\\mathcal{V}}}, \\mathcal{X}_{\\mathcal{E}}), \\tilde{\\mathcal{X}^0_{\\mathcal{V}}} \\sim \\texttt{M}(\\mathcal{G})$\n$\\mathcal{G}_i = (\\mathcal{V}, \\mathcal{E}^i, \\tilde{\\mathcal{X}^i_{\\mathcal{V}}}, \\mathcal{X}^i_{\\mathcal{E}}), \\tilde{\\mathcal{X}^i_{\\mathcal{V}}} \\sim \\texttt{M}(\\mathcal{G}), (\\mathcal{E}^i, \\mathcal{X}^i_{\\mathcal{E}}) \\sim \\texttt{P}(\\mathcal{G})$\nPersonal opinion\nThe real usage of discriminator loss will be to push original \u0026amp; perturbed graph apart, while applying edit distance loss. Discrepancy with Edit distance\nHow dissimilar?\nUsually, we need to measure the graph distance, such as edit distance.\nEdit distance: number of insertion, deletion, and substitution operations for nodes \u0026amp; edges to transform one graph from another. → NP hard!\nBut we know the exact number of perturbations for each graphs\n→ use it as distance.\n$\\mathcal{L}_{edit} = \\sum_{i, j} \\Big ( \\frac{d_i}{e_i} - \\frac{d_j}{e_j}\\Big )^2 \\text{ with } d_i = f_{\\text{diff}}(h_{\\mathcal{G}_0}, h_{\\mathcal{G}_i})$\n$f_{\\text{diff}}$ measures the embedding level differences between graphs with L2 norm.\n$e_i$: edit distance (number of perturbations)\nThe trivial solution for the edit distance loss is $d_i = d_j = 0$. But because of the discriminator loss, this is not possible.\nRelative discrepancy learning with other graphs\nAssumption:\nDistance between original and negative graphs in the same batch is larger than the distance between the original and perturbed graphs with some amount of margin.\nFormally,\n$\\mathcal{L}_{margin} = \\sum_{i, j} \\max (0, \\alpha + d_i - d\u0026rsquo;_j)$\n$d_i$: distance between original and its perturbed graphs\n$d\u0026rsquo;_j$: distance between original and negative graphs\nIntuitively, $\\alpha + d_i \u0026lt; d\u0026rsquo;_j$ !\nOverall loss $\\mathcal{L} = \\mathcal{L}_{GD} + \\lambda_1 \\mathcal{L}_{edit} + \\lambda_2 \\mathcal{L}_{margin}$\nResults ","date":"2022-09-21T00:00:00Z","image":"https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/thumbnail_hu1da554d6c291b842a8ea287e9b40bdf6_233064_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/","title":"Graph Self-supervised Learning with Accurate Discrepancy Learning"},{"content":"NeurIPS Poster, \u0026lsquo;21,\nPhysChem: Deep Molecular Representation Learning via Fusing Physical and Chemical Information\nSummary Used physicist network (PhysNet) and chemist network (ChemNet) simultaneously, and each network shares information to solve individual tasks. PhysNet: Neural physical engine. Mimics molecular dynamics to predict conformation. ChemNet: Message passing network for chemical \u0026amp; biomedical property prediction. Molecule without 3D conformation can be inferred during test time. Preliminaries Molecular representation learning:\nEmbedding molecules into latent space for downstream tasks.\nNeural Physical Engines\nNeural networks are capable of learning annotated potentials and forces in particle systems.\nHamNet proposed a neural physical engine that operated on a generalized space, where positions and momentums of atoms were defined as high-dimensional vectors.\nMulti-task learning\nSharing representations for different but related tasks.\nModel fusion\nMerging different models on identical tasks to improve performance.\nNotation Graph $\\mathcal{M} = (\\mathcal{V}, \\mathcal{E}, n, m, \\mathbf{X}^v, \\mathbf{X}^e)$\n$\\mathcal{V}$: set of $n$ atoms $\\mathcal{E}$: set of $m$ chemical bonds $\\mathbf{X}^v \\in \\mathbb{R}^{n \\times d_v} = (x^v_1, \u0026hellip;, x^v_n)^\\top$: matrix of atomic features $\\mathbf{X}^e \\in \\mathbb{R}^{m \\times d_e} = (x^e_1, \u0026hellip;, x^e_m)^\\top$: matrix of bond features Model Figure 1. PhysChem Architecture\nInitializer\nInput: atomic features, bond features (from RDKit) Layer: fully connected layers Output: bond states, atom states for ChemNet\n$v^{(0)}_i = \\text{FC}(x^v_i), i\\in \\mathcal{V}$\n$e^{(0)}_{i,j} = \\text{FC}(x^e_{i,j}), (i, j)\\in \\mathcal{E}$ atom positions, atomic momenta for PhysNet\nBond strength adjacency matrix\n$$A(i,j)=\\begin{cases}0, \u0026amp; \\text{if $(i,j) \\notin \\mathcal{E}$} \\\\ \\text{FC}_{\\text{sigmoid}}(x^e_{i,j}), \u0026amp; \\text{if $(i,j) \\in \\mathcal{E}$} \\end{cases}$$ $\\tilde{V} = \\text{GCN}(A, V^{(0)})$\n${ (q^{(0)}_i \\oplus p^{(0)}_i)} = \\text{LSTM}({\\tilde{v}_i}), i \\in \\mathcal{V}$ PhysNet\nPhysNet is inspired by HamNet.\nHamNet showed that neural networks can simulate molecular dynamics for conformation prediction. Directly parameterize the forces between each pair of atoms. Consider the effects of chemical interactions(e.g. bond types) by cooperating with ChemNet’s bond states. Introduces torsion forces. Output: 3D conformation ChemNet\nChemNet modifies MPNN(message passing neural network) for molecular representation learning. Output: Molecule representation Loss $L_{\\text{phys}}$: Conn-k loss for Conformation prediction (PhysNet)\n$k$-hop connectivity loss\n$L_{\\text{Conn}-k}(\\hat{\\mathbf{R}}, \\mathbf{R}) = |\\frac{1}{n} \\hat{\\mathbf{C}}^{(k)} \\odot (\\hat{\\mathbf{D}} - \\mathbf{D}) \\odot (\\hat{\\mathbf{D}} - \\mathbf{D}) |_{F}$\n$\\odot$: element-wise product\n$| \\cdot |$: Frobenius norm\n$(\\hat{\\mathbf{D}} - \\mathbf{D})$ : distance matrix of the real and predicted conformations $(\\hat{\\mathbf{R}} - \\mathbf{R})$\n$\\hat{\\mathbf{C}}^{(k)}$: normalized $k$-hop connectivity matrix\n$L_{\\text{chem}}$: MAE or Cross entropy loss for Property prediction (ChemNet)\nTotal loss\n$L_{\\text{total}} = \\lambda L_{\\text{phys}} + L_{\\text{chem}}$\nCheckpoints\nIs Conn-k loss generally used in other conformation prediction models?\nNo! But seems related to local distance loss.\nIs triplet descriptor generally used in other models?\nNo!\n","date":"2022-07-29T00:00:00Z","image":"https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/thumbnail_hu614a3ffe516a8cd557a8bbd359513341_952945_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/","title":"PhysChem: Deep Molecular Representation Learning via Fusing Physical and Chemical Information"},{"content":"Oncogene, \u0026lsquo;20\nCMap based Drug Repositioning of BTZ to reverse the metastatic effect of GALNT14 in lung cancer\nSummary In-silico approach based on CMap to identify drug candidates for lung cancer metastasis Revealed the underlying mechanisms of undruggable target (GALNT14) and targeted the downstream transcription factor Repositioned drug: BTZ (Bortezomib) Integrated multiple independent expression signatures from cancer patients (TCGA), genetic perturbations(knock-down or overexpression), and drug treatment (CMap) Background GALNT14: a putative driver of lung cancer metastasis, leading to poor survival \u0026amp; has poor druggability. Bortezomib: drug used for multiple myeloma and mantle cell lymphoma CMap: a collection of genome wide expression profiles of cell lines treated with \u0026gt; 20,000 chemicals Main Results Figure 1. GALNT14 as a putative molecular target for lung cancer metastasis.\n1a. TCGA Lung adenocarcinoma cohort의 516명 lung cancer 환자의 transcriptome data. 1b. relapse-free survival / DEG 분석에서 7개의 gene들이 검출되었고, 그 7개의 gene의 expression이 높은 group, 낮은 group으로 분류. 1c. metastasis와 tumor signature가 high-expression group에서 enrich 되었음. 1d. GALNT14만 단독으로 보아도 metastasis와 tumor 에서 enrich 되어 있음을 알 수 있음. 1e, 1f. GALNT14이 각각 metastatic potential과 tumorigenic potential이 있다는 in vivo 실험 결과. 1g. Metastatic lung cancer cell이 non-metastatic cancer보다 GALNT14 depletion에 더 vulnerable. 1h. GALNT14이 survival에 분명한 negative correlation을 보임.\n이것으로 미루어보아, GALNT14이 lung cancer metastasis의 promising molecular target이라는 것을 알 수 있음. Figure 5. In vivo validation of the anti-metastatic effect of BTZ. BTZ의 anti-migration, anti-invasion effect를 in vitro level에서 확인한 뒤 in vivo에서 cancer metastasis efficacy를 확인한 실험 결과\n5a. 쥐의 꼬리 정맥으로 H460 lung cancer cell을 주입하여 local metastasis를 유도하고 control 군, BTZ 처리군, CFZ 처리군으로 구분하였음. 5b. BTZ, CFZ의 proteasome inhibition을 확인하기 위해 혈액에서 proteasome activity를 측정한 결과. 상당히 줄어들었음을 알 수 있음. P.C. 는 positive control 5c. Body weight 정보. 항암제 처리로 인해 다른 조직 등에 dramatic한 영향은 없었음. 5d. Lung cancer로 metastasis 유무 사진. Vehicle과 CFZ는 상당부분 Metastasis가 일어난 것을 볼 수 있지만 BTZ는 6개 중 1개만 미약하게 metastasis 발생. 5e. H\u0026amp;E staining 후의 lung image 5f. tumor nodule size의 average. BTZ는 매우 작음. 5g. tumor nodule의 수 분포. BTZ 매우 적음. 5h. proteasome activity 차이 Discussion Unlike other studies that used CMap, they focused exclusively on a target gene related to a pertinent phenotype and identified BTZ as a drug candidate with novel anti-metastatic effects. In pathway level, the most enriched pathway was TGF￼ signaling, and they also identified the GALNT14-TGF￼ signature, which has invasive properties that are attenuated by BTZ. They integrated multiple independent expression signatures from cancer patients(TCGA), genetic perturbations(knock-down or overexpression), and drug treatment(CMap). ","date":"2022-07-22T00:00:00Z","image":"https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/thumbnail_hub7b0114b78cd33a49a5f6e1bdf51e130_146004_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/","title":"CMap based Drug Repositioning of BTZ to reverse the metastatic effect of GALNT14 in lung cancer"},{"content":"ICLR, \u0026lsquo;17,\nFew-Shot Learning with Graph Neural Networks\nSummary Used similarity value between samples for few shot learning. Regard each sample as nodes, and similarity kernel as edges. Similarity kernel is trainable. (i.e. Not just simple inner product) Can be applied to semi-supervised learning and active learning. State-of-the-art performance in Omniglot and Mini-ImageNet in 2017. Keywords Few shot learning Graph neural network Semi-supervised learning Active learning with Attention Introduction Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks. However, there are some tasks(e.g. few shot learning) that cannot achieve high performance with conventional methods. New supervised learning setup Input-output setup: With i.i.d. samples of collections of images and their associated label similarity cf) conventional setup: i.i.d. samples of images and their associated labels Authors\u0026rsquo; model can be extended to semi-supervised and active learning Semi-supervised learning:\nLearning from a mixture of labeled and unlabeled examples\nActive learning:\nThe learner has the option to request those missing labels that will be most helpful for the prediction task\nClosely related works and ideas [Research article] Matching Networks for One shot learning - Vinyals et al.(2016)\nMapped support set of images into the desired label.\nAnd developed an end-to-end trainable k-nearest neighbors, accepting those support sets as input via attention LSTM.\n$k$: number of data in support set $\\hat{x}$: new data $\\hat{y}$: its class $\\hat{y}$ is a linear combination of the labels in the support set $a$: attention mechanism, which is a kernel [Research article] Prototypical Networks for Few-shot Learning - Snell et al.(2017)\nAuthors point out the overfitting problem of Matching networks.\nPrototype: Center(mean) of each class cluster Similarity: $-\\text{Euclidean distance}$ [Review article] Geometric deep learning - Bronstein et al.(2017)\n\u0026ldquo;Geometric deep learning is an umbrella term for emerging techniques attempting to generalize deep models to non-Euclidian domains such as graphs and manifolds\u0026rdquo;\n[Research article] Message passing - Gilmer et al.(2017)\n$$ m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw}) \\ h^{t+1}_v = U_t(h^t_v, m^{t+1}_v ) $$\n$M_t$: message functions $U_t$: vertex update functions $h^t_v$: hidden states of node $v$ in the graph at time $t$ $m^{t+1}_v$: messages of node $v$ in the graph at time $t+1$ $N(v)$: neighbors of node $v$ e.g.)\nProblem set-up Authors view the task as a supervised interpolation problem on a graph\nNodes: Images Edges: Similarity kernels → TRAINABLE General set-up Input-output pairs $(\\mathcal{T}_i, Y_i)_i$ drawn from i.i.d. from a distribution $\\mathcal{P}$ of partially labeled image collections\n$s$: # labeled samples $r$: # unlabled samples $t$: # samples to classify $K$: # classes $\\mathcal{P}_l(\\mathbb{R}^N)$: class-specific image distribution over $\\mathbb{R}^N$ targets $Y_i$ are associated with $\\bar{x}_1, \u0026hellip;, \\bar{x}_t \\in \\mathcal{T}_i$ Learning objective:\n$\\min_\\Theta \\frac{1}{L} \\sum_{i \\leq L} \\ell(\\Phi(\\mathcal{T}_i, \\Theta), Y_i) + \\mathcal{R}(\\Theta)$\n($\\mathcal{R}$ is the standard regularization objective)\nFew shot learning setting $r=0, t=1, s=qK$ $\\longrightarrow$ $q-\\text{shot} , K-\\text{way}$\nSemi-supervised learning setting $r \u0026gt; 0, t=1$\nModel can use the auxiliary images(unlabeled set) ${ \\tilde{x}_1, \u0026hellip;, \\tilde{x}_r }$ to improve the prediction accuracy, by leveraging the fact that these samples are drawn from the common distributions.\nActive learning setting The learner has the ability to request labels from the auxiliary images ${\\tilde{x}_1, \u0026hellip;, \\tilde{x}_r}$.\nModel $\\phi(x)$: CNN $h(l)$: One-hot encoded label(for labeled set), or uniform distribution(for unlabeled set) Set and Graph Input Representations The goal of few shot learning:\nTo propagate label information from labeled samples towards the unlabeled query image\n→ The propagation can be formalized as a posterior inference over a graphical model\n$G_\\mathcal{T} = (V,E)$\nSimilarity measure is not pre-specified, but learned!\nc.f.) in Siamese network, the similarity measure is fixed(L1 distance)!\n본 논문(Few shot learning with GNN)에 쓰인 문장 구조가 이상해서 헷갈리게 쓰여있음.\nGraph Neural Networks We are given an input signal $F \\in \\mathbb{R}^{V \\times d}$ on the vertices of a weighted graph $G$.\nThen we consider a family, or a set \u0026ldquo;$\\mathcal{A}$\u0026rdquo; of graph intrinsic linear operators.\n$\\mathcal{A} = {\\tilde{A}^{(k)}, \\mathbf{1}}$\nLinear operator\ne.g.) Simplest linear operator is adjacency operator $A$, where $(AF)i = \\sum{j \\sim i} w_{i,j}F_j$ ($w_{i,j}$ is associated weight)\nGNN layer\nA GNN layer $\\text{Gc}(\\cdot)$ receives as input a signal $\\mathbf{x}^{(k)} \\in \\mathbb{R}^{V\\times d_k}$ and produces $\\mathbf{x}^{(k+1)} \\in \\mathbb{R}^{V\\times d_{k+1}}$\n$$ \\mathbf{x}^{(k+1)} = \\text{Gc}(\\mathbf{x}^{(k)}) = \\rho\\Big(\\sum_{B\\in\\mathcal{A}} B\\mathbf{x}^{(k)}\\theta^{(k)}_{B, l}\\Big ) $$\n$\\mathbf{x}^{(k)}$: representation vector of a certain node at time step $k$\n$\\theta$: trainable parameters\n$\\rho$: Leaky ReLU\nConstruction of edge feature matrix, inspired by message passing algorithm\n$$ \\tilde{A}^{(k)}_{i, j} = \\varphi_{\\tilde{\\theta}}(\\mathbf{x}^{(k)}_i, \\mathbf{x}^{(k)}_j ) $$\n$\\tilde{A}^{(k)}_{i, j}$: learned edge features from the node\u0026rsquo;s current hidden representation(at time step $k$)\n$\\varphi$: a metric and a symmetric function parameterized with neural network\n$$ \\varphi_{\\tilde{\\theta}}(\\mathbf{x}^{(k)}_i, \\mathbf{x}^{(k)}_j ) = \\text{MLP}_{\\tilde{\\theta}}(abs(\\mathbf{x}^{(k)}_i - \\mathbf{x}^{(k)}_j)) $$\n→ $\\tilde{A}^{(k)}$ is then normalized by row-wise softmax\n→ And added to the family $\\mathcal{A} = {\\tilde{A}^{(k)}, \\mathbf{1}}$\n$\\mathbf{1}$: Identity matrix, which is the self-edge to aggregate vertex\u0026rsquo;s own features Construction of initial node features\n$$ \\mathbf{x}^{(0)}_i = (\\phi(x_i), h(l_i)) $$\n$\\phi$: convolutional neural network\n$h(l) \\in \\mathbb{R}^K_+$ : a one-hot encoding of the label\nFor images with unknown label, $\\tilde{x}_j$(unlabeled data) and $\\bar{x}_j$(test data), $h(l_j)$ is set with uniform distribution.\nTraining Few-shot and Semi-supervised learning The final layer of GNN is a softmax mapping. We then use cross-entropy loss:\n$$ \\ell(\\Phi(\\mathcal{T}; \\Theta), Y) = -\\sum_k y_k \\log P(Y_* = y_k , |, \\mathcal{T}) $$\nThe semi-supervised setting is trained identically, but the initial label fields of $\\tilde{x}_j$s will be filled with uniform distribution.\nActive learning (with attention) In active learning, the model has the intrinsic ability to query for one of the labels from ${ \\tilde{x}_1, \u0026hellip;, \\tilde{x}_r }$.\nThe network will learn to ask for the most informative label to classify the sample $\\bar{x}$.\nThe querying is done after the first layer of GNN by using a softmax attention over the unlabeled nodes of the graph.\nAttention\nWe apply a function $g(\\mathbf{x}^{(1)}_i) \\in \\mathbb{R}^1$ that maps each unlabeld vector node to a scalar value.\nA softmax is applied over the ${1, \u0026hellip;, r}$ scalar values obtained after applying $g$:\n$r$: # unlabeled samples\n$$ \\text{Attention} = \\text{Softmax}(g(\\mathbf{x}^{(1)}_{{1,\u0026hellip;,r}})) $$\nTo query only one sample we set all elements to zero except for one. → $\\text{Attention}'$\nAt training, model randomly samples one value based on its multinomial probability. At test, model just keeps the maximum value. Then we multiply this with the label vectors\n$$ w \\cdot h(l_{i*}) = \\langle \\text{Attention}\u0026rsquo;, h(l_{{1, \u0026hellip;, r}}) \\rangle $$\n($w$ is scaling factor)\nThis value is then summed to the current representation.\n$$ \\mathbf{x}^{(1)}_{i*} = [\\text{Gc}(\\mathbf{x}^{(0)}_{i*}), \\mathbf{x}^{(0)}_{i*}] = [\\text{Gc}(\\mathbf{x}^{(0)}_{i*}), (\\phi(x_{i*}), h(l_{i*}))] $$\nResults Few-shot learning Omniglot\n# of parameters: $\\sim5\\text{M} (\\text{TCML})$, $\\sim300 \\text{K}(3 \\text{layers GNN})$\nOmniglot: 1,623 characters X 20 examples for each characters\nMini-ImageNet: Originally introduced by Vinyals et al.(2016)\n# of parameters: $\\sim 11\\text{M} (\\text{TCML})$, $\\sim 400 \\text{K}(3 \\text{ layers GNN})$\nMini-ImageNet\nDivided into 64 training, 16 validation, 20 testing classes each containing 600 examples.\nSemi-supervised learning Omniglot\nMini-ImageNet\nActive learning Random: Network chooses a random sample to be labeled, instead of one that maximally reduces the loss of the classification task $\\mathcal{T}$\nReferences ","date":"2021-10-15T00:00:00Z","image":"https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/thumbnail_hu8ba07e37572d4400f76afaadd55834a8_158186_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/","title":"Few-shot Learning with Graph Neural Networks"},{"content":"ICML, \u0026lsquo;17\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nSummary MAML is a general and model-agnostic algorithm that can be directly applied to a model trained with gradient descent procedure. MAML does not expand the number of learned parameters. MAML does not place constraints on the model architecture. Keywords Model agnostic Fast adaptation Optimization based approach Learning good model parameters Preliminaries Common Approaches of Meta-Learning and MAML\nMAML is one of the most influential model of optimization-based approaches.\nA few terminologies of meta-learning problems\nIntroduction Goal of ideal artificial agent:\nLearning and adapting quickly from only a few examples.\nTo do so, an agent must..\nIntegrate its prior experience with a small amount of new information. Avoid overfitting to the new data. → Meta-learning has same goals.\nMAML:\n\u0026ldquo;The key idea of MAML is to train the model\u0026rsquo;s initial parameters such that the model has maximal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task.\u0026rdquo;\nLearning process of MAML:\nMAML maximizes the sensitivity of the loss functions of new tasks.\nAuthors demonstrated the algorithm on three different model types.\nFew-shot regression Image classification Reinforcement learning 2. Model-Agnostic Meta Learning 2.1. Meta-Learning Problem Set-Up To apply MAML to a variety of learning problems, authors introduce a generic notion of a learning task:\n$\\mathcal{T} = { \\mathcal{L}(\\mathbf{x}_1, \\mathbf{a}_1, \u0026hellip;, \\mathbf{x}_H, \\mathbf{a}_H), q(\\mathbf{x}_1), q(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t), H }$\nEach task $\\mathcal{T}$ consists of..\n$\\mathcal{L}$: a loss function, might be misclassification loss or a cost function in a Markov decision process\n$q(\\mathbf{x}_1)$: a distribution over initial observations\n$q(\\mathbf{x}_{t+1}|\\mathbf{x}_t , \\mathbf{a}_t)$: a transition distribution\n$H$: an episode length(e.g. in i.i.d. supervised learning problems, the length $H = 1$.)\nAuthors consider a distribution over tasks $p(\\mathcal{T})$\nMeta-training:\nA new task $\\mathcal{T}_i$ is sampled from $p(\\mathcal{T})$.\nThe model is trained with only $K$ samples drawn from $q_i$.\nLoss $\\mathcal{L}_{\\mathcal{T}_i}$ is calculated and feedbacked to model.\nModel $f$ is tested on new samples from $\\mathcal{T}_i$.\nThe model $f$ is then improved by considering how the $test$ error on new data from $q_i$ changes with respect to the parameters.\n2.2. A Model-Agnostic Meta-Learning Algorithm Intuition: Some internal representations are more transferrable than others. How can we encourage the emergence of such general-purpose representations?\nA model $f_\\theta$ has paramters $\\theta$.\nFor each task $\\mathcal{T}_i$, $f_\\theta$\u0026rsquo;s parameters $\\theta$ become $\\theta_i\u0026rsquo;$.\nAlgorithm\ncf) Terminologies for below description(temporarily defined by JH Gu)\nDivide tasks\nSeparate tasks into meta-training task set(${\\mathcal{T}_i^{\\text{tr}}}$) and meta-test task set(${\\mathcal{T}_i^{\\text{test}}}$).\n(We can think of ${\\mathcal{T}_i^{\\text{tr}}}$ as monthly tests(모의고사), and ${\\mathcal{T}_i^{\\text{test}}}$ as annual tests(수능))\nFor each task, divide each samples into $\\mathcal{D}_{\\mathcal{T}_i}^{\\text{study}}$(task-specific samples for studying, also called as support set), $\\mathcal{D}_{\\mathcal{T_i}}^{\\text{check}}$(task-specific samples for checking, also called as query set)\n(We can think of $\\mathcal{D}_{\\mathcal{T}_i}^{\\text{study}}$ as 필수예제 in 수학의 정석, and $\\mathcal{D}_{\\mathcal{T}_i}^{\\text{check}}$ as 연습문제 in 수학의 정석)\nMeta-training using meta-training task set ${\\mathcal{T}_i^{\\text{tr}}}$\nInner loop(task-specific $K$-shot learning)\nFor each $\\mathcal{T}_i$ in ${\\mathcal{T}_i^{\\text{tr}}}$, a new parameter $\\theta_i\u0026rsquo;$ is created.\nEach $\\theta_i\u0026rsquo;$ is initialized as $\\theta$.\nWith task-specific samples for studying($\\mathcal{D}_{\\mathcal{T}_i^{\\text{tr}}}^{\\text{study}}$), each $\\theta_i\u0026rsquo;$ is updated by:\n$$ \\theta_i\u0026rsquo; = \\theta - \\alpha \\nabla_\\theta \\mathcal{L}_{\\mathcal{T}_i}(f_\\theta) $$\nOuter loop(meta-learning across tasks)\nWith task-specific samples for checking($\\mathcal{D}_{\\mathcal{T_i}^{\\text{tr}}}^{\\text{check}}$), $\\theta$ is updated by:\n$$ \\theta = \\theta - \\beta \\nabla_\\theta \\sum_{\\mathcal{T}_i \\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_i} (f_{\\theta_i\u0026rsquo;}) $$\ncf) second-order derivative(Hessian) problem\nThe MAML meta-gradient update(outer loop) involves a gradient through a gradient, which can be resource-intensive. This requires an additional backward pass through $f$ to compute Hessian vector products.\n$$ \\begin{align*} \\textcolor{red}{\\theta\u0026rsquo;} \u0026amp;= \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta) \\ \\nabla_\\theta \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;}) \u0026amp;= (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\cdot (\\nabla_\\theta \\textcolor{red}{\\theta\u0026rsquo;}) \\ \u0026amp;= (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\cdot (\\nabla_\\theta (\\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta)) \\ \u0026amp;\\approx (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\cdot (\\nabla_\\theta \\theta) \\ \u0026amp;= (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\end{align*} $$\nAuthors included a comparison to drop the backward pass term and using just the first-order approximation, which showed not much difference.\nMeasure model performance using meta-test task set ${\\mathcal{T}_i^{\\text{test}}}$\nFor each $\\mathcal{T}_i$ in ${\\mathcal{T}_i^{\\text{test}}}$, adjust task-specific parameters with $\\mathcal{D}_{\\mathcal{T}_i^{\\text{test}}}^{\\text{study}}$. Test the performance with $\\mathcal{D}_{\\mathcal{T_i}^{\\text{test}}}^{\\text{check}}$. 3. Species of MAML 3.1. Supervised Regression and Classification Algorithm\nFormalizing supervised regression and classification\nHorizon $H = 1$ Drop the timestep subscript on $\\mathbf{x}_t$ (since model accepts a single input and produces a single output) The task $\\mathcal{T}_i$ generates $K$ i.i.d. observations $\\mathbf{x}$ from $q_i$ Task loss is represented by the error between the model\u0026rsquo;s output for $\\mathbf{x}$ and the corresponding target values $\\mathbf{y}$. Loss functions\nMSE for regression\n$$ \\mathcal{L}_{\\mathcal{T}_i}(f_\\phi) = \\sum_{\\mathbf{x}^{(j)}, \\mathbf{y}^{(j)} \\sim \\mathcal{T}_i} | f_\\phi(\\mathbf{x}^{(j)}) - \\mathbf{y}^{(j)}|^2_2 $$\nCross entropy loss for discrete classification\n$$ \\mathcal{L}_{\\mathcal{T}_i}(f_\\phi) = \\sum_{\\mathbf{x}^{(j)}, \\mathbf{y}^{(j)} \\sim \\mathcal{T}_i} \\big\\{ \\mathbf{y}^{(j)} \\log f_\\phi (\\mathbf{x}^{(j)}) - (1-\\mathbf{y}^{(j)})\\log(1-f_\\phi(\\mathbf{x}^{(j)}) \\big\\} $$\n3.2. Reinforcement Learning Algorithm\nGoal of MAML in RL:\nQuickly acquire a policy for a new test task using only a small amount of experience in the test setting.\nFormalizing RL\nEach RL task $\\mathcal{T}_i$ contains..\nInitial state distribution $q_i(\\mathbf{x}_1)$ Transition distribution $q_i(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)$ $\\mathbf{a}_t$: action Loss $\\mathcal{L}_{\\mathcal{T}_i}$, which corresponds to the negative reward function $R$ Therefore, entire task is a Markov decision process(MDP) with horizon $H$\nThe model being learned, $f_\\theta$, is a policy that maps from states $\\mathbf{x}_t$ to a distribution over actions $\\mathbf{a}_t$ at each timestep $t \\in { 1, \u0026hellip;, H}$\nLoss function for task $\\mathcal{T}_i$ and model $f_\\phi$:\n$$ \\mathcal{L}_{\\mathcal{T}_i}(f_\\phi) = -\\mathbb{E}_{\\mathbf{x}_t, \\mathbf{a}_t \\sim f_\\phi, q_{\\mathcal{T}_i}} \\bigg [ \\sum_{t=1}^H R_i(\\mathbf{x}_t, \\mathbf{a}_t) \\bigg ] $$\nPolicy gradient method\nSince the expected reward is generally not differentiable due to unknown dynamics, authors used policy gradient methods to estimate the gradient.\nThe policy gradient method is an on-policy algorithm\n→ There are additional sampling procedures in step 5 and 8.\n4. Comparison with related works Comparison with other popular approaches\nTraining a meta-learner that learns how to update the parameters of the learner\u0026rsquo;s model\nex) On the optimization of a synaptic learning rule(Bengio et al. 1992)\n→ Requires additional parameters, while MAML does not.\nTraining to compare new examples in a learned metric space\nex) Siamese networks(Koch, 2015), recurrence with attention mechanisms(Vinyals et al. 2016)\n→ Difficult to directly extend to our problems, such as reinforcement learning.\nTraining memory-augmented models\nex) Meta-learning with memory-augmented neural networks(Santoro et al. 2016)\nThe recurrent learner is trained to adapt to new tasks as it is rolled out.\n→ Not really straightforward.\n5. Experimental Evaluation Three questions\nCan MAML enable fast learning of new tasks? Can MAML be used for meta-learning in multiple different domains? Can a model learned with MAML continue to improve with additional gradient updates and/or examples? 5.1. Regression 5.2. Classification 5.3. Reinforcement Learning References KAIST NeuroAI JC_#1 Meta Learning (편집본)\nhttps://ai.stanford.edu/~cbfinn/_files/dissertation.pdf\n[10주차] (MAML) Model-agnostic Meta Learning for Fast Adaptation of Deep Networks 논문 리뷰\nMeta-Learning: Learning to Learn Fast\n","date":"2021-09-15T00:00:00Z","image":"https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/thumbnail_huac13bc7b7974135f98c493a6d9abadaf_36220_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/","title":"MAML: Model-Agnostic Meta -Learning for Fast Adaptation of Deep Networks"}]