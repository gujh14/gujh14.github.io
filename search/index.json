[{"content":"NeurIPS, \u0026lsquo;22 https://arxiv.org/abs/2207.06010\nSummary Self-supervised pre-training alone does not provide statistically significant improvements over non-pre-trained methods on downstream tasks. Data splits, hand-crafted rich features, or hyperparameters can bring significant improvements. Preliminaries Pre-train objectives\nSelf-supervised Node prediction Context prediction Motif prediction Contrastive learning Supervised Related tasks with label (e.g. ChEMBL dataset) Graph features\nBasic Feature set used in Hu et al.\nRich\nFeature set used in Rong et al. This is the superset of basic features. In downstream tasks, additional 2d normalized rdNormalizedDescriptors are used (not in pre-training).\nDownstream splits\nScaffold\nSorts the molecule according to the scaffold, then partition the sorted list into train/valid/test splits. → Deterministic\nMolecules of each set are most different ones.\nBalanced scaffold\nIntroduces the randomness in the sorting and splitting stages of Scaffold split.\nGNN architecture\nGIN GraphSAGE Pre-train dataset\nZINC15 (self-supervised)\n2 million molecules. Pre-processed following Hu et al.\nSAVI (self-supervised)\n1 billion drug-like molecules synthesized by computer simulated reactions.\nChEMBL (supervised)\n500k drugable molecules with 1,310 prediction target labels from bio-activity assays.\nResults Key Takeaways When pre-training might help? Related supervised pre-training dataset. But not always feasible. If the rich features are absent. If the downstream split distributions are substantially different. When the gain dimishes? If using rich features. If don’t have the highly relevant supervisions. If the downstream split is balanced. If the self-supervised learning dataset lacks diversity. Why pre-training may not help in some cases? Some of the pre-training methods (e.g. node label prediction) might be too easy\n→ Transfer less knowledge.\nSo…\nUse rich features Use balanced scaffold Use related supervised pre-training dataset Use difficult pre-training task (for self-supervised pre-training) and use high-quality negative samples. ","date":"2022-11-22T00:00:00Z","image":"https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/0_hube49b7519c97672414e9b0ddbe5479a4_229014_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/","title":"Does GNN Pre-training Help Molecular Representation?"},{"content":"NeurIPS, \u0026lsquo;22,\nhttps://openreview.net/forum?id=SgZ-glWWUlq\nSummary Authors proposed a framework called D-SLA that aims to learn the exact discrepancy between the original and the perturbed graphs. Three major components Learn to distinguish whether each graph is the original graph or the perturbed one. Capture the amount of discrepancy for each perturbed graph (using edit distance) Learn relative discrepancy with other graphs Preliminaries Graph Neural Networks (GNN) Aggregate the features from its neighbors Combining the aggregated message Variants of Update \u0026amp; Aggregate functions\nGraph Convolution Network (GCN)\nGeneral convolution operation + Mean aggregation\nGraphSAGE\nConcatenate representations of neighbors with its own representation when updating\nGraph Attention Network (GAT)\nConsiders the relative importance among neighboring nodes when aggregation\nGraph Isomorphism Network (GIN)\nSum aggregation\nSelf-supervised learning for graphs (GSL) Aims to learn a good representation of the graphs in an unsupervised manner.\n→ Transfer this knowledge to downstream tasks.\nMost prevalent framework for GSL\nPredictive learning (PL)\nAims to learn contextual relationships by predicting sub-graphical features (nodes, edges, subgraphs)\npredict the attributes of masked nodes predict the presence of an edge or a path predict the generative sequence, contextual property, and motifs But predictive learning may not capture the global structures and/or semantics of graphs.\nContrastive learning (CL)\nAims to capture global level information.\nEarly CL learn the similarity between the entire graph and its substructure. Others include attribute masking, edge perturbation, and subgraph sampling. Recent CL adversarial methods generate positive examples either by adaptively removing the edges or by adjusting the attributes. But CL may not distinguish two topologically similar graphs yet having completely different properties.\nMinimize $\\mathcal{L}_{CL} = - \\log \\frac{f_{\\text{sim}} (h_{\\mathcal{G}_i}, h_{\\mathcal{G}_j})}{\\sum_{\\mathcal{G}\u0026rsquo;, \\mathcal{G\u0026rsquo; \\neq \\mathcal{G}_0}}f_{\\text{sim}}(h_{\\mathcal{G}_i}, h_{\\mathcal{G}\u0026rsquo;})}$\n$\\mathcal{G}_0$: original graph $\\mathcal{G}_i, \\mathcal{G}_j$: perturbed graphs $\\mathcal{G}\u0026rsquo;$: other graph in the same batch with the $\\mathcal{G}_0$, a.k.a. negative graph positive pair: $(\\mathcal{G}_i, \\mathcal{G}_j)$; negative pair: $(\\mathcal{G}_i, \\mathcal{G}\u0026rsquo;)$ $f_\\text{sim}$: similarity function between two graphs → $L_2$ distance or cosine similarity → similarity of positive pair $\\uparrow$, similarity of negative pair $\\downarrow$\nDiscrepancy Learning Discriminate original vs perturbed\nPerturbed graph could be semantically incorrect!\n→ Embed perturbed graph apart from original.\n$\\mathcal{L}_{GD} = - \\log \\Big (\\frac{e^{S_0}}{e^{S_0} + \\sum_{i \\geq 1}e^{S_i}} \\Big ) \\text{ with } S = f_S(h_{\\mathcal{G}})$\nIntuitively,\nlarge value of $e^{S_0}$ for the original graph small value of $e^{S_i}$ for the perturbed graphs How to perturb?\nAim at perturbed graph to be semantically incorrect\nRemove or add a small number of edges\nManipulate the edge set by removing existing edges + adding new edges on $\\mathcal{X}_\\mathcal{E}$\nMask node attributes\nRandomly mask the node attributes on $\\mathcal{X}_\\mathcal{V}$ for both original and perturbed graphs\n(to make it more difficult to distinguish between them)\n$\\mathcal{G}_0 = (\\mathcal{V}, \\mathcal{E}, \\tilde{\\mathcal{X}^0_{\\mathcal{V}}}, \\mathcal{X}_{\\mathcal{E}}), \\tilde{\\mathcal{X}^0_{\\mathcal{V}}} \\sim \\texttt{M}(\\mathcal{G})$\n$\\mathcal{G}_i = (\\mathcal{V}, \\mathcal{E}^i, \\tilde{\\mathcal{X}^i_{\\mathcal{V}}}, \\mathcal{X}^i_{\\mathcal{E}}), \\tilde{\\mathcal{X}^i_{\\mathcal{V}}} \\sim \\texttt{M}(\\mathcal{G}), (\\mathcal{E}^i, \\mathcal{X}^i_{\\mathcal{E}}) \\sim \\texttt{P}(\\mathcal{G})$\nPersonal opinion\nThe real usage of discriminator loss will be to push original \u0026amp; perturbed graph apart, while applying edit distance loss. Discrepancy with Edit distance\nHow dissimilar?\nUsually, we need to measure the graph distance, such as edit distance.\nEdit distance: number of insertion, deletion, and substitution operations for nodes \u0026amp; edges to transform one graph from another. → NP hard!\nBut we know the exact number of perturbations for each graphs\n→ use it as distance.\n$\\mathcal{L}_{edit} = \\sum_{i, j} \\Big ( \\frac{d_i}{e_i} - \\frac{d_j}{e_j}\\Big )^2 \\text{ with } d_i = f_{\\text{diff}}(h_{\\mathcal{G}_0}, h_{\\mathcal{G}_i})$\n$f_{\\text{diff}}$ measures the embedding level differences between graphs with L2 norm.\n$e_i$: edit distance (number of perturbations)\nThe trivial solution for the edit distance loss is $d_i = d_j = 0$. But because of the discriminator loss, this is not possible.\nRelative discrepancy learning with other graphs\nAssumption:\nDistance between original and negative graphs in the same batch is larger than the distance between the original and perturbed graphs with some amount of margin.\nFormally,\n$\\mathcal{L}_{margin} = \\sum_{i, j} \\max (0, \\alpha + d_i - d\u0026rsquo;_j)$\n$d_i$: distance between original and its perturbed graphs\n$d\u0026rsquo;_j$: distance between original and negative graphs\nIntuitively, $\\alpha + d_i \u0026lt; d\u0026rsquo;_j$ !\nOverall loss $\\mathcal{L} = \\mathcal{L}_{GD} + \\lambda_1 \\mathcal{L}_{edit} + \\lambda_2 \\mathcal{L}_{margin}$\nResults ","date":"2022-09-21T00:00:00Z","image":"https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/0_hu1da554d6c291b842a8ea287e9b40bdf6_233064_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/","title":"Graph Self-supervised Learning with Accurate Discrepancy Learning"},{"content":"NeurIPS Poster, \u0026lsquo;21,\nhttps://arxiv.org/abs/2112.04624\nSummary Used physicist network (PhysNet) and chemist network (ChemNet) simultaneously, and each network shares information to solve individual tasks. PhysNet: Neural physical engine. Mimics molecular dynamics to predict conformation. ChemNet: Message passing network for chemical \u0026amp; biomedical property prediction. Molecule without 3D conformation can be inferred during test time. Preliminaries Molecular representation learning:\nEmbedding molecules into latent space for downstream tasks.\nNeural Physical Engines\nNeural networks are capable of learning annotated potentials and forces in particle systems.\nHamNet proposed a neural physical engine that operated on a generalized space, where positions and momentums of atoms were defined as high-dimensional vectors.\nMulti-task learning\nSharing representations for different but related tasks.\nModel fusion\nMerging different models on identical tasks to improve performance.\nNotation Graph $\\mathcal{M} = (\\mathcal{V}, \\mathcal{E}, n, m, \\mathbf{X}^v, \\mathbf{X}^e)$\n$\\mathcal{V}$: set of $n$ atoms $\\mathcal{E}$: set of $m$ chemical bonds $\\mathbf{X}^v \\in \\mathbb{R}^{n \\times d_v} = (x^v_1, \u0026hellip;, x^v_n)^\\top$: matrix of atomic features $\\mathbf{X}^e \\in \\mathbb{R}^{m \\times d_e} = (x^e_1, \u0026hellip;, x^e_m)^\\top$: matrix of bond features Model Figure 1. PhysChem Architecture\nInitializer\nInput: atomic features, bond features (from RDKit) Layer: fully connected layers Output: bond states, atom states for ChemNet\n$v^{(0)}_i = \\text{FC}(x^v_i), i\\in \\mathcal{V}$\n$e^{(0)}_{i,j} = \\text{FC}(x^e_{i,j}), (i, j)\\in \\mathcal{E}$ atom positions, atomic momenta for PhysNet\nBond strength adjacency matrix\n$$A(i,j)=\\begin{cases}0, \u0026amp; \\text{if $(i,j) \\notin \\mathcal{E}$} \\\\ \\text{FC}_{\\text{sigmoid}}(x^e_{i,j}), \u0026amp; \\text{if $(i,j) \\in \\mathcal{E}$} \\end{cases}$$ $\\tilde{V} = \\text{GCN}(A, V^{(0)})$\n${ (q^{(0)}_i \\oplus p^{(0)}_i)} = \\text{LSTM}({\\tilde{v}_i}), i \\in \\mathcal{V}$ PhysNet\nPhysNet is inspired by HamNet.\nHamNet showed that neural networks can simulate molecular dynamics for conformation prediction. Directly parameterize the forces between each pair of atoms. Consider the effects of chemical interactions(e.g. bond types) by cooperating with ChemNet’s bond states. Introduces torsion forces. Output: 3D conformation ChemNet\nChemNet modifies MPNN(message passing neural network) for molecular representation learning. Output: Molecule representation Loss $L_{\\text{phys}}$: Conn-k loss for Conformation prediction (PhysNet)\n$k$-hop connectivity loss\n$L_{\\text{Conn}-k}(\\hat{\\mathbf{R}}, \\mathbf{R}) = |\\frac{1}{n} \\hat{\\mathbf{C}}^{(k)} \\odot (\\hat{\\mathbf{D}} - \\mathbf{D}) \\odot (\\hat{\\mathbf{D}} - \\mathbf{D}) |_{F}$\n$\\odot$: element-wise product\n$| \\cdot |$: Frobenius norm\n$(\\hat{\\mathbf{D}} - \\mathbf{D})$ : distance matrix of the real and predicted conformations $(\\hat{\\mathbf{R}} - \\mathbf{R})$\n$\\hat{\\mathbf{C}}^{(k)}$: normalized $k$-hop connectivity matrix\n$L_{\\text{chem}}$: MAE or Cross entropy loss for Property prediction (ChemNet)\nTotal loss\n$L_{\\text{total}} = \\lambda L_{\\text{phys}} + L_{\\text{chem}}$\nCheckpoints\nIs Conn-k loss generally used in other conformation prediction models?\nNo! But seems related to local distance loss.\nIs triplet descriptor generally used in other models?\nNo!\n","date":"2022-07-29T00:00:00Z","image":"https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/physchem_1_hu614a3ffe516a8cd557a8bbd359513341_952945_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/","title":"PhysChem: Deep Molecular Representation Learning via Fusing Physical and Chemical Information"},{"content":"Oncogene, \u0026lsquo;20\nhttps://www.nature.com/articles/s41388-020-1316-2\nSummary In-silico approach based on CMap to identify drug candidates for lung cancer metastasis Revealed the underlying mechanisms of undruggable target (GALNT14) and targeted the downstream transcription factor Repositioned drug: BTZ (Bortezomib) Integrated multiple independent expression signatures from cancer patients (TCGA), genetic perturbations(knock-down or overexpression), and drug treatment (CMap) Background GALNT14: a putative driver of lung cancer metastasis, leading to poor survival \u0026amp; has poor druggability. Bortezomib: drug used for multiple myeloma and mantle cell lymphoma CMap: a collection of genome wide expression profiles of cell lines treated with \u0026gt; 20,000 chemicals Main Results Figure 1. GALNT14 as a putative molecular target for lung cancer metastasis.\n1a. TCGA Lung adenocarcinoma cohort의 516명 lung cancer 환자의 transcriptome data. 1b. relapse-free survival / DEG 분석에서 7개의 gene들이 검출되었고, 그 7개의 gene의 expression이 높은 group, 낮은 group으로 분류. 1c. metastasis와 tumor signature가 high-expression group에서 enrich 되었음. 1d. GALNT14만 단독으로 보아도 metastasis와 tumor 에서 enrich 되어 있음을 알 수 있음. 1e, 1f. GALNT14이 각각 metastatic potential과 tumorigenic potential이 있다는 in vivo 실험 결과. 1g. Metastatic lung cancer cell이 non-metastatic cancer보다 GALNT14 depletion에 더 vulnerable. 1h. GALNT14이 survival에 분명한 negative correlation을 보임.\n이것으로 미루어보아, GALNT14이 lung cancer metastasis의 promising molecular target이라는 것을 알 수 있음. Figure 5. In vivo validation of the anti-metastatic effect of BTZ. BTZ의 anti-migration, anti-invasion effect를 in vitro level에서 확인한 뒤 in vivo에서 cancer metastasis efficacy를 확인한 실험 결과\n5a. 쥐의 꼬리 정맥으로 H460 lung cancer cell을 주입하여 local metastasis를 유도하고 control 군, BTZ 처리군, CFZ 처리군으로 구분하였음. 5b. BTZ, CFZ의 proteasome inhibition을 확인하기 위해 혈액에서 proteasome activity를 측정한 결과. 상당히 줄어들었음을 알 수 있음. P.C. 는 positive control 5c. Body weight 정보. 항암제 처리로 인해 다른 조직 등에 dramatic한 영향은 없었음. 5d. Lung cancer로 metastasis 유무 사진. Vehicle과 CFZ는 상당부분 Metastasis가 일어난 것을 볼 수 있지만 BTZ는 6개 중 1개만 미약하게 metastasis 발생. 5e. H\u0026amp;E staining 후의 lung image 5f. tumor nodule size의 average. BTZ는 매우 작음. 5g. tumor nodule의 수 분포. BTZ 매우 적음. 5h. proteasome activity 차이 Discussion Unlike other studies that used CMap, they focused exclusively on a target gene related to a pertinent phenotype and identified BTZ as a drug candidate with novel anti-metastatic effects. In pathway level, the most enriched pathway was TGF￼ signaling, and they also identified the GALNT14-TGF￼ signature, which has invasive properties that are attenuated by BTZ. They integrated multiple independent expression signatures from cancer patients(TCGA), genetic perturbations(knock-down or overexpression), and drug treatment(CMap). ","date":"2022-07-22T00:00:00Z","image":"https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_0_hub7b0114b78cd33a49a5f6e1bdf51e130_146004_120x120_fill_box_smart1_3.png","permalink":"https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/","title":"CMap based Drug Repositioning of BTZ to reverse the metastatic effect of GALNT14 in lung cancer"},{"content":"ICML, \u0026lsquo;17\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nSummary MAML is a general and model-agnostic algorithm that can be directly applied to a model trained with gradient descent procedure. MAML does not expand the number of learned parameters. MAML does not place constraints on the model architecture. Keywords Model agnostic Fast adaptation Optimization based approach Learning good model parameters Preliminaries Common Approaches of Meta-Learning and MAML\nMAML is one of the most influential model of optimization-based approaches.\nA few terminologies of meta-learning problems\nIntroduction Goal of ideal artificial agent:\nLearning and adapting quickly from only a few examples.\nTo do so, an agent must..\nIntegrate its prior experience with a small amount of new information. Avoid overfitting to the new data. → Meta-learning has same goals.\nMAML:\n\u0026ldquo;The key idea of MAML is to train the model\u0026rsquo;s initial parameters such that the model has maximal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task.\u0026rdquo;\nLearning process of MAML:\nMAML maximizes the sensitivity of the loss functions of new tasks.\nAuthors demonstrated the algorithm on three different model types.\nFew-shot regression Image classification Reinforcement learning 2. Model-Agnostic Meta Learning 2.1. Meta-Learning Problem Set-Up To apply MAML to a variety of learning problems, authors introduce a generic notion of a learning task:\n$\\mathcal{T} = { \\mathcal{L}(\\mathbf{x}_1, \\mathbf{a}_1, \u0026hellip;, \\mathbf{x}_H, \\mathbf{a}_H), q(\\mathbf{x}_1), q(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t), H }$\nEach task $\\mathcal{T}$ consists of..\n$\\mathcal{L}$: a loss function, might be misclassification loss or a cost function in a Markov decision process\n$q(\\mathbf{x}_1)$: a distribution over initial observations\n$q(\\mathbf{x}_{t+1}|\\mathbf{x}_t , \\mathbf{a}_t)$: a transition distribution\n$H$: an episode length(e.g. in i.i.d. supervised learning problems, the length $H = 1$.)\nAuthors consider a distribution over tasks $p(\\mathcal{T})$\nMeta-training:\nA new task $\\mathcal{T}_i$ is sampled from $p(\\mathcal{T})$.\nThe model is trained with only $K$ samples drawn from $q_i$.\nLoss $\\mathcal{L}_{\\mathcal{T}_i}$ is calculated and feedbacked to model.\nModel $f$ is tested on new samples from $\\mathcal{T}_i$.\nThe model $f$ is then improved by considering how the $test$ error on new data from $q_i$ changes with respect to the parameters.\n2.2. A Model-Agnostic Meta-Learning Algorithm Intuition: Some internal representations are more transferrable than others. How can we encourage the emergence of such general-purpose representations?\nA model $f_\\theta$ has paramters $\\theta$.\nFor each task $\\mathcal{T}_i$, $f_\\theta$\u0026rsquo;s parameters $\\theta$ become $\\theta_i\u0026rsquo;$.\nAlgorithm\ncf) Terminologies for below description(temporarily defined by JH Gu)\nDivide tasks\nSeparate tasks into meta-training task set(${\\mathcal{T}_i^{\\text{tr}}}$) and meta-test task set(${\\mathcal{T}_i^{\\text{test}}}$).\n(We can think of ${\\mathcal{T}_i^{\\text{tr}}}$ as monthly tests(모의고사), and ${\\mathcal{T}_i^{\\text{test}}}$ as annual tests(수능))\nFor each task, divide each samples into $\\mathcal{D}_{\\mathcal{T}_i}^{\\text{study}}$(task-specific samples for studying, also called as support set), $\\mathcal{D}_{\\mathcal{T_i}}^{\\text{check}}$(task-specific samples for checking, also called as query set)\n(We can think of $\\mathcal{D}_{\\mathcal{T}_i}^{\\text{study}}$ as 필수예제 in 수학의 정석, and $\\mathcal{D}_{\\mathcal{T}_i}^{\\text{check}}$ as 연습문제 in 수학의 정석)\nMeta-training using meta-training task set ${\\mathcal{T}_i^{\\text{tr}}}$\nInner loop(task-specific $K$-shot learning)\nFor each $\\mathcal{T}_i$ in ${\\mathcal{T}_i^{\\text{tr}}}$, a new parameter $\\theta_i\u0026rsquo;$ is created.\nEach $\\theta_i\u0026rsquo;$ is initialized as $\\theta$.\nWith task-specific samples for studying($\\mathcal{D}_{\\mathcal{T}_i^{\\text{tr}}}^{\\text{study}}$), each $\\theta_i\u0026rsquo;$ is updated by:\n$$ \\theta_i\u0026rsquo; = \\theta - \\alpha \\nabla_\\theta \\mathcal{L}_{\\mathcal{T}_i}(f_\\theta) $$\nOuter loop(meta-learning across tasks)\nWith task-specific samples for checking($\\mathcal{D}_{\\mathcal{T_i}^{\\text{tr}}}^{\\text{check}}$), $\\theta$ is updated by:\n$$ \\theta = \\theta - \\beta \\nabla_\\theta \\sum_{\\mathcal{T}_i \\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_i} (f_{\\theta_i\u0026rsquo;}) $$\ncf) second-order derivative(Hessian) problem\nThe MAML meta-gradient update(outer loop) involves a gradient through a gradient, which can be resource-intensive. This requires an additional backward pass through $f$ to compute Hessian vector products.\n$$ \\begin{align*} \\textcolor{red}{\\theta\u0026rsquo;} \u0026amp;= \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta) \\ \\nabla_\\theta \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;}) \u0026amp;= (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\cdot (\\nabla_\\theta \\textcolor{red}{\\theta\u0026rsquo;}) \\ \u0026amp;= (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\cdot (\\nabla_\\theta (\\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta)) \\ \u0026amp;\\approx (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\cdot (\\nabla_\\theta \\theta) \\ \u0026amp;= (\\textcolor{red}\\nabla_{\\textcolor{red}{\\theta\u0026rsquo;}} \\mathcal{L}(\\textcolor{red}{\\theta\u0026rsquo;})) \\end{align*} $$\nAuthors included a comparison to drop the backward pass term and using just the first-order approximation, which showed not much difference.\nMeasure model performance using meta-test task set ${\\mathcal{T}_i^{\\text{test}}}$\nFor each $\\mathcal{T}_i$ in ${\\mathcal{T}_i^{\\text{test}}}$, adjust task-specific parameters with $\\mathcal{D}_{\\mathcal{T}_i^{\\text{test}}}^{\\text{study}}$. Test the performance with $\\mathcal{D}_{\\mathcal{T_i}^{\\text{test}}}^{\\text{check}}$. 3. Species of MAML 3.1. Supervised Regression and Classification Algorithm\nFormalizing supervised regression and classification\nHorizon $H = 1$ Drop the timestep subscript on $\\mathbf{x}_t$ (since model accepts a single input and produces a single output) The task $\\mathcal{T}_i$ generates $K$ i.i.d. observations $\\mathbf{x}$ from $q_i$ Task loss is represented by the error between the model\u0026rsquo;s output for $\\mathbf{x}$ and the corresponding target values $\\mathbf{y}$. Loss functions\nMSE for regression\n$$ \\mathcal{L}_{\\mathcal{T}_i}(f_\\phi) = \\sum_{\\mathbf{x}^{(j)}, \\mathbf{y}^{(j)} \\sim \\mathcal{T}_i} | f_\\phi(\\mathbf{x}^{(j)}) - \\mathbf{y}^{(j)}|^2_2 $$\nCross entropy loss for discrete classification\n$$ \\mathcal{L}_{\\mathcal{T}_i}(f_\\phi) = \\sum_{\\mathbf{x}^{(j)}, \\mathbf{y}^{(j)} \\sim \\mathcal{T}_i} \\big\\{ \\mathbf{y}^{(j)} \\log f_\\phi (\\mathbf{x}^{(j)}) - (1-\\mathbf{y}^{(j)})\\log(1-f_\\phi(\\mathbf{x}^{(j)}) \\big\\} $$\n3.2. Reinforcement Learning Algorithm\nGoal of MAML in RL:\nQuickly acquire a policy for a new test task using only a small amount of experience in the test setting.\nFormalizing RL\nEach RL task $\\mathcal{T}_i$ contains..\nInitial state distribution $q_i(\\mathbf{x}_1)$ Transition distribution $q_i(\\mathbf{x}_{t+1}|\\mathbf{x}_t, \\mathbf{a}_t)$ $\\mathbf{a}_t$: action Loss $\\mathcal{L}_{\\mathcal{T}_i}$, which corresponds to the negative reward function $R$ Therefore, entire task is a Markov decision process(MDP) with horizon $H$\nThe model being learned, $f_\\theta$, is a policy that maps from states $\\mathbf{x}_t$ to a distribution over actions $\\mathbf{a}_t$ at each timestep $t \\in { 1, \u0026hellip;, H}$\nLoss function for task $\\mathcal{T}_i$ and model $f_\\phi$:\n$$ \\mathcal{L}_{\\mathcal{T}_i}(f_\\phi) = -\\mathbb{E}_{\\mathbf{x}_t, \\mathbf{a}_t \\sim f_\\phi, q_{\\mathcal{T}_i}} \\bigg [ \\sum_{t=1}^H R_i(\\mathbf{x}_t, \\mathbf{a}_t) \\bigg ] $$\nPolicy gradient method\nSince the expected reward is generally not differentiable due to unknown dynamics, authors used policy gradient methods to estimate the gradient.\nThe policy gradient method is an on-policy algorithm\n→ There are additional sampling procedures in step 5 and 8.\n4. Comparison with related works Comparison with other popular approaches\nTraining a meta-learner that learns how to update the parameters of the learner\u0026rsquo;s model\nex) On the optimization of a synaptic learning rule(Bengio et al. 1992)\n→ Requires additional parameters, while MAML does not.\nTraining to compare new examples in a learned metric space\nex) Siamese networks(Koch, 2015), recurrence with attention mechanisms(Vinyals et al. 2016)\n→ Difficult to directly extend to our problems, such as reinforcement learning.\nTraining memory-augmented models\nex) Meta-learning with memory-augmented neural networks(Santoro et al. 2016)\nThe recurrent learner is trained to adapt to new tasks as it is rolled out.\n→ Not really straightforward.\n5. Experimental Evaluation Three questions\nCan MAML enable fast learning of new tasks? Can MAML be used for meta-learning in multiple different domains? Can a model learned with MAML continue to improve with additional gradient updates and/or examples? 5.1. Regression 5.2. Classification 5.3. Reinforcement Learning References KAIST NeuroAI JC_#1 Meta Learning (편집본)\nhttps://ai.stanford.edu/~cbfinn/_files/dissertation.pdf\n[10주차] (MAML) Model-agnostic Meta Learning for Fast Adaptation of Deep Networks 논문 리뷰\nMeta-Learning: Learning to Learn Fast\n","date":"2021-09-15T00:00:00Z","image":"https://gujh14.github.io/thumbnail.png","permalink":"https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/","title":"MAML: Model-Agnostic Meta -Learning for Fast Adaptation of Deep Networks"}]