<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>JH Gu&#39;s Tech Blog</title>
        <link>https://gujh14.github.io/</link>
        <description>Recent content on JH Gu&#39;s Tech Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 01 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gujh14.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Useful Resources for Molecule Learning (in Progress)</title>
        <link>https://gujh14.github.io/p/useful-resources-for-molecule-learning-in-progress/</link>
        <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/useful-resources-for-molecule-learning-in-progress/</guid>
        <description>&lt;h2 id=&#34;m2d2&#34;&gt;M2D2&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://m2d2.io/talks/m2d2/about/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;m2d2.io&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M2D2 is an abbreviation for Molecular Modeling &amp;amp; Drug Discovery. This is a community is co-organized by Valence Discovery and Mila - Quebec AI Institute.&lt;/li&gt;
&lt;li&gt;New presentation comes up every Tuesday via Zoom. Authors of recent papers introduce their work themselves and answers questions. Most of the sessions are recorded and uploaded on Youtube.&lt;/li&gt;
&lt;li&gt;Most papers have been accepted at top conferences.&lt;/li&gt;
&lt;li&gt;Highly recommend to watch those videos.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hugging-face&#34;&gt;Hugging Face&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/blog/intro-graphml&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;huggingface.co/blog/intro-graphml&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This blog article is a tutorial about machine learning techniques for Graphs.&lt;/li&gt;
&lt;li&gt;Blog is not tailor-made for molecules, but substantially introduced.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking</title>
        <link>https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/</link>
        <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/</guid>
        <description>&lt;p&gt;ICLR, &amp;lsquo;23&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.01776&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail.png&#34;
	width=&#34;1110&#34;
	height=&#34;534&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail_hu35359f9550b3436da89f85bd47cc7f53_260405_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail_hu35359f9550b3436da89f85bd47cc7f53_260405_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;498px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This article is one of the first research work that formulated molecular docking as a generative problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Showed very interesting results with decent performance gain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are interested in molecular docking and diffusion models, this is definitely a must-read paper!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is highly recommended to watch &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=gAmTGw601dA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;youtube video&lt;/a&gt; explained by the authors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Molecular docking as a generative problem, not regression!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Problem of learning a distribution over ligand poses conditioned on the target protein structure $p(\mathbf{x} | \mathbf{y})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Used “Diffusion process” for generation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two separate model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Score model: $s(\mathbf{x}, \mathbf{y}, t)$&lt;/p&gt;
&lt;p&gt;Predicts score based on ligand pose $\mathbf{x}$, protein structure $\mathbf{y}$, and timestep $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confidence model: $d(\mathbf{x}, \mathbf{y})$&lt;/p&gt;
&lt;p&gt;Predicts whether the ligand pose has RMSD below 2Å compared to ground truth ligand pose&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diffusion on Product space $\mathbb{P}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduced degrees of freedom $3n \rightarrow (m+6)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;h3 id=&#34;molecular-docking&#34;&gt;Molecular Docking&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled.png&#34;
	width=&#34;1942&#34;
	height=&#34;1116&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled_huf40d7e8c2d3df3da9d828794a85e8161_935838_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled_huf40d7e8c2d3df3da9d828794a85e8161_935838_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;417px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Definition:&lt;/p&gt;
&lt;p&gt;Predicting the position, orientation, and conformation of a ligand when bound to a target protein&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two types of tasks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Known-pocket docking
&lt;ul&gt;
&lt;li&gt;Given: position of the binding pocket&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Blind docking
&lt;ul&gt;
&lt;li&gt;More general setting: no prior knowledge about binding pocket&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;previous-works-search-based--regression-based&#34;&gt;Previous works: Search-based / Regression-based&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1.png&#34;
	width=&#34;1462&#34;
	height=&#34;1104&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1_huc1cd3bae2f7177be1a7cfda9b4564a85_264966_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1_huc1cd3bae2f7177be1a7cfda9b4564a85_264966_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;317px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Search based docking methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Traditional methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consist of parameterized physics-based &lt;strong&gt;scoring function&lt;/strong&gt; and a &lt;strong&gt;search algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scoring function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input: 3D structures&lt;/li&gt;
&lt;li&gt;Output: estimate of the quality/likelihood of the given pose&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stochastically modifies the ligand pose (position, orientation, torsion angles)&lt;/li&gt;
&lt;li&gt;Goal: finding the global optimum of the scoring function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML has been applied to parameterize the scoring function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But very computationally expensive (large search space)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2.png&#34;
	width=&#34;1776&#34;
	height=&#34;166&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2_huf43150a0e47223ef3cf22aed6633617b_81428_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2_huf43150a0e47223ef3cf22aed6633617b_81428_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1069&#34;
		data-flex-basis=&#34;2567px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression based methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Recent deep learning method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Significant speedup compared to search based methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;No improvements in accuracy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3.png&#34;
	width=&#34;1152&#34;
	height=&#34;144&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3_hu6b088dca52a85205018e5f1b36628350_49655_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3_hu6b088dca52a85205018e5f1b36628350_49655_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;800&#34;
		data-flex-basis=&#34;1920px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.05146&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;EquiBind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4.png&#34;
	width=&#34;2030&#34;
	height=&#34;550&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4_hu663897c2650aaacc73297b198e4cc5b3_753604_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4_hu663897c2650aaacc73297b198e4cc5b3_753604_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;369&#34;
		data-flex-basis=&#34;885px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tried to tackle the blind docking task as a &lt;strong&gt;regression problem&lt;/strong&gt; by directly predicting pocket keypoints on both ligand and protein and aligning them.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.biorxiv.org/content/10.1101/2022.06.06.495043v3&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TANKBind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5.png&#34;
	width=&#34;1988&#34;
	height=&#34;732&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5_hub1f86d8413b43196d2dbec69310f8555_603864_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5_hub1f86d8413b43196d2dbec69310f8555_603864_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;271&#34;
		data-flex-basis=&#34;651px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improved over this by independently predicting a docking pose for each possible pocket and then ranking them.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.06069&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;E3Bind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6.png&#34;
	width=&#34;1536&#34;
	height=&#34;562&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6_hu13edc0df2b71d59ba0b45cb41de53f20_193128_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6_hu13edc0df2b71d59ba0b45cb41de53f20_193128_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used ligand-constrained &amp;amp; protein-constrained update layer to embed ligand atoms and iteratively updated coordinates.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;docking-objective&#34;&gt;Docking objective&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7.png&#34;
	width=&#34;1520&#34;
	height=&#34;1214&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7_hu5f4844bc8df901aef67687f97d63b662_915257_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7_hu5f4844bc8df901aef67687f97d63b662_915257_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;125&#34;
		data-flex-basis=&#34;300px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard evaluation metric:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mathcal{L}_{\epsilon} = \sum_{x, y} I_{\text{RMSD}(y, \hat{y}(x))&amp;lt;\epsilon}$:&lt;/p&gt;
&lt;p&gt;proportion of predictions with $\text{RMSD} &amp;lt; \epsilon$ → Not differentiable!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instead, we use $\text{argmin}_{\hat{y}} \lim_{\epsilon \rightarrow 0} \mathcal{L}_\epsilon$ as objective function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regression is suitable for docking only if it is unimodal.&lt;/li&gt;
&lt;li&gt;Docking has significant aleatoric (irreducible) &amp;amp; epistemic (reducible) uncertainty
&lt;ul&gt;
&lt;li&gt;Regression methods will minimize $\sum \|y - \hat{y}\|^2_2$ → will produce weighted mean of multiple modes&lt;/li&gt;
&lt;li&gt;On the other hand, generative model will populate all/most modes!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8.png&#34;
	width=&#34;2028&#34;
	height=&#34;1108&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8_hu6b313aafde63670bc308ca51bccee486_1187222_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8_hu6b313aafde63670bc308ca51bccee486_1187222_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;439px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression (EquiBind) model set conformer in the middle of the modes.&lt;/li&gt;
&lt;li&gt;Generative samples can populate conformer in most modes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9.png&#34;
	width=&#34;2002&#34;
	height=&#34;1090&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9_hu04edc1aff53bfad5a2de658c9efbf1ee_1153872_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9_hu04edc1aff53bfad5a2de658c9efbf1ee_1153872_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;440px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Much less steric clashes for generative models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;diffdock-overview&#34;&gt;DiffDock Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15.png&#34;
	width=&#34;1110&#34;
	height=&#34;534&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15_hu35359f9550b3436da89f85bd47cc7f53_260405_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15_hu35359f9550b3436da89f85bd47cc7f53_260405_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;498px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two-step approach
&lt;ul&gt;
&lt;li&gt;Score model: Reverse diffusion over translation, rotation, and torsion&lt;/li&gt;
&lt;li&gt;Confidence model: Predict whether or not each ligand pose is $\text{RMSD} &amp;lt; 2\text{Å}$ compared to ground truth ligand pose&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;score-model&#34;&gt;Score model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ligand pose: $\mathbb{R}^{3n}$ ($n$: number of atoms)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But molecular docking needs far less degrees of freedom.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16.png&#34;
	width=&#34;2044&#34;
	height=&#34;1074&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16_hu38cf56affd34313fb9003ca22c483d35_693157_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16_hu38cf56affd34313fb9003ca22c483d35_693157_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;456px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Reduced degree of freedom: $(m+6)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Local structure: Fixed (rigid) after conformer generation with RDKit &lt;code&gt;EmbedMolecule(mol)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bond length, angles, small rings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Position (translation): $\mathbb{R}^3$ - 3D vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17.png&#34;
	width=&#34;698&#34;
	height=&#34;394&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17_hubaace3e56e7acf27bdb1a448af48eb5b_40034_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17_hubaace3e56e7acf27bdb1a448af48eb5b_40034_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;425px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Orientation (rotation): $SO(3)$ - three Euler angle vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a.gif&#34;
	width=&#34;340&#34;
	height=&#34;322&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a_hu98bbb1e40ff0f96440af8978741eb4d9_406254_480x0_resize_box_1.gif 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a_hu98bbb1e40ff0f96440af8978741eb4d9_406254_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Euler2a.gif&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;105&#34;
		data-flex-basis=&#34;253px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Torsion angles: $\mathbb{T}^m$ ($m$: number of rotatable bonds)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim.gif&#34;
	width=&#34;185&#34;
	height=&#34;200&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim_hua8427fe8cdaef099affccff0d8af866f_448496_480x0_resize_box_1.gif 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim_hua8427fe8cdaef099affccff0d8af866f_448496_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Dihedral-angles-anim.gif&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;222px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can perform diffusion on product space $\mathbb{P}: \mathbb{R}^3 \times SO(3) \times \mathbb{T}^m$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For a given seed conformation $\mathbf{c}$, the map $A(\cdot, \mathbf{c}): \mathbb{P} \rightarrow \mathcal{M}_\mathbf{c}$ is a bijection!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18.png&#34;
	width=&#34;2618&#34;
	height=&#34;1122&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18_huf848a016eb7455cba5e115c7211e6f55_618737_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18_huf848a016eb7455cba5e115c7211e6f55_618737_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;560px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;confidence-model&#34;&gt;Confidence Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19.png&#34;
	width=&#34;1740&#34;
	height=&#34;356&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19_hud7e426523596c6c1031c9a3d319cc3e4_305102_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19_hud7e426523596c6c1031c9a3d319cc3e4_305102_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;488&#34;
		data-flex-basis=&#34;1173px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generative model can sample an arbitrary number of poses, but researchers are interested in one or a fixed number of them.&lt;/li&gt;
&lt;li&gt;Confidence predictions are very useful for downstream tasks.&lt;/li&gt;
&lt;li&gt;Confidence model $d(\mathbf{x}, \mathbf{y})$
&lt;ul&gt;
&lt;li&gt;$\mathbf{x}$: pose of a ligand&lt;/li&gt;
&lt;li&gt;$\mathbf{y}$: target protein structure&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Samples are ranked by score and the score of the best is used as overall confidence score.&lt;/li&gt;
&lt;li&gt;Training &amp;amp; Inference
&lt;ul&gt;
&lt;li&gt;Ran the trained diffusion model to obtain a set of candidate poses for every training example and generate binary labels: each pose has RMSD below $2 \text{Å}$ or not.&lt;/li&gt;
&lt;li&gt;Then the confidence model is trained with cross entropy loss to predict the binary label for each pose.&lt;/li&gt;
&lt;li&gt;During inference, diffusion model is run to generate $N$ poses in parallel, and passed to the confidence model that ranks them based on its confidence that they have RMSD below $2\text{Å}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;diffdock-workflow&#34;&gt;DiffDock Workflow&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20.png&#34;
	width=&#34;1786&#34;
	height=&#34;706&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20_huf37ccbd6cea1d3a5450b7473806471ee_503096_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20_huf37ccbd6cea1d3a5450b7473806471ee_503096_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;607px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;diffdock-results&#34;&gt;DiffDock Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Standard benchmark: PDBBind&lt;/p&gt;
&lt;p&gt;19k experimentally determined structures of small molecules + proteins&lt;/p&gt;
&lt;p&gt;Baselines: search-based &amp;amp; deep learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction correctness&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21.png&#34;
	width=&#34;1476&#34;
	height=&#34;782&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21_hu6492e850775254853f2253df277369e5_216895_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21_hu6492e850775254853f2253df277369e5_216895_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;452px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Outperform search-based, deep learning, and pocket prediction + search-based methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22.png&#34;
	width=&#34;1380&#34;
	height=&#34;776&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22_hu1e9d177239c85ffcc71145b4c82f01a0_199858_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22_hu1e9d177239c85ffcc71145b4c82f01a0_199858_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;3 times faster than the most accurate baseline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to unseen receptors&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23.png&#34;
	width=&#34;1494&#34;
	height=&#34;784&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23_hu6b0a4098791f8fb33cad44759483ab95_176122_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23_hu6b0a4098791f8fb33cad44759483ab95_176122_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;457px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Able to generalize: outperform classical method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reverse diffusion process GIF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock.gif&#34;
	width=&#34;500&#34;
	height=&#34;282&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock_huf4cb667dc82f8b11af86b654f4b03337_2904312_480x0_resize_box_1.gif 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock_huf4cb667dc82f8b11af86b654f4b03337_2904312_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;425px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confidence score quality&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24.png&#34;
	width=&#34;1154&#34;
	height=&#34;768&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24_hu647c20d3e720e07563131b84bd083487_321886_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24_hu647c20d3e720e07563131b84bd083487_321886_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;High selective accuracy: valuable information for practitioners&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;personal-opinions&#34;&gt;Personal opinions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It is impressive that the authors formulated molecular docking as a generative problem, conditioned on protein structure.&lt;/li&gt;
&lt;li&gt;But it is not an end-to-end approach. And there are some discrepancy between the inputs and output of the confidence model. The input is the predicted ligand pose $\hat{\mathbf{x}}$ and protein structure $\mathbf{y}$, but the output is “whether the RMSE is below 2Å between predicted ligand pose $\hat{\mathbf{x}}$ and ground truth ligand pose $\mathbf{x}$”.&lt;/li&gt;
&lt;li&gt;There are quite a room to improve the performance, but it requires heavy workloads of GPUs.&lt;/li&gt;
&lt;li&gt;I’m skeptical about the generalizability of this model since there are almost no physics informed inductive bias in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Article&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=kKF8_K-mBbS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Youtube&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/gAmTGw601dA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Blog&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://yang-song.net/blog/2021/score/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What are Diffusion Models?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Graph Representation Learning for Drug Discovery</title>
        <link>https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/</link>
        <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/</guid>
        <description>&lt;p&gt;LoG conference, &amp;lsquo;22,&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/live/ouuxi5uW-JA?feature=share&amp;amp;t=8477&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LoG conference Day 4 Youtube&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/thumbnail.png&#34;
	width=&#34;1458&#34;
	height=&#34;804&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/thumbnail_hu0d4bc15913e42a594fe5b478b080476a_169216_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/thumbnail_hu0d4bc15913e42a594fe5b478b080476a_169216_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;435px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;lecturer-djork-arné-clevert&#34;&gt;Lecturer: Djork-Arné Clevert&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2022.05.19~09.22: Bayer Pharma, Head of Machine Learning Research (All work presented today were published during Bayer.)&lt;/li&gt;
&lt;li&gt;2022.10.22~: Pfizer, Head of Machine Learning Research&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Most ML methods are focused on early drug discovery part.&lt;/li&gt;
&lt;li&gt;Major applications of ML in drug discovery include:
&lt;ul&gt;
&lt;li&gt;ADMET modeling&lt;/li&gt;
&lt;li&gt;Representation learning&lt;/li&gt;
&lt;li&gt;Conditional &lt;em&gt;de novo&lt;/em&gt; hit design&lt;/li&gt;
&lt;li&gt;Inverse molecule modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;drug-discovery-vs-drug-development&#34;&gt;Drug Discovery vs Drug Development&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled.png&#34;
	width=&#34;1772&#34;
	height=&#34;970&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled_hua64a52a27cd2ec95511902cc0e2eb6a9_979997_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled_hua64a52a27cd2ec95511902cc0e2eb6a9_979997_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;438px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Drug discovery: the early part (Hit identification ~ Pre-clinical phase)&lt;/li&gt;
&lt;li&gt;Drug development: the later part (Clinical trials phase 1 ~ Phase 4)&lt;/li&gt;
&lt;li&gt;Most ML research is focused on the &lt;strong&gt;Drug discovery&lt;/strong&gt; part, since there is a larger quantity of data available that is more convenient to input into a computer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;background-bioactivity--admet-modeling&#34;&gt;Background bioactivity / ADMET modeling&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled1.png&#34;
	width=&#34;1738&#34;
	height=&#34;814&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled1_hudbb6af4119d46068c7258fb6b3cff36e_759561_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled1_hudbb6af4119d46068c7258fb6b3cff36e_759561_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;213&#34;
		data-flex-basis=&#34;512px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bioactivity modeling have been used since the 1960s.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://www.mdpi.com/1420-3049/25/1/44&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Modeling Physico-Chemical ADMET Endpoints with Multitask Graph Convolutional Networks&lt;/a&gt; (Molecules, 2019)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled2.png&#34;
	width=&#34;988&#34;
	height=&#34;526&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled2_hu649375d0013ae572ab7fc69e5611002e_317129_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled2_hu649375d0013ae572ab7fc69e5611002e_317129_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;187&#34;
		data-flex-basis=&#34;450px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multitask GCN for modeling physico-chemical properties.&lt;/li&gt;
&lt;li&gt;Performed a molecular property prediction with GCN in multi-task setting.&lt;/li&gt;
&lt;li&gt;This is an early work in this field using GNNs to predict properties.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2105.04854&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Improving Molecular GCNs Explainability with Orthonormality and Sparsity&lt;/a&gt; (ICML, 2021)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled3.png&#34;
	width=&#34;730&#34;
	height=&#34;270&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled3_hu67cdd698a9b14f619c5bc18bdc6fa78a_106175_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled3_hu67cdd698a9b14f619c5bc18bdc6fa78a_106175_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;270&#34;
		data-flex-basis=&#34;648px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled4.png&#34;
	width=&#34;1072&#34;
	height=&#34;536&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled4_hu5a470f2e0dbe53944268ff17baac79fd_159224_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled4_hu5a470f2e0dbe53944268ff17baac79fd_159224_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;200&#34;
		data-flex-basis=&#34;480px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Proposed two regularization techniques to improve the accuracy and explainability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Batch Representation Orthonormalization (BRO)&lt;/p&gt;
&lt;p&gt;→ encourages graph convolution operations to generate orthonormal node embeddings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gini regularization&lt;/p&gt;
&lt;p&gt;→ applied to the weights of the output layer and constrains the number of dimensions the model can use to make predictions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explainability results&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled5.png&#34;
	width=&#34;1036&#34;
	height=&#34;376&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled5_hu77aafb4785149c07b72e5319e0c6bbb8_284430_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled5_hu77aafb4785149c07b72e5319e0c6bbb8_284430_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;275&#34;
		data-flex-basis=&#34;661px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=kv4xUo5Pu6&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Representation Learning on Biomolecular Structures using Equivariant Graph Attention&lt;/a&gt; (LoG, 2022)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled6.png&#34;
	width=&#34;916&#34;
	height=&#34;504&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled6_hu8922ffb1f267cd2627e49ecaa8b65e6a_100926_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled6_hu8922ffb1f267cd2627e49ecaa8b65e6a_100926_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;436px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let’s not focus only on invariant feature, but on &lt;strong&gt;equivariant&lt;/strong&gt; feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;EQGAT operates with Cartesian coordinates to incorporate directionality and is implemented with a novel attention mechanism, acting as a content and spatial dependent filter when propagating information between nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Performed well on large biomolecule dataset (ATOM3D), and it is efficient.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled7.png&#34;
	width=&#34;1334&#34;
	height=&#34;452&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled7_hu1d3d9c5ac687ab432cca3fa8ea0ddc8c_164144_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled7_hu1d3d9c5ac687ab432cca3fa8ea0ddc8c_164144_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;295&#34;
		data-flex-basis=&#34;708px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-diversity-of-data-in-drug-discovery&#34;&gt;The Diversity of Data in Drug Discovery&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled8.png&#34;
	width=&#34;1896&#34;
	height=&#34;954&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled8_hu8b0a0e6d6568a7fc43f2031f929b4b79_2304778_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled8_hu8b0a0e6d6568a7fc43f2031f929b4b79_2304778_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;476px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are many types of data in molecule domain, including various spectrometry data, graph, sequence, image, 3D point clouds, …&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;molecular-representations-for-drug-discovery&#34;&gt;Molecular Representations for Drug Discovery&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc04175j&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations&lt;/a&gt; (Chemical Science, 2019)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled9.png&#34;
	width=&#34;391&#34;
	height=&#34;186&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled9_hu32ad2bb6d680ca9c51bc225e7623fb8d_32073_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled9_hu32ad2bb6d680ca9c51bc225e7623fb8d_32073_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;504px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To learn molecule representation, authors made a autoencoder model that translates any SMILES into canonical SMILES. It was good for predicting downstream tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Performance results&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled10.png&#34;
	width=&#34;813&#34;
	height=&#34;439&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled10_huba82b51e21959611fb45dd12d2c351d5_104101_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled10_huba82b51e21959611fb45dd12d2c351d5_104101_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;444px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://pubs.rsc.org/en/content/articlelanding/2019/sc/c9sc01928f&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Efficient multi-objective molecular optimization in a continuous latent space&lt;/a&gt; (Chemical Science, 2019)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled11.png&#34;
	width=&#34;1892&#34;
	height=&#34;944&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled11_hudad0f192432291e5f773df0db2d70c8e_873804_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled11_hudad0f192432291e5f773df0db2d70c8e_873804_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;200&#34;
		data-flex-basis=&#34;481px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This method takes a starting compound as input and proposes new molecules with more desirable (predicted) properties.&lt;/li&gt;
&lt;li&gt;Objective function combines multiple &lt;em&gt;in silico&lt;/em&gt; prediction models, defined desirability ranges and substructure constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conditional-molecular-de-novo-hit-design&#34;&gt;Conditional Molecular de novo Hit Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://www.nature.com/articles/s41467-019-13807-w&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;De novo generation of hit-like molecules from gene expression signatures using artificial intelligence&lt;/a&gt; (Nature Communications, 2020)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled12.png&#34;
	width=&#34;1848&#34;
	height=&#34;790&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled12_hu658b0b96fa629f553c4f90069980bcb2_578254_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled12_hu658b0b96fa629f553c4f90069980bcb2_578254_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;561px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A generative model (GAN) that bridges systems biology and molecular design, conditioning a generative adversarial network with transcriptomic data.&lt;/li&gt;
&lt;li&gt;When the model is provided with the desired state of gene expression signature, it is able to design active-like molecules for desired targets without any previous target annotation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00081d&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cell morphology-guided &lt;em&gt;de novo&lt;/em&gt; hit design by conditioning GANs on phenotypic image features&lt;/a&gt; (Digital Discovery, 2023)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled13.png&#34;
	width=&#34;378&#34;
	height=&#34;139&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled13_hub33e4072a68c13ca0fa58f7bd15eb1ef_30956_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled13_hub33e4072a68c13ca0fa58f7bd15eb1ef_30956_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;271&#34;
		data-flex-basis=&#34;652px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled14.png&#34;
	width=&#34;969&#34;
	height=&#34;400&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled14_hua12dd3e019d28bbe3d9a2f4945050a88_185135_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled14_hua12dd3e019d28bbe3d9a2f4945050a88_185135_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;581px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Utilized cellular morphology (cell painting morphological profiles) to directly guide the de novo design of small molecules. Authors used conditional GAN as a generative model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inverse-molecular-problems&#34;&gt;Inverse Molecular problems&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled15.png&#34;
	width=&#34;1750&#34;
	height=&#34;872&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled15_hubb543cc87a122c441e1bec25137b29dc_452957_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled15_hubb543cc87a122c441e1bec25137b29dc_452957_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;200&#34;
		data-flex-basis=&#34;481px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Is it possible to inverse fingerprint, molecular depiction, or resonance spectrum into a molecule structure?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://pubs.rsc.org/en/content/articlelanding/2020/sc/d0sc03115a&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neuraldecipher – reverse-engineering extended-connectivity fingerprints (ECFPs) to their molecular structures&lt;/a&gt; (Chemical Science, 2020)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled16.png&#34;
	width=&#34;1804&#34;
	height=&#34;832&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled16_hu5e25f4e2b0c931956509d3146a825ff5_919120_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled16_hu5e25f4e2b0c931956509d3146a825ff5_919120_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since ECFP representation is made with hash function, they are often non-invertible.&lt;/li&gt;
&lt;li&gt;Neuraldecipher is a neural net model that predicts a compact vector representation of compounds, given ECFPs.&lt;/li&gt;
&lt;li&gt;Then utilize another pre-trained model to retrieve the molecular structure as SMILES representation.&lt;/li&gt;
&lt;li&gt;This model were able to correctly deduce up to 69% of molecular structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc01839f&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Img2Mol – accurate SMILES recognition from molecular graphical depictions&lt;/a&gt; (Chemical Science, 2021)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled17.png&#34;
	width=&#34;1890&#34;
	height=&#34;858&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled17_huda5efde8e717cf3bfcb320e9ae4d2b02_786847_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled17_huda5efde8e717cf3bfcb320e9ae4d2b02_786847_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;528px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This model use CNN for molecule depictions and a pre-trained decoder that translates the latent representation into the SMILES representation of the molecules.&lt;/li&gt;
&lt;li&gt;Img2Mol was able to correctly translate up to 88% of the molecular depictions into SMILES.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;learning-graph-level-representation&#34;&gt;Learning Graph level representation&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled18.png&#34;
	width=&#34;1768&#34;
	height=&#34;670&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled18_hua8dda04f02b4954a8c1192f6974d7ca5_446734_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled18_hua8dda04f02b4954a8c1192f6974d7ca5_446734_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;633px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;[Research article] &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2104.09856&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning&lt;/a&gt; (NeurIPS, 2021, Poster)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled19.png&#34;
	width=&#34;1744&#34;
	height=&#34;906&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled19_huc674cc97c1269f6580191657e3776c67_631068_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled19_huc674cc97c1269f6580191657e3776c67_631068_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;192&#34;
		data-flex-basis=&#34;461px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled20.png&#34;
	width=&#34;1822&#34;
	height=&#34;844&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled20_hued4e9253d969d40a6d007bf2c75af722_710121_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled20_hued4e9253d969d40a6d007bf2c75af722_710121_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;518px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled21.png&#34;
	width=&#34;1810&#34;
	height=&#34;824&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled21_hu96f0312f1590e881ca0db88f95ac34cf_411416_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-representation-learning-for-drug-discovery/imgs/Untitled21_hu96f0312f1590e881ca0db88f95ac34cf_411416_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;219&#34;
		data-flex-basis=&#34;527px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph representation is highly complexed, which can be represented by $(\#\text{ nodes})!$ equivalent adjacency matrices.&lt;/li&gt;
&lt;li&gt;This model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching.&lt;/li&gt;
&lt;li&gt;Showed promising results in graph classification, generation, clustering, interpolation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;personal-opinion&#34;&gt;Personal Opinion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It was interesting to see what kinds of research is being performed in big pharmas.&lt;/li&gt;
&lt;li&gt;Big pharmas seem to be more interested in applying ML methods on small problems than creating state-of-the-arts techniques.&lt;/li&gt;
&lt;li&gt;Inverse molecular modeling seems interesting for me.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/live/ouuxi5uW-JA?feature=share&amp;amp;t=8477&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.youtube.com/live/ouuxi5uW-JA?feature=share&amp;t=8477&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Neighborhood-aware Scalable Temporal Network Representation Learning</title>
        <link>https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/</link>
        <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/</guid>
        <description>&lt;p&gt;LoG, &amp;lsquo;22&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.01084&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neighborhood-aware Scalable Temporal Network Representation Learning&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail.png&#34;
	width=&#34;914&#34;
	height=&#34;388&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail_hu242d3a243b2ed8387d2472cbaa72b75a_149908_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail_hu242d3a243b2ed8387d2472cbaa72b75a_149908_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;565px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There are few models to infer network-scientific properties on time-varying temporal network.&lt;/li&gt;
&lt;li&gt;Traditional GNNs cannot deal with structural features, and most of the existing works used distance between nodes on static networks.&lt;/li&gt;
&lt;li&gt;Authors proposed dictionary-type node representation and neighborhood cache as a scalable way to represent temporal networks, and achieved SOTA performance on many link prediction tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;temporal-network&#34;&gt;Temporal Network&lt;/h2&gt;
&lt;p&gt;Temporal network is an abstraction of complex interactive systems.&lt;/p&gt;
&lt;p&gt;Network structures evolve over time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Examples:
&lt;ul&gt;
&lt;li&gt;User-item network&lt;/li&gt;
&lt;li&gt;Social media network&lt;/li&gt;
&lt;li&gt;Email network&lt;/li&gt;
&lt;li&gt;Engineering control networks&lt;/li&gt;
&lt;li&gt;Mobility networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Goal:&lt;/p&gt;
&lt;p&gt;Predicting how temporal networks evolve → link prediction in temporal networks&lt;/p&gt;
&lt;p&gt;Application:&lt;/p&gt;
&lt;p&gt;recommendation, anomaly detection, …&lt;/p&gt;
&lt;h2 id=&#34;temporal-networks-in-network-science&#34;&gt;Temporal networks in network science&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Network science: how network structures evolves reflects the fundamental laws of these complex systems
&lt;ul&gt;
&lt;li&gt;Triadic closure in social network: people with common friends tend to know each other.&lt;/li&gt;
&lt;li&gt;Feed-forward control in biological/engineering control systems: positive stimuli are followed with negative stimuli.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;issues-of-previous-approaches-effectiveness&#34;&gt;Issues of Previous Approaches (Effectiveness)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GNNs cannot capture structural features that involve multiple nodes of interest, such as triadic closure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled.png&#34;
	width=&#34;862&#34;
	height=&#34;444&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled_hu0af0c10cdb137a663b0f78c123bc4e21_292022_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled_hu0af0c10cdb137a663b0f78c123bc4e21_292022_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;At time step $t_3$, it is hard for traditional GNN to distinguish nodes $w$ and $v$, since their computation graphs are same.&lt;/p&gt;
&lt;p&gt;→ Temporal network representation learning, if following traditional GNN-type computation, will fail to learn such information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Works over static graph&lt;/p&gt;
&lt;p&gt;There were some works to deal with this issues on static graphs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SEAL (Zhang et al. 2018)&lt;/li&gt;
&lt;li&gt;Distance encoding (Li et al. 2020)&lt;/li&gt;
&lt;li&gt;Labeling trick (Zhang et al. 2021)&lt;/li&gt;
&lt;li&gt;SUREL (Yin et al. 2022)&lt;/li&gt;
&lt;li&gt;ELPH (Chamberlain et al. 2022)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of the idea is to build structural feature (usually shortest path distance) and use it as extra feature.&lt;/p&gt;
&lt;p&gt;How can we apply this idea to temporal networks, in an efficient and scalable way?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recent work over temporal networks&lt;/p&gt;
&lt;p&gt;CAWN (Wang et al. 2021): but high computation overhead.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each queried node pair, random walks need to be sampled.&lt;/li&gt;
&lt;li&gt;The relative positional encoding needs to be computed online.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neighborhood-aware-temporal-network-nat&#34;&gt;Neighborhood Aware Temporal Network (NAT)&lt;/h2&gt;
&lt;p&gt;Key ideas&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dictionary-type node representations
&lt;ul&gt;
&lt;li&gt;Constructs structural feature efficiently&lt;/li&gt;
&lt;li&gt;Avoids online neighbor sampling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Neighborhood Caches (N-caches)
&lt;ul&gt;
&lt;li&gt;Maintains dictionary representations with parallel hashing scalably&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dictionary-representations&#34;&gt;Dictionary representations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Do not use long-vector representations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Each node u is represented as a dictionary&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keys: Down-sampled neighbors in 0-hop (self), 1-hop, 2-hop, …&lt;/li&gt;
&lt;li&gt;Values: Short vector representations (2~8 dim)
&lt;ul&gt;
&lt;li&gt;representation for node $a$ as a neighbor of node $u$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled2.png&#34;
	width=&#34;1872&#34;
	height=&#34;880&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled2_huade67fa509dc97d339187804f4125a21_1059953_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled2_huade67fa509dc97d339187804f4125a21_1059953_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;510px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;→ Captures joint neighborhood structural features between $u$ and $v$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled3.png&#34;
	width=&#34;2118&#34;
	height=&#34;1040&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled3_hu66849c16475129caf62098591160e14d_1210153_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled3_hu66849c16475129caf62098591160e14d_1210153_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;488px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;→ NAT combines structural feature construction and traditional vector representations&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nat-architecture&#34;&gt;NAT architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Architecture
&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled4.png&#34;
	width=&#34;2492&#34;
	height=&#34;1133&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled4_hu37c1e302144fb2103f270f74417e8020_310888_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled4_hu37c1e302144fb2103f270f74417e8020_310888_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;219&#34;
		data-flex-basis=&#34;527px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Experiments
&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled5.png&#34;
	width=&#34;1800&#34;
	height=&#34;760&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled5_huc6510b21f6b038d5eeef0509f8d82bd4_443546_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled5_huc6510b21f6b038d5eeef0509f8d82bd4_443546_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;236&#34;
		data-flex-basis=&#34;568px&#34;
	
&gt;
Performed on both inductive &amp;amp; transductive setting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Performance
&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled6.png&#34;
	width=&#34;1948&#34;
	height=&#34;780&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled6_hu1244281417561286df41d8b587100bd3_1715747_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled6_hu1244281417561286df41d8b587100bd3_1715747_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;599px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computation &amp;amp; Scalability
&lt;img src=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled7.png&#34;
	width=&#34;1452&#34;
	height=&#34;996&#34;
	srcset=&#34;https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled7_hua5e9fd5770935b069b9c5fb370894ce1_814749_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled7_hua5e9fd5770935b069b9c5fb370894ce1_814749_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;349px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;takeaways&#34;&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Structural features from a joint neighborhood of multiple nodes are crucial to predict temporal network evolution&lt;/li&gt;
&lt;li&gt;Dictionary-type representations can combine structural feature construction with traditional vector representations.&lt;/li&gt;
&lt;li&gt;Dictionary-type representations allow online construction of such structural features in an efficient way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.01084&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neighborhood-aware Scalable Temporal Network Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/live/wp5S9GHyAgw?feature=share&amp;amp;t=8409&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning on Graphs Conference 2022 - Day 1 Livestream&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>A Generalist Neural Algorithmic Learner</title>
        <link>https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/</link>
        <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/</guid>
        <description>&lt;p&gt;LoG, 22 &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.11142&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Generalist Neural Algorithmic Learner&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/thumbnail.png&#34;
	width=&#34;1042&#34;
	height=&#34;528&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/thumbnail_hudd1318377aff45244e3590338f52153c_112022_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/thumbnail_hudd1318377aff45244e3590338f52153c_112022_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;473px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Neural network, especially GNN, can learn traditional computer science algorithms in CLRS book.&lt;/li&gt;
&lt;li&gt;A generalist neural algorithmic learner is necessary if the algorithm is not obvious.&lt;/li&gt;
&lt;li&gt;Chunking mechanism was important to stabilize multi-task learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;starting-point-a-benchmark-to-train-neural-computer-scientists&#34;&gt;Starting point: A benchmark to train neural computer scientists&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.15659&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The CLRS Algorithmic Reasoning Benchmark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/deepmind/clrs&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/deepmind/clrs&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can we train a neural network to execute classical CS algorithms?&lt;/li&gt;
&lt;li&gt;A differentiable computer scientist could then apply its &amp;ldquo;knowledge&amp;rdquo; to natural inputs.&lt;/li&gt;
&lt;li&gt;We will also ponder: can it learn multiple algorithms at the same time?&lt;/li&gt;
&lt;li&gt;Typically the problem is modeled with a recurrent architecture:
&lt;ul&gt;
&lt;li&gt;LSTMs as in, e.g., Differentiable Neural Computers&lt;/li&gt;
&lt;li&gt;Transformers, as in the Universal Transformer&lt;/li&gt;
&lt;li&gt;ConvNets&lt;/li&gt;
&lt;li&gt;GNNs - our approach&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Introduction to Algorithms: CLRS&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled.png&#34;
	width=&#34;442&#34;
	height=&#34;500&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled_hua63265e8260ecb294a7579e9be70a8cf_138413_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled_hua63265e8260ecb294a7579e9be70a8cf_138413_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;88&#34;
		data-flex-basis=&#34;212px&#34;
	
&gt;
&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled1.png&#34;
	width=&#34;1352&#34;
	height=&#34;2296&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled1_hucd8b39bbcea98e1dc70ae8292aef00b9_466801_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled1_hucd8b39bbcea98e1dc70ae8292aef00b9_466801_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;List of algorithms included in the benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;58&#34;
		data-flex-basis=&#34;141px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Representation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;All algorithms have been boiled down to a common graph representation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Each algorithm is specified by a fixed number of &amp;ldquo;probes&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For example, the spec of insertion sort consists of the following 6 probes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;pos&#39;: (Stage. INPUT, Location.NODE, Type.SCALAR)&lt;/code&gt; → the id of each node&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;key&#39;: (Stage.INPUT, Location. NODE, Type.SCALAR)&lt;/code&gt; → the values to sort&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;pred&#39;: (Stage.OUTPUT, Location. NODE, Type.POINTER)&lt;/code&gt; → the final node order&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;pred h&#39;: (Stage. HINT, Location. NODE, Type. POINTER)&lt;/code&gt; → the node order along execution&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;i&#39;: (Stage.HINT, Location.NODE, Type.MASK_ONE)&lt;/code&gt; → index for insertion&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;j&#39;: (Stage.HINT, Location.NODE, Type.MASK_ONE)&lt;/code&gt; → index tracking &amp;ldquo;sorted up to&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A probe can be input, output or hint.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The inputs and outputs are fixed during algorithm execution, the hints change during execution&lt;/p&gt;
&lt;p&gt;→ they specify the algorithm (e.g., all sorting algorithms have the same inputs and outputs, differing only in their hints).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;representation-encoding&#34;&gt;Representation: Encoding&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled2.png&#34;
	width=&#34;1890&#34;
	height=&#34;978&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled2_hu7e853881e8b757dd1a85a210730154a4_566098_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled2_hu7e853881e8b757dd1a85a210730154a4_566098_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;463px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pos&lt;/code&gt;: Positional ID (ID of node) is encoded as vector by encoder.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled3.png&#34;
	width=&#34;1876&#34;
	height=&#34;966&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled3_hu7baf8378b39f5fa240e98e584947107a_603402_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled3_hu7baf8378b39f5fa240e98e584947107a_603402_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;466px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;key&lt;/code&gt;: The value to be processed is also encoded as vector and added to &lt;code&gt;pos&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled4.png&#34;
	width=&#34;1890&#34;
	height=&#34;974&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled4_hucd1cc34f62c703accc3d07c639180e51_633524_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled4_hucd1cc34f62c703accc3d07c639180e51_633524_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pred_h&lt;/code&gt;: pointer used as hint is encoded and used as edge embedding.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled5.png&#34;
	width=&#34;1886&#34;
	height=&#34;988&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled5_huaaecf81d46faf81870899897fc493e3b_810218_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled5_huaaecf81d46faf81870899897fc493e3b_810218_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;458px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;i&lt;/code&gt;, &lt;code&gt;j&lt;/code&gt;: index needed for insertion are also added.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled6.png&#34;
	width=&#34;1408&#34;
	height=&#34;872&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled6_hue476ddce194debfa21fdc58a72e3d857_332819_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled6_hue476ddce194debfa21fdc58a72e3d857_332819_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;161&#34;
		data-flex-basis=&#34;387px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled7.png&#34;
	width=&#34;1420&#34;
	height=&#34;832&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled7_hua63bee05ca500a02321ab8fba0e28fac_373127_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled7_hua63bee05ca500a02321ab8fba0e28fac_373127_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled8.png&#34;
	width=&#34;1388&#34;
	height=&#34;838&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled8_hu182e9048a4d33125a9f3cad1eff4208e_440343_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled8_hu182e9048a4d33125a9f3cad1eff4208e_440343_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;representation-decoding&#34;&gt;Representation: Decoding&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled9.png&#34;
	width=&#34;2160&#34;
	height=&#34;1006&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled9_hu1c5ed84ce779c0009f05f3a5c7d8805a_1301394_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled9_hu1c5ed84ce779c0009f05f3a5c7d8805a_1301394_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;515px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Processing step is agnostic to the algorithm. Processing parameter is shared.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled10.png&#34;
	width=&#34;1910&#34;
	height=&#34;1120&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled10_hu780304005a57f8d4da75acf6761a1447_1221193_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled10_hu780304005a57f8d4da75acf6761a1447_1221193_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Hint: used only when training (not testing)&lt;/p&gt;
&lt;p&gt;When training, hint loss is also added along with output loss.&lt;/p&gt;
&lt;h2 id=&#34;details&#34;&gt;Details&lt;/h2&gt;
&lt;p&gt;Trained on unlimited samples of size (number of nodes) &amp;lt; 16&lt;/p&gt;
&lt;p&gt;The training distribution doesn&amp;rsquo;t cover all possible inputs though (e.g., we use only
Erdös-Rényi graphs)&lt;/p&gt;
&lt;p&gt;Tested on samples of size 64.&lt;/p&gt;
&lt;p&gt;The length of the trajectory is given → both at train &amp;amp; test time.&lt;/p&gt;
&lt;p&gt;Early stopping based on in-distribution scores.&lt;/p&gt;
&lt;h2 id=&#34;but-why-even-care-about-building-a-generalist&#34;&gt;But.. why even care about building a generalist?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled11.png&#34;
	width=&#34;1788&#34;
	height=&#34;920&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled11_hud75bfcc42b7bb40e9cc645bc00eace8d_751629_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled11_hud75bfcc42b7bb40e9cc645bc00eace8d_751629_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;466px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;→ It is all about problem solving!&lt;/p&gt;
&lt;p&gt;How do we solve problems?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Example: Route recommendation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled12.png&#34;
	width=&#34;2190&#34;
	height=&#34;1214&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled12_hu7e6e9af231f964be6a6ec86c54c77193_1572950_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled12_hu7e6e9af231f964be6a6ec86c54c77193_1572950_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;details-1&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled13.png&#34;
	width=&#34;1136&#34;
	height=&#34;572&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled13_hue16f66050313a9e43df5d697661818d7_432973_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled13_hue16f66050313a9e43df5d697661818d7_432973_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;476px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;With Neural Algorithmic Reasoning, we break the &lt;strong&gt;blue&lt;/strong&gt; bottleneck!&lt;/p&gt;
&lt;p&gt;A generalist processor would break the &lt;strong&gt;red&lt;/strong&gt; one!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the model have a shared latent space where all the &amp;ldquo;key&amp;rdquo; algorithms would be executed&amp;hellip;&lt;/li&gt;
&lt;li&gt;No longer need to decide upfront which algorithm to use!&lt;/li&gt;
&lt;li&gt;The algorithm (combo) can be softly selected, learned by backprop through the encoder.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;to-get-a-generalist-first-we-need-a-good-specialist&#34;&gt;To get a generalist, first we need a good specialist&lt;/h2&gt;
&lt;p&gt;However, training a generalist is not as easy as simple training over all 30 algos in CLRS-30!&lt;/p&gt;
&lt;p&gt;Initial runs of this kind led to NaNs.&lt;/p&gt;
&lt;p&gt;Prior results, e.g. NE++ (Xhonneux et al., NeurIPS&#39;21) imply this can be successful only if the algorithms being learnt together are highly related (e.g. Prim + Dijkstra)..&lt;/p&gt;
&lt;p&gt;Key limitation:&lt;/p&gt;
&lt;p&gt;Tasks with high learning instabilities cause breakages for all other.&lt;/p&gt;
&lt;p&gt;→ Set out to improve single-task stability first!&lt;/p&gt;
&lt;h3 id=&#34;bucket-list-of-improvements&#34;&gt;Bucket list of improvements&lt;/h3&gt;
&lt;p&gt;Key improvements include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Removing teacher forcing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training data augmentation (e.g. sampling multiple sizes below 16)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Soft hint propagation (e.g. do not apply $\argmax$ to the hints; compute $\text{softmax}$ instead)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Static hint elimination (if a hint provably never changes, convert it to an input)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoder initialization (Xavier) + gradient clipping&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Randomized positional embeddings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Permutation decoders using the Sinkhorn operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gating mechanisms in the processor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Triplet reasoning&lt;/p&gt;
&lt;p&gt;$t_{ijk} = \psi_t (h_i, h_j, h_k, e_{ij}, e_{ik}, e_{kj}, g)$&lt;/p&gt;
&lt;p&gt;$h_{ij} = \phi_t(\max_k t_{ijk})$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled14.png&#34;
	width=&#34;1870&#34;
	height=&#34;888&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled14_huf219c2988f878053e5a1b857a2f9f297_1096341_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled14_huf219c2988f878053e5a1b857a2f9f297_1096341_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;505px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;final-step-to-the-generalist-chunking&#34;&gt;Final step to the generalist: Chunking&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;chunking mechanism&lt;/strong&gt; was important for multi-task learning!&lt;/p&gt;
&lt;p&gt;This not only helps protect against OOM issues, it also improves learning stability!&lt;/p&gt;
&lt;p&gt;The idea is conceptually simple (though tricky to implement)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The length of the trajectory is set to 16.&lt;/li&gt;
&lt;li&gt;Shorter samples are not padded, but concatenated by next sample.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;i.e. if trajectory doesn&amp;rsquo;t fully fit in the chunk, this is OK-can restart from a midpoint hint.&lt;/p&gt;
&lt;p&gt;Initialization of the hidden state should not matter, since CLRS-30 tasks are Markovian!&lt;/p&gt;
&lt;h3 id=&#34;single-generalist-that-matches-the-thirty-specialists&#34;&gt;Single generalist that matches the thirty specialists&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled15.png&#34;
	width=&#34;2232&#34;
	height=&#34;730&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled15_hu1f7b1e05436ecf853aeb420a00a1cb90_1553889_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled15_hu1f7b1e05436ecf853aeb420a00a1cb90_1553889_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;733px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;chunking-helps-significantly&#34;&gt;Chunking helps significantly&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled16.png&#34;
	width=&#34;2196&#34;
	height=&#34;818&#34;
	srcset=&#34;https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled16_hue2c0561d8ac9d7e91097433a7ef8d5ad_959613_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/a-generalist-neural-algorithmic-learner/imgs/Untitled16_hue2c0561d8ac9d7e91097433a7ef8d5ad_959613_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;644px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/live/wp5S9GHyAgw?feature=share&amp;amp;t=10170&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning on Graphs Conference 2022 - Day 1 Livestream&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.11142&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Generalist Neural Algorithmic Learner&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>About JH Gu</title>
        <link>https://gujh14.github.io/about-jh-gu/</link>
        <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/about-jh-gu/</guid>
        <description>&lt;h2 id=&#34;profile&#34;&gt;Profile&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ph.D student in Seoul National University, Seoul, Korea.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2014.03 ~ 2016.02: Department of Chemical &amp;amp; Biological Enineering, Seoul National University&lt;/li&gt;
&lt;li&gt;2016.03 ~ 2020.02: Department of Pharmacy, Seoul National University&lt;/li&gt;
&lt;li&gt;2021.02 ~ : Interdisciplinary program of Artificial Intelligence, Seoul National University&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;awards&#34;&gt;Awards&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2022 Samsung AI Challenge (Materials Discovery) 3rd Place&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;research-interest&#34;&gt;Research Interest&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Drug Discovery&lt;/li&gt;
&lt;li&gt;Representation Learning&lt;/li&gt;
&lt;li&gt;Graph Neural Network&lt;/li&gt;
&lt;li&gt;3D Geometric Learning&lt;/li&gt;
&lt;li&gt;Generative learning (Diffusion models)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experience&#34;&gt;Experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2020.02 ~ 2020.04: Internship in Business Development at HurayPositive&lt;/li&gt;
&lt;li&gt;2021.09 ~ : Official member of Deepest&lt;/li&gt;
&lt;li&gt;2021.12 ~ : OG (Original Gangster) member of El Txoko (&lt;a class=&#34;link&#34; href=&#34;https://instagram.com/eltxoko_seoul&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;eltxoko_seoul&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Does GNN Pre-training Help Molecular Representation?</title>
        <link>https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/</link>
        <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/</guid>
        <description>&lt;p&gt;NeurIPS, &amp;lsquo;22&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2207.06010&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Does GNN Pre-training Help Molecular Representation?&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/thumbnail.png&#34;
	width=&#34;1816&#34;
	height=&#34;446&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/thumbnail_hube49b7519c97672414e9b0ddbe5479a4_229014_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/thumbnail_hube49b7519c97672414e9b0ddbe5479a4_229014_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;407&#34;
		data-flex-basis=&#34;977px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Self-supervised pre-training alone &lt;strong&gt;does not&lt;/strong&gt; provide statistically significant improvements over non-pre-trained methods on downstream tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data splits, hand-crafted rich features, or hyperparameters&lt;/strong&gt; can bring significant improvements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled.png&#34;
	width=&#34;1722&#34;
	height=&#34;582&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled_hu2522057aecca1aa237485e0d3503f9e7_141482_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled_hu2522057aecca1aa237485e0d3503f9e7_141482_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;295&#34;
		data-flex-basis=&#34;710px&#34;
	
&gt;
Pre-train objectives&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Self-supervised
&lt;ul&gt;
&lt;li&gt;Node prediction&lt;/li&gt;
&lt;li&gt;Context prediction&lt;/li&gt;
&lt;li&gt;Motif prediction&lt;/li&gt;
&lt;li&gt;Contrastive learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supervised
&lt;ul&gt;
&lt;li&gt;Related tasks with label (e.g. ChEMBL dataset)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graph features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Basic
Feature set used in &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1905.12265&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hu et al.&lt;/a&gt;&lt;br&gt;
&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled1.png&#34;
	width=&#34;1694&#34;
	height=&#34;508&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled1_hu88729ad961e3498c17ab82c4660c440e_117274_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled1_hu88729ad961e3498c17ab82c4660c440e_117274_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;333&#34;
		data-flex-basis=&#34;800px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rich&lt;/p&gt;
&lt;p&gt;Feature set used in &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2007.02835&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Rong et al.&lt;/a&gt; This is the superset of basic features.
&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled2.png&#34;
	width=&#34;1338&#34;
	height=&#34;1076&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled2_hub6d6462cb2f07779a844e0b6f42f690b_168013_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled2_hub6d6462cb2f07779a844e0b6f42f690b_168013_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;124&#34;
		data-flex-basis=&#34;298px&#34;
	
&gt;
In downstream tasks, additional 2d normalized &lt;code&gt;rdNormalizedDescriptors&lt;/code&gt; are used (not in pre-training).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Downstream splits&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Scaffold&lt;/p&gt;
&lt;p&gt;Sorts the molecule according to the scaffold, then partition the sorted list into train/valid/test splits. → Deterministic&lt;/p&gt;
&lt;p&gt;Molecules of each set are most different ones.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Balanced scaffold&lt;/p&gt;
&lt;p&gt;Introduces the randomness in the sorting and splitting stages of Scaffold split.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GNN architecture&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GIN&lt;/li&gt;
&lt;li&gt;GraphSAGE&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pre-train dataset&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ZINC15 (self-supervised)&lt;/p&gt;
&lt;p&gt;2 million molecules. Pre-processed following Hu et al.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SAVI (self-supervised)&lt;/p&gt;
&lt;p&gt;1 billion drug-like molecules synthesized by computer simulated reactions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ChEMBL (supervised)&lt;/p&gt;
&lt;p&gt;500k drugable molecules with 1,310 prediction target labels from bio-activity assays.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled3.png&#34;
	width=&#34;1696&#34;
	height=&#34;402&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled3_hu23dfaf19760eb9198a64586f6e9e538d_129521_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled3_hu23dfaf19760eb9198a64586f6e9e538d_129521_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;421&#34;
		data-flex-basis=&#34;1012px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled4.png&#34;
	width=&#34;1694&#34;
	height=&#34;306&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled4_hub456f2da350b268e245cd11a5535bf94_107470_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled4_hub456f2da350b268e245cd11a5535bf94_107470_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;553&#34;
		data-flex-basis=&#34;1328px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled5.png&#34;
	width=&#34;1690&#34;
	height=&#34;390&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled5_hu2c1b41f4be72da8a39843fe8bece40ca_127650_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled5_hu2c1b41f4be72da8a39843fe8bece40ca_127650_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;433&#34;
		data-flex-basis=&#34;1040px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled6.png&#34;
	width=&#34;1712&#34;
	height=&#34;308&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled6_hu9e3333a7416211da0979291a20928457_104323_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled6_hu9e3333a7416211da0979291a20928457_104323_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;555&#34;
		data-flex-basis=&#34;1334px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled7.png&#34;
	width=&#34;1700&#34;
	height=&#34;326&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled7_hufd2c8cd9ddab44b099368583aeb3c4b4_98592_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled7_hufd2c8cd9ddab44b099368583aeb3c4b4_98592_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;521&#34;
		data-flex-basis=&#34;1251px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled8.png&#34;
	width=&#34;1702&#34;
	height=&#34;316&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled8_hue724faf0b9e1050d8cd4ed3ab8021048_107066_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled8_hue724faf0b9e1050d8cd4ed3ab8021048_107066_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;538&#34;
		data-flex-basis=&#34;1292px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled9.png&#34;
	width=&#34;1710&#34;
	height=&#34;324&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled9_hu3469e656073a5e47d18db581b35e22f4_102002_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled9_hu3469e656073a5e47d18db581b35e22f4_102002_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;527&#34;
		data-flex-basis=&#34;1266px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled10.png&#34;
	width=&#34;1696&#34;
	height=&#34;328&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled10_hu3e2f5f911381baac558177a0aa61cd89_114788_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled10_hu3e2f5f911381baac558177a0aa61cd89_114788_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;517&#34;
		data-flex-basis=&#34;1240px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled11.png&#34;
	width=&#34;1698&#34;
	height=&#34;358&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled11_hucbfacb23d6af299525c858ef04d38582_129938_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled11_hucbfacb23d6af299525c858ef04d38582_129938_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;474&#34;
		data-flex-basis=&#34;1138px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled12.png&#34;
	width=&#34;1704&#34;
	height=&#34;368&#34;
	srcset=&#34;https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled12_hu98ddd8342e269d06b326bfdd4dd2ec69_121600_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/does-gnn-pre-training-help-molecular-representation/imgs/Untitled12_hu98ddd8342e269d06b326bfdd4dd2ec69_121600_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;463&#34;
		data-flex-basis=&#34;1111px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt;
&lt;h3 id=&#34;when-pre-training-might-help&#34;&gt;When pre-training might help?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Related &lt;strong&gt;supervised&lt;/strong&gt; pre-training dataset. But not always feasible.&lt;/li&gt;
&lt;li&gt;If the rich features are absent.&lt;/li&gt;
&lt;li&gt;If the downstream split distributions are substantially different.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;when-the-gain-dimishes&#34;&gt;When the gain dimishes?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;If using rich features.&lt;/li&gt;
&lt;li&gt;If don’t have the highly relevant supervisions.&lt;/li&gt;
&lt;li&gt;If the downstream split is balanced.&lt;/li&gt;
&lt;li&gt;If the self-supervised learning dataset lacks diversity.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;why-pre-training-may-not-help-in-some-cases&#34;&gt;Why pre-training may not help in some cases?&lt;/h3&gt;
&lt;p&gt;Some of the pre-training methods (e.g. node label prediction) might be too easy&lt;br&gt;
→ Transfer less knowledge.&lt;/p&gt;
&lt;p&gt;So…&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use rich features&lt;/li&gt;
&lt;li&gt;Use balanced scaffold&lt;/li&gt;
&lt;li&gt;Use related supervised pre-training dataset&lt;/li&gt;
&lt;li&gt;Use difficult pre-training task (for self-supervised pre-training) and use high-quality negative samples.&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Graph Self-supervised Learning with Accurate Discrepancy Learning</title>
        <link>https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/</link>
        <pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/</guid>
        <description>&lt;p&gt;NeurIPS, &amp;lsquo;22,&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=SgZ-glWWUlq&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Graph Self-supervised Learning with Accurate Discrepancy Learning&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/thumbnail.png&#34;
	width=&#34;1414&#34;
	height=&#34;484&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/thumbnail_hu1da554d6c291b842a8ea287e9b40bdf6_233064_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/thumbnail_hu1da554d6c291b842a8ea287e9b40bdf6_233064_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;292&#34;
		data-flex-basis=&#34;701px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Authors proposed a framework called D-SLA that aims to learn the exact discrepancy between the original and the perturbed graphs.&lt;/li&gt;
&lt;li&gt;Three major components
&lt;ol&gt;
&lt;li&gt;Learn to distinguish whether each graph is the original graph or the perturbed one.&lt;/li&gt;
&lt;li&gt;Capture the amount of discrepancy for each perturbed graph (using edit distance)&lt;/li&gt;
&lt;li&gt;Learn relative discrepancy with other graphs&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;h3 id=&#34;graph-neural-networks-gnn&#34;&gt;Graph Neural Networks (GNN)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Aggregate the features from its neighbors&lt;/li&gt;
&lt;li&gt;Combining the aggregated message&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Variants of Update &amp;amp; Aggregate functions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Graph Convolution Network (GCN)&lt;/p&gt;
&lt;p&gt;General convolution operation + Mean aggregation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GraphSAGE&lt;/p&gt;
&lt;p&gt;Concatenate representations of neighbors with its own representation when updating&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graph Attention Network (GAT)&lt;/p&gt;
&lt;p&gt;Considers the relative importance among neighboring nodes when aggregation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graph Isomorphism Network (GIN)&lt;/p&gt;
&lt;p&gt;Sum aggregation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;self-supervised-learning-for-graphs-gsl&#34;&gt;Self-supervised learning for graphs (GSL)&lt;/h3&gt;
&lt;p&gt;Aims to learn a good representation of the graphs in an unsupervised manner.&lt;/p&gt;
&lt;p&gt;→ Transfer this knowledge to downstream tasks.&lt;/p&gt;
&lt;p&gt;Most prevalent framework for GSL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Predictive learning (PL)&lt;/p&gt;
&lt;p&gt;Aims to learn &lt;strong&gt;contextual relationships&lt;/strong&gt; by predicting sub-graphical features (nodes, edges, subgraphs)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;predict the attributes of masked nodes&lt;/li&gt;
&lt;li&gt;predict the presence of an edge or a path&lt;/li&gt;
&lt;li&gt;predict the generative sequence, contextual property, and motifs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But predictive learning &lt;strong&gt;may not capture the global structures and/or semantics of graphs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled.png&#34;
	width=&#34;896&#34;
	height=&#34;520&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled_huff270fcc5aeca6685caf1879e044c99c_227289_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled_huff270fcc5aeca6685caf1879e044c99c_227289_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;172&#34;
		data-flex-basis=&#34;413px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contrastive learning (CL)&lt;/p&gt;
&lt;p&gt;Aims to &lt;strong&gt;capture global level information.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Early CL learn the similarity between the entire graph and its substructure.&lt;/li&gt;
&lt;li&gt;Others include attribute masking, edge perturbation, and subgraph sampling.&lt;/li&gt;
&lt;li&gt;Recent CL adversarial methods generate positive examples either by adaptively removing the edges or by adjusting the attributes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But CL &lt;strong&gt;may not distinguish two topologically similar graphs yet having completely different properties.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled1.png&#34;
	width=&#34;894&#34;
	height=&#34;526&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled1_hu2f022890e791016794b0ba84351f912a_250914_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled1_hu2f022890e791016794b0ba84351f912a_250914_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;169&#34;
		data-flex-basis=&#34;407px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled2.png&#34;
	width=&#34;946&#34;
	height=&#34;384&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled2_hu5b683b5549a725d0a4cfb0bcb47c4b83_188133_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled2_hu5b683b5549a725d0a4cfb0bcb47c4b83_188133_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Minimize $\mathcal{L}_{CL} = - \log \frac{f_{\text{sim}} (h_{\mathcal{G}_i}, h_{\mathcal{G}_j})}{\sum_{\mathcal{G}&amp;rsquo;, \mathcal{G&amp;rsquo; \neq \mathcal{G}_0}}f_{\text{sim}}(h_{\mathcal{G}_i}, h_{\mathcal{G}&amp;rsquo;})}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{G}_0$: original graph&lt;/li&gt;
&lt;li&gt;$\mathcal{G}_i, \mathcal{G}_j$: perturbed graphs&lt;/li&gt;
&lt;li&gt;$\mathcal{G}&amp;rsquo;$: other graph in the same batch with the $\mathcal{G}_0$, a.k.a. negative graph&lt;/li&gt;
&lt;li&gt;positive pair: $(\mathcal{G}_i, \mathcal{G}_j)$; negative pair: $(\mathcal{G}_i, \mathcal{G}&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;$f_\text{sim}$: similarity function between two graphs → $L_2$ distance or cosine similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ similarity of positive pair $\uparrow$, similarity of negative pair $\downarrow$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;discrepancy-learning&#34;&gt;Discrepancy Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled3.png&#34;
	width=&#34;1312&#34;
	height=&#34;402&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled3_hu733a3810c757eb627bd173cc2c52ec22_403552_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled3_hu733a3810c757eb627bd173cc2c52ec22_403552_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;326&#34;
		data-flex-basis=&#34;783px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Discriminate original vs perturbed&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Perturbed graph could be semantically incorrect!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;→ Embed perturbed graph apart from original.&lt;/p&gt;
&lt;p&gt;$\mathcal{L}_{GD} = - \log \Big (\frac{e^{S_0}}{e^{S_0} + \sum_{i \geq 1}e^{S_i}} \Big ) \text{ with } S = f_S(h_{\mathcal{G}})$&lt;/p&gt;
&lt;p&gt;Intuitively,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large value of $e^{S_0}$ for the original graph&lt;/li&gt;
&lt;li&gt;small value of $e^{S_i}$ for the perturbed graphs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to perturb?&lt;/p&gt;
&lt;p&gt;Aim at perturbed graph to be semantically incorrect&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Remove or add a small number of edges&lt;/p&gt;
&lt;p&gt;Manipulate the edge set by removing existing edges  + adding new edges on $\mathcal{X}_\mathcal{E}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mask node attributes&lt;/p&gt;
&lt;p&gt;Randomly mask the node attributes on $\mathcal{X}_\mathcal{V}$ for both original and perturbed graphs&lt;/p&gt;
&lt;p&gt;(to make it more difficult to distinguish between them)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$\mathcal{G}_0 = (\mathcal{V}, \mathcal{E}, \tilde{\mathcal{X}^0_{\mathcal{V}}}, \mathcal{X}_{\mathcal{E}}), \tilde{\mathcal{X}^0_{\mathcal{V}}} \sim \texttt{M}(\mathcal{G})$&lt;/p&gt;
&lt;p&gt;$\mathcal{G}_i = (\mathcal{V}, \mathcal{E}^i, \tilde{\mathcal{X}^i_{\mathcal{V}}}, \mathcal{X}^i_{\mathcal{E}}), \tilde{\mathcal{X}^i_{\mathcal{V}}} \sim \texttt{M}(\mathcal{G}), (\mathcal{E}^i, \mathcal{X}^i_{\mathcal{E}}) \sim \texttt{P}(\mathcal{G})$&lt;/p&gt;
&lt;p&gt;Personal opinion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The real usage of discriminator loss will be to push original &amp;amp; perturbed graph apart, while applying edit distance loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discrepancy with Edit distance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How dissimilar?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Usually, we need to measure the graph distance, such as edit distance.&lt;/p&gt;
&lt;p&gt;Edit distance: number of insertion, deletion, and substitution operations for nodes &amp;amp; edges to transform one graph from another. → NP hard!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But we know the exact number of perturbations for each graphs&lt;/p&gt;
&lt;p&gt;→ use it as distance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mathcal{L}_{edit} = \sum_{i, j} \Big ( \frac{d_i}{e_i} - \frac{d_j}{e_j}\Big )^2 \text{ with } d_i = f_{\text{diff}}(h_{\mathcal{G}_0}, h_{\mathcal{G}_i})$&lt;/p&gt;
&lt;p&gt;$f_{\text{diff}}$ measures the embedding level differences between graphs with &lt;code&gt;L2 norm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;$e_i$: edit distance (number of perturbations)&lt;/p&gt;
&lt;p&gt;The trivial solution for the edit distance loss is $d_i = d_j = 0$. But because of the discriminator loss, this is not possible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relative discrepancy learning with other graphs&lt;/p&gt;
&lt;p&gt;Assumption:&lt;/p&gt;
&lt;p&gt;Distance between original and negative graphs in the same batch is larger than the distance between the original and perturbed graphs with some amount of margin.&lt;/p&gt;
&lt;p&gt;Formally,&lt;/p&gt;
&lt;p&gt;$\mathcal{L}_{margin} = \sum_{i, j} \max (0, \alpha + d_i - d&amp;rsquo;_j)$&lt;/p&gt;
&lt;p&gt;$d_i$: distance between original and its perturbed graphs&lt;/p&gt;
&lt;p&gt;$d&amp;rsquo;_j$: distance between original and negative graphs&lt;/p&gt;
&lt;p&gt;Intuitively, $\alpha + d_i &amp;lt; d&amp;rsquo;_j$ !&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;overall-loss&#34;&gt;Overall loss&lt;/h3&gt;
&lt;p&gt;$\mathcal{L} = \mathcal{L}_{GD} + \lambda_1 \mathcal{L}_{edit} + \lambda_2 \mathcal{L}_{margin}$&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled4.png&#34;
	width=&#34;2760&#34;
	height=&#34;878&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled4_hu61ce1efc03ade92431ecba0566b173fe_1254349_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled4_hu61ce1efc03ade92431ecba0566b173fe_1254349_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;314&#34;
		data-flex-basis=&#34;754px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled5.png&#34;
	width=&#34;662&#34;
	height=&#34;656&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled5_hub614ec3849a1c45ba1bddf42f077afb8_259131_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled5_hub614ec3849a1c45ba1bddf42f077afb8_259131_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;100&#34;
		data-flex-basis=&#34;242px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled6.png&#34;
	width=&#34;2072&#34;
	height=&#34;640&#34;
	srcset=&#34;https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled6_hua1d295acff7e146071f146ac8c9b5420_722837_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/graph-self-supervised-learning-with-accurate-discrepancy-learning/imgs/Untitled6_hua1d295acff7e146071f146ac8c9b5420_722837_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;323&#34;
		data-flex-basis=&#34;777px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PhysChem: Deep Molecular Representation Learning via Fusing Physical and Chemical Information</title>
        <link>https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/</link>
        <pubDate>Fri, 29 Jul 2022 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/</guid>
        <description>&lt;p&gt;NeurIPS Poster, &amp;lsquo;21,&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2112.04624&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PhysChem: Deep Molecular Representation Learning via Fusing Physical and Chemical Information&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/thumbnail.png&#34;
	width=&#34;2000&#34;
	height=&#34;709&#34;
	srcset=&#34;https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/thumbnail_hu614a3ffe516a8cd557a8bbd359513341_952945_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/thumbnail_hu614a3ffe516a8cd557a8bbd359513341_952945_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;282&#34;
		data-flex-basis=&#34;677px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Used physicist network (PhysNet) and chemist network (ChemNet) simultaneously, and each network shares information to solve individual tasks.&lt;/li&gt;
&lt;li&gt;PhysNet: Neural physical engine. Mimics molecular dynamics to predict conformation.&lt;/li&gt;
&lt;li&gt;ChemNet: Message passing network for chemical &amp;amp; biomedical property prediction.&lt;/li&gt;
&lt;li&gt;Molecule without 3D conformation can be inferred during test time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Molecular representation learning:&lt;br&gt;
Embedding molecules into latent space for downstream tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Physical Engines&lt;br&gt;
Neural networks are capable of learning annotated potentials and forces in particle systems.&lt;br&gt;
HamNet proposed a neural physical engine that operated on a generalized space, where positions and momentums of atoms were defined as high-dimensional vectors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-task learning&lt;br&gt;
Sharing representations for different but related tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model fusion&lt;br&gt;
Merging different models on identical tasks to improve performance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notation&#34;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Graph $\mathcal{M} = (\mathcal{V}, \mathcal{E}, n, m, \mathbf{X}^v, \mathbf{X}^e)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{V}$: set of $n$ atoms&lt;/li&gt;
&lt;li&gt;$\mathcal{E}$: set of $m$ chemical bonds&lt;/li&gt;
&lt;li&gt;$\mathbf{X}^v \in \mathbb{R}^{n \times d_v} = (x^v_1, &amp;hellip;, x^v_n)^\top$: matrix of atomic features&lt;/li&gt;
&lt;li&gt;$\mathbf{X}^e \in \mathbb{R}^{m \times d_e} = (x^e_1, &amp;hellip;, x^e_m)^\top$: matrix of bond features&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/physchem_1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
&gt;
Figure 1. PhysChem Architecture&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initializer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input: atomic features, bond features (from RDKit)&lt;/li&gt;
&lt;li&gt;Layer: fully connected layers&lt;/li&gt;
&lt;li&gt;Output:&lt;/li&gt;
&lt;li&gt;bond states, atom states for ChemNet&lt;br&gt;
$v^{(0)}_i = \text{FC}(x^v_i), i\in \mathcal{V}$&lt;br&gt;
$e^{(0)}_{i,j} = \text{FC}(x^e_{i,j}), (i, j)\in \mathcal{E}$&lt;/li&gt;
&lt;li&gt;atom positions, atomic momenta for PhysNet&lt;br&gt;
Bond strength adjacency matrix&lt;br&gt;
$$A(i,j)=\begin{cases}0, &amp;amp; \text{if $(i,j) \notin \mathcal{E}$} \\ \text{FC}_{\text{sigmoid}}(x^e_{i,j}), &amp;amp; \text{if $(i,j) \in \mathcal{E}$} \end{cases}$$
$\tilde{V} = \text{GCN}(A, V^{(0)})$&lt;br&gt;
${ (q^{(0)}_i \oplus p^{(0)}_i)}  = \text{LSTM}({\tilde{v}_i}), i \in \mathcal{V}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PhysNet&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PhysNet is inspired by &lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=q-cnWaaoUTH&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;HamNet&lt;/a&gt;.&lt;br&gt;
HamNet showed that neural networks can simulate molecular dynamics for conformation prediction.&lt;/li&gt;
&lt;li&gt;Directly parameterize the forces between each pair of atoms.&lt;/li&gt;
&lt;li&gt;Consider the effects of chemical interactions(e.g. bond types) by cooperating with ChemNet’s bond states.&lt;/li&gt;
&lt;li&gt;Introduces torsion forces.&lt;/li&gt;
&lt;li&gt;Output: 3D conformation
&lt;img src=&#34;https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/physnet.jpeg&#34;
	width=&#34;1424&#34;
	height=&#34;1000&#34;
	srcset=&#34;https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/physnet_huc04268559fc6ac467b55debd1d11cbcb_463344_480x0_resize_q75_box.jpeg 480w, https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/physnet_huc04268559fc6ac467b55debd1d11cbcb_463344_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ChemNet&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ChemNet modifies MPNN(message passing neural network) for molecular representation learning.&lt;/li&gt;
&lt;li&gt;Output: Molecule representation
&lt;img src=&#34;https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/chemnet.jpeg&#34;
	width=&#34;1598&#34;
	height=&#34;1000&#34;
	srcset=&#34;https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/chemnet_huea3837eddd924d917074e41bba2e849a_561970_480x0_resize_q75_box.jpeg 480w, https://gujh14.github.io/p/physchem-deep-molecular-representation-learning-via-fusing-physical-and-chemical-information/chemnet_huea3837eddd924d917074e41bba2e849a_561970_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;383px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;loss&#34;&gt;Loss&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$L_{\text{phys}}$: Conn-k loss for Conformation prediction (PhysNet)&lt;/p&gt;
&lt;p&gt;$k$-hop connectivity loss&lt;/p&gt;
&lt;p&gt;$L_{\text{Conn}-k}(\hat{\mathbf{R}}, \mathbf{R}) = |\frac{1}{n} \hat{\mathbf{C}}^{(k)} \odot (\hat{\mathbf{D}} - \mathbf{D}) \odot (\hat{\mathbf{D}} - \mathbf{D}) |_{F}$&lt;/p&gt;
&lt;p&gt;$\odot$: element-wise product&lt;/p&gt;
&lt;p&gt;$| \cdot |$: Frobenius norm&lt;/p&gt;
&lt;p&gt;$(\hat{\mathbf{D}} - \mathbf{D})$ : distance matrix of the real and predicted conformations $(\hat{\mathbf{R}} - \mathbf{R})$&lt;/p&gt;
&lt;p&gt;$\hat{\mathbf{C}}^{(k)}$: normalized $k$-hop connectivity matrix&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$L_{\text{chem}}$: MAE or Cross entropy loss for Property prediction (ChemNet)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Total loss&lt;/p&gt;
&lt;p&gt;$L_{\text{total}} = \lambda L_{\text{phys}} + L_{\text{chem}}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Checkpoints&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Is Conn-k loss generally used in other conformation prediction models?&lt;/p&gt;
&lt;p&gt;No! But seems related to local distance loss.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is triplet descriptor generally used in other models?&lt;/p&gt;
&lt;p&gt;No!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>CMap based Drug Repositioning of BTZ to reverse the metastatic effect of GALNT14 in lung cancer</title>
        <link>https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/</link>
        <pubDate>Fri, 22 Jul 2022 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/</guid>
        <description>&lt;p&gt;Oncogene, &amp;lsquo;20&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.nature.com/articles/s41388-020-1316-2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;CMap based Drug Repositioning of BTZ to reverse the metastatic effect of GALNT14 in lung cancer&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/thumbnail.png&#34;
	width=&#34;1784&#34;
	height=&#34;690&#34;
	srcset=&#34;https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/thumbnail_hub7b0114b78cd33a49a5f6e1bdf51e130_146004_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/thumbnail_hub7b0114b78cd33a49a5f6e1bdf51e130_146004_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;258&#34;
		data-flex-basis=&#34;620px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;In-silico approach based on CMap to identify drug candidates for lung cancer metastasis&lt;/li&gt;
&lt;li&gt;Revealed the underlying mechanisms of undruggable target (GALNT14) and targeted the downstream transcription factor&lt;/li&gt;
&lt;li&gt;Repositioned drug: BTZ (Bortezomib)&lt;/li&gt;
&lt;li&gt;Integrated multiple independent expression signatures from cancer patients (TCGA), genetic perturbations(knock-down or overexpression), and drug treatment (CMap)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GALNT14: a putative driver of lung cancer metastasis, leading to poor survival &amp;amp; has poor druggability.&lt;/li&gt;
&lt;li&gt;Bortezomib: drug used for multiple myeloma and mantle cell lymphoma&lt;/li&gt;
&lt;li&gt;CMap: a collection of genome wide expression profiles of cell lines treated with &amp;gt; 20,000 chemicals&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;main-results&#34;&gt;Main Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_1.png&#34;
	width=&#34;1024&#34;
	height=&#34;1528&#34;
	srcset=&#34;https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_1_hu6d4a003a2f6221a0fe806f7889e0f77f_1360011_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_1_hu6d4a003a2f6221a0fe806f7889e0f77f_1360011_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;67&#34;
		data-flex-basis=&#34;160px&#34;
	
&gt;
Figure 1. GALNT14 as a putative molecular target for lung cancer metastasis.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1a. TCGA Lung adenocarcinoma cohort의 516명 lung cancer 환자의 transcriptome data.&lt;/li&gt;
&lt;li&gt;1b. relapse-free survival / DEG 분석에서 7개의 gene들이 검출되었고, 그 7개의 gene의 expression이 높은 group, 낮은 group으로 분류.&lt;/li&gt;
&lt;li&gt;1c. metastasis와 tumor signature가 high-expression group에서 enrich 되었음.&lt;/li&gt;
&lt;li&gt;1d. GALNT14만 단독으로 보아도 metastasis와 tumor 에서 enrich 되어 있음을 알 수 있음.&lt;/li&gt;
&lt;li&gt;1e, 1f. GALNT14이 각각 metastatic potential과 tumorigenic potential이 있다는 in vivo 실험 결과.&lt;/li&gt;
&lt;li&gt;1g. Metastatic lung cancer cell이 non-metastatic cancer보다 GALNT14 depletion에 더 vulnerable.&lt;/li&gt;
&lt;li&gt;1h. GALNT14이 survival에 분명한 negative correlation을 보임.&lt;br&gt;
이것으로 미루어보아, GALNT14이 lung cancer metastasis의 promising molecular target이라는 것을 알 수 있음.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_2.png&#34;
	width=&#34;946&#34;
	height=&#34;1304&#34;
	srcset=&#34;https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_2_hu005e6e010ae2c58e20e9931e08cc1b64_761825_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/cmap-based-drug-repositioning-of-btz-to-reverse-the-metastatic-effect-of-galnt14-in-lung-cancer/cmap_2_hu005e6e010ae2c58e20e9931e08cc1b64_761825_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;72&#34;
		data-flex-basis=&#34;174px&#34;
	
&gt;
Figure 5. In vivo validation of the anti-metastatic effect of BTZ.
BTZ의 anti-migration, anti-invasion effect를 in vitro level에서 확인한 뒤 in vivo에서 cancer metastasis efficacy를 확인한 실험 결과&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5a. 쥐의 꼬리 정맥으로 H460 lung cancer cell을 주입하여 local metastasis를 유도하고 control 군, BTZ 처리군, CFZ 처리군으로 구분하였음.&lt;/li&gt;
&lt;li&gt;5b. BTZ, CFZ의 proteasome inhibition을 확인하기 위해 혈액에서 proteasome activity를 측정한 결과. 상당히 줄어들었음을 알 수 있음. P.C. 는 positive control&lt;/li&gt;
&lt;li&gt;5c. Body weight 정보. 항암제 처리로 인해 다른 조직 등에 dramatic한 영향은 없었음.&lt;/li&gt;
&lt;li&gt;5d. Lung cancer로 metastasis 유무 사진. Vehicle과 CFZ는 상당부분 Metastasis가 일어난 것을 볼 수 있지만 BTZ는 6개 중 1개만 미약하게 metastasis 발생.&lt;/li&gt;
&lt;li&gt;5e. H&amp;amp;E staining 후의 lung image&lt;/li&gt;
&lt;li&gt;5f. tumor nodule size의 average. BTZ는 매우 작음.&lt;/li&gt;
&lt;li&gt;5g. tumor nodule의 수 분포. BTZ 매우 적음.&lt;/li&gt;
&lt;li&gt;5h. proteasome activity 차이&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Unlike other studies that used CMap, they focused exclusively on a target gene related to a pertinent phenotype and identified BTZ as a drug candidate with novel anti-metastatic effects.&lt;/li&gt;
&lt;li&gt;In pathway level, the most enriched pathway was TGF￼ signaling, and they also identified the GALNT14-TGF￼ signature, which has invasive properties that are attenuated by BTZ.&lt;/li&gt;
&lt;li&gt;They integrated multiple independent expression signatures from cancer patients(TCGA), genetic perturbations(knock-down or overexpression), and drug treatment(CMap).&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Few-shot Learning with Graph Neural Networks</title>
        <link>https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/</link>
        <pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/</guid>
        <description>&lt;p&gt;ICLR, &amp;lsquo;17,&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1711.04043&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Few-Shot Learning with Graph Neural Networks&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/thumbnail.png&#34;
	width=&#34;820&#34;
	height=&#34;582&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/thumbnail_hu8ba07e37572d4400f76afaadd55834a8_158186_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/thumbnail_hu8ba07e37572d4400f76afaadd55834a8_158186_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;338px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Used similarity value between samples for few shot learning.&lt;/li&gt;
&lt;li&gt;Regard each sample as nodes, and similarity kernel as edges.&lt;/li&gt;
&lt;li&gt;Similarity kernel is trainable. (i.e. Not just simple inner product)&lt;/li&gt;
&lt;li&gt;Can be applied to semi-supervised learning and active learning.&lt;/li&gt;
&lt;li&gt;State-of-the-art performance in Omniglot and Mini-ImageNet in 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keywords&#34;&gt;Keywords&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Few shot learning&lt;/li&gt;
&lt;li&gt;Graph neural network&lt;/li&gt;
&lt;li&gt;Semi-supervised learning&lt;/li&gt;
&lt;li&gt;Active learning with Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks.&lt;/li&gt;
&lt;li&gt;However, there are some tasks(e.g. few shot learning) that cannot achieve high performance with conventional methods.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;New&lt;/strong&gt; supervised learning setup
&lt;ul&gt;
&lt;li&gt;Input-output setup:
&lt;ul&gt;
&lt;li&gt;With i.i.d. samples  of collections of images and their associated label similarity&lt;/li&gt;
&lt;li&gt;cf) conventional setup: i.i.d. samples of images and their associated labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Authors&amp;rsquo; model can be extended to semi-supervised and active learning
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Semi-supervised learning:&lt;/p&gt;
&lt;p&gt;Learning from a mixture of labeled and unlabeled examples&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled2.png&#34;
	width=&#34;1024&#34;
	height=&#34;428&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled2_huc1bfe4fa7e46f3525a992e0a384e32c8_196077_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled2_huc1bfe4fa7e46f3525a992e0a384e32c8_196077_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;https://blog.est.ai/2020/11/ssl/&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Active learning:&lt;/p&gt;
&lt;p&gt;The learner has the option to request those missing labels that will be most helpful for the prediction task&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled3.png&#34;
	width=&#34;1704&#34;
	height=&#34;610&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled3_huc6f63af45ed0ccc0628e36f08d894c39_473048_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled3_huc6f63af45ed0ccc0628e36f08d894c39_473048_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;ICML 2019 active learning tutorial&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;279&#34;
		data-flex-basis=&#34;670px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled4.png&#34;
	width=&#34;1024&#34;
	height=&#34;428&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled4_hu29a98bb38a9c671de7bd69329e42d18f_525600_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled4_hu29a98bb38a9c671de7bd69329e42d18f_525600_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Annotated by JH Gu&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;closely-related-works-and-ideas&#34;&gt;Closely related works and ideas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[Research article] Matching Networks for One shot learning - Vinyals et al.(2016)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mapped support set of images into the desired label.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And developed an end-to-end trainable k-nearest neighbors, accepting those support sets as input via attention LSTM.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled5.png&#34;
	width=&#34;2610&#34;
	height=&#34;1132&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled5_hu0e12fc917e2d1e75fc1d11c369c80fd5_198931_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled5_hu0e12fc917e2d1e75fc1d11c369c80fd5_198931_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Vinyals et al.(2016), cited over 3000 times&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;230&#34;
		data-flex-basis=&#34;553px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled6.png&#34;
	width=&#34;1722&#34;
	height=&#34;1194&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled6_hu682bd0f012da9d625d12e31d1ed30fa9_900563_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled6_hu682bd0f012da9d625d12e31d1ed30fa9_900563_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled7.png&#34;
	width=&#34;2114&#34;
	height=&#34;786&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled7_huc8e7223a5841039c99c90f590a76f75b_236044_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled7_huc8e7223a5841039c99c90f590a76f75b_236044_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;645px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$k$: number of data in support set&lt;/li&gt;
&lt;li&gt;$\hat{x}$: new data&lt;/li&gt;
&lt;li&gt;$\hat{y}$: its class&lt;/li&gt;
&lt;li&gt;$\hat{y}$ is a linear combination of the labels in the support set&lt;/li&gt;
&lt;li&gt;$a$: attention mechanism, which is a kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[Research article] Prototypical Networks for Few-shot Learning - Snell et al.(2017)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled8.png&#34;
	width=&#34;1096&#34;
	height=&#34;320&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled8_hu673a9f33c51a89e6c015e050431013a1_40631_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled8_hu673a9f33c51a89e6c015e050431013a1_40631_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;342&#34;
		data-flex-basis=&#34;822px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.kakaocdn.net/dn/QTf75/btqV2blwJop/GPGDedaSftJNpHDXvq2XGk/img.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;https://blog.kakaocdn.net/dn/QTf75/btqV2blwJop/GPGDedaSftJNpHDXvq2XGk/img.gif&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Authors point out the overfitting problem of Matching networks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prototype: Center(mean) of each class cluster&lt;/li&gt;
&lt;li&gt;Similarity: $-\text{Euclidean distance}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[Review article] Geometric deep learning - Bronstein et al.(2017)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled9.png&#34;
	width=&#34;3380&#34;
	height=&#34;760&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled9_huce1372555d0695877b90856ae5320997_165518_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled9_huce1372555d0695877b90856ae5320997_165518_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;444&#34;
		data-flex-basis=&#34;1067px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Geometric deep learning is an umbrella term for emerging techniques attempting to generalize deep models to non-Euclidian domains such as graphs and manifolds&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[Research article] Message passing - Gilmer et al.(2017)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled10.png&#34;
	width=&#34;3232&#34;
	height=&#34;614&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled10_hu1eef0041f9c47cde680bf6ae160fab5e_104258_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled10_hu1eef0041f9c47cde680bf6ae160fab5e_104258_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;526&#34;
		data-flex-basis=&#34;1263px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;$$
m^{t+1}_v = \sum_{w \in N(v)} M_t(h^t_v, h^t_w, e_{vw}) \ h^{t+1}_v = U_t(h^t_v, m^{t+1}_v )
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M_t$: message functions&lt;/li&gt;
&lt;li&gt;$U_t$: vertex update functions&lt;/li&gt;
&lt;li&gt;$h^t_v$: hidden states of node $v$ in the graph at time $t$&lt;/li&gt;
&lt;li&gt;$m^{t+1}_v$: messages of node $v$ in the graph at time $t+1$&lt;/li&gt;
&lt;li&gt;$N(v)$: neighbors of node $v$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*oSQyFjtUkI7_u7lJXWU68Q.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;GIF from https://towardsdatascience.com/introduction-to-message-passing-neural-networks-e670dc103a87&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;problem-set-up&#34;&gt;Problem set-up&lt;/h2&gt;
&lt;p&gt;Authors view the task as a supervised interpolation problem on a graph&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nodes: &lt;strong&gt;Images&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Edges: &lt;strong&gt;Similarity kernels →&lt;/strong&gt; &lt;em&gt;TRAINABLE&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-set-up&#34;&gt;General set-up&lt;/h3&gt;
&lt;p&gt;Input-output pairs $(\mathcal{T}_i, Y_i)_i$ drawn from i.i.d. from a distribution $\mathcal{P}$ of partially labeled image collections&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled11.png&#34;
	width=&#34;4000&#34;
	height=&#34;423&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled11_hu5436287761932c1cedb9fbd5fa84a3d6_135160_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled11_hu5436287761932c1cedb9fbd5fa84a3d6_135160_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;945&#34;
		data-flex-basis=&#34;2269px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s$: # labeled samples&lt;/li&gt;
&lt;li&gt;$r$: # unlabled samples&lt;/li&gt;
&lt;li&gt;$t$: # samples to classify&lt;/li&gt;
&lt;li&gt;$K$: # classes&lt;/li&gt;
&lt;li&gt;$\mathcal{P}_l(\mathbb{R}^N)$: class-specific image distribution over $\mathbb{R}^N$&lt;/li&gt;
&lt;li&gt;targets $Y_i$ are associated with $\bar{x}_1, &amp;hellip;, \bar{x}_t \in \mathcal{T}_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learning objective:&lt;/p&gt;
&lt;p&gt;$\min_\Theta \frac{1}{L} \sum_{i \leq L} \ell(\Phi(\mathcal{T}_i, \Theta), Y_i) + \mathcal{R}(\Theta)$&lt;/p&gt;
&lt;p&gt;($\mathcal{R}$ is the standard regularization objective)&lt;/p&gt;
&lt;h3 id=&#34;few-shot-learning-setting&#34;&gt;Few shot learning setting&lt;/h3&gt;
&lt;p&gt;$r=0, t=1, s=qK$   $\longrightarrow$   $q-\text{shot} , K-\text{way}$&lt;/p&gt;
&lt;h3 id=&#34;semi-supervised-learning-setting&#34;&gt;Semi-supervised learning setting&lt;/h3&gt;
&lt;p&gt;$r &amp;gt; 0, t=1$&lt;/p&gt;
&lt;p&gt;Model can use the auxiliary images(unlabeled set) ${ \tilde{x}_1, &amp;hellip;, \tilde{x}_r }$ to improve the prediction accuracy, by leveraging the fact that these samples are drawn from the common distributions.&lt;/p&gt;
&lt;h3 id=&#34;active-learning-setting&#34;&gt;Active learning setting&lt;/h3&gt;
&lt;p&gt;The learner has the ability to request labels from the auxiliary images ${\tilde{x}_1, &amp;hellip;, \tilde{x}_r}$.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled12.png&#34;
	width=&#34;1188&#34;
	height=&#34;900&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled12_hu518b9887168c60fff5bff4e932a1b548_336596_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled12_hu518b9887168c60fff5bff4e932a1b548_336596_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;316px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\phi(x)$: CNN&lt;/li&gt;
&lt;li&gt;$h(l)$: One-hot encoded label(for labeled set), or uniform distribution(for unlabeled set)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;set-and-graph-input-representations&#34;&gt;Set and Graph Input Representations&lt;/h3&gt;
&lt;p&gt;The goal of few shot learning:&lt;/p&gt;
&lt;p&gt;To propagate label information from labeled samples towards the unlabeled query image&lt;/p&gt;
&lt;p&gt;→ The propagation can be formalized as a posterior inference over a graphical model&lt;/p&gt;
&lt;p&gt;$G_\mathcal{T} = (V,E)$&lt;/p&gt;
&lt;p&gt;Similarity measure is not pre-specified, but learned!&lt;/p&gt;
&lt;p&gt;c.f.) in Siamese network, the similarity measure is fixed(L1 distance)!&lt;/p&gt;
&lt;p&gt;본 논문(Few shot learning with GNN)에 쓰인 문장 구조가 이상해서 헷갈리게 쓰여있음.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled13.png&#34;
	width=&#34;3024&#34;
	height=&#34;1004&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled13_hu195ce8430d176ca0bb51f3bd3cc2cd4a_386759_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled13_hu195ce8430d176ca0bb51f3bd3cc2cd4a_386759_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Koch et al.(2015), https://tyami.github.io/deep learning/Siamese-neural-networks/&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;301&#34;
		data-flex-basis=&#34;722px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;graph-neural-networks&#34;&gt;Graph Neural Networks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled14.png&#34;
	width=&#34;1982&#34;
	height=&#34;838&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled14_hu2b7b1594699112bb43a74ec4c994064f_379402_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled14_hu2b7b1594699112bb43a74ec4c994064f_379402_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;236&#34;
		data-flex-basis=&#34;567px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We are given an input signal $F \in \mathbb{R}^{V \times d}$ on the vertices of a weighted graph $G$.&lt;/p&gt;
&lt;p&gt;Then we consider a family, or a set &amp;ldquo;$\mathcal{A}$&amp;rdquo; of graph intrinsic linear operators.&lt;/p&gt;
&lt;p&gt;$\mathcal{A} = {\tilde{A}^{(k)}, \mathbf{1}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Linear operator&lt;/p&gt;
&lt;p&gt;e.g.) Simplest linear operator is adjacency operator $A$, where $(AF)&lt;em&gt;i = \sum&lt;/em&gt;{j \sim i} w_{i,j}F_j$ ($w_{i,j}$ is associated weight)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GNN layer&lt;/p&gt;
&lt;p&gt;A GNN layer $\text{Gc}(\cdot)$ receives as input a signal $\mathbf{x}^{(k)} \in \mathbb{R}^{V\times d_k}$ and produces $\mathbf{x}^{(k+1)} \in \mathbb{R}^{V\times d_{k+1}}$&lt;/p&gt;
&lt;p&gt;$$
\mathbf{x}^{(k+1)} = \text{Gc}(\mathbf{x}^{(k)}) = \rho\Big(\sum_{B\in\mathcal{A}} B\mathbf{x}^{(k)}\theta^{(k)}_{B, l}\Big )
$$&lt;/p&gt;
&lt;p&gt;$\mathbf{x}^{(k)}$: representation vector of a certain node at time step $k$&lt;/p&gt;
&lt;p&gt;$\theta$: trainable parameters&lt;/p&gt;
&lt;p&gt;$\rho$: Leaky ReLU&lt;/p&gt;
&lt;p&gt;Construction of edge feature matrix, inspired by message passing algorithm&lt;/p&gt;
&lt;p&gt;$$
\tilde{A}^{(k)}_{i, j} = \varphi_{\tilde{\theta}}(\mathbf{x}^{(k)}_i, \mathbf{x}^{(k)}_j )
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{A}^{(k)}_{i, j}$: &lt;strong&gt;learned&lt;/strong&gt; edge features from the node&amp;rsquo;s current hidden representation(at time step $k$)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\varphi$: a metric and a symmetric function parameterized with neural network&lt;/p&gt;
&lt;p&gt;$$
\varphi_{\tilde{\theta}}(\mathbf{x}^{(k)}_i, \mathbf{x}^{(k)}_j ) = \text{MLP}_{\tilde{\theta}}(abs(\mathbf{x}^{(k)}_i - \mathbf{x}^{(k)}_j))
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ $\tilde{A}^{(k)}$ is then normalized by row-wise softmax&lt;/p&gt;
&lt;p&gt;→ And added to the family $\mathcal{A} = {\tilde{A}^{(k)}, \mathbf{1}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{1}$: Identity matrix, which is the self-edge to aggregate vertex&amp;rsquo;s own features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Construction of initial node features&lt;/p&gt;
&lt;p&gt;$$
\mathbf{x}^{(0)}_i = (\phi(x_i), h(l_i))
$$&lt;/p&gt;
&lt;p&gt;$\phi$: convolutional neural network&lt;/p&gt;
&lt;p&gt;$h(l) \in \mathbb{R}^K_+$ : a one-hot encoding of the label&lt;/p&gt;
&lt;p&gt;For images with unknown label, $\tilde{x}_j$(unlabeled data) and  $\bar{x}_j$(test data), $h(l_j)$ is set with uniform distribution.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;h3 id=&#34;few-shot-and-semi-supervised-learning&#34;&gt;Few-shot and Semi-supervised learning&lt;/h3&gt;
&lt;p&gt;The final layer of GNN is a softmax mapping. We then use cross-entropy loss:&lt;/p&gt;
&lt;p&gt;$$
\ell(\Phi(\mathcal{T}; \Theta), Y) = -\sum_k y_k \log P(Y_* = y_k , |, \mathcal{T})
$$&lt;/p&gt;
&lt;p&gt;The semi-supervised setting is trained identically, but the initial label fields of $\tilde{x}_j$s will be filled with uniform distribution.&lt;/p&gt;
&lt;h3 id=&#34;active-learning-with-attention&#34;&gt;Active learning (with attention)&lt;/h3&gt;
&lt;p&gt;In active learning, the model has the intrinsic ability to query for one of the labels from ${ \tilde{x}_1, &amp;hellip;, \tilde{x}_r }$.&lt;/p&gt;
&lt;p&gt;The network will learn to ask for the most informative label to classify the sample $\bar{x}$.&lt;/p&gt;
&lt;p&gt;The querying is done after the first layer of GNN by using a softmax attention over the unlabeled nodes of the graph.&lt;/p&gt;
&lt;p&gt;Attention&lt;/p&gt;
&lt;p&gt;We apply a function $g(\mathbf{x}^{(1)}_i) \in \mathbb{R}^1$ that maps each unlabeld vector node to a scalar value.&lt;/p&gt;
&lt;p&gt;A softmax is applied over the ${1, &amp;hellip;, r}$ scalar values obtained after applying $g$:&lt;/p&gt;
&lt;p&gt;$r$: # unlabeled samples&lt;/p&gt;
&lt;p&gt;$$
\text{Attention} = \text{Softmax}(g(\mathbf{x}^{(1)}_{{1,&amp;hellip;,r}}))
$$&lt;/p&gt;
&lt;p&gt;To query only one sample we set all elements to zero except for one. → $\text{Attention}&#39;$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At training, model randomly samples one value based on its multinomial probability.&lt;/li&gt;
&lt;li&gt;At test, model just keeps the maximum value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we multiply this with the label vectors&lt;/p&gt;
&lt;p&gt;$$
w \cdot h(l_{i*}) = \langle \text{Attention}&amp;rsquo;, h(l_{{1, &amp;hellip;, r}}) \rangle
$$&lt;/p&gt;
&lt;p&gt;($w$ is scaling factor)&lt;/p&gt;
&lt;p&gt;This value is then summed to the current representation.&lt;/p&gt;
&lt;p&gt;$$
\mathbf{x}^{(1)}_{i*} = [\text{Gc}(\mathbf{x}^{(0)}_{i*}), \mathbf{x}^{(0)}_{i*}] = [\text{Gc}(\mathbf{x}^{(0)}_{i*}), (\phi(x_{i*}), h(l_{i*}))]
$$&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;few-shot-learning&#34;&gt;Few-shot learning&lt;/h3&gt;
&lt;p&gt;Omniglot&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled15.png&#34;
	width=&#34;1744&#34;
	height=&#34;720&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled15_hud1dea68979c0bbf767eb191d1fd1d37e_221954_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled15_hud1dea68979c0bbf767eb191d1fd1d37e_221954_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;581px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;# of parameters: $\sim5\text{M} (\text{TCML})$, $\sim300 \text{K}(3 \text{layers GNN})$&lt;/p&gt;
&lt;p&gt;Omniglot: 1,623 characters  X 20 examples for each characters&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled16.png&#34;
	width=&#34;2170&#34;
	height=&#34;945&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled16_hu734905f970a0a113fc187b3c1a108a68_954773_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled16_hu734905f970a0a113fc187b3c1a108a68_954773_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Omniglot&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;229&#34;
		data-flex-basis=&#34;551px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Mini-ImageNet: Originally introduced by Vinyals et al.(2016)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled17.png&#34;
	width=&#34;1628&#34;
	height=&#34;584&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled17_hu78e7bfaf35d1da92942e4f4a1fe7dadb_170810_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled17_hu78e7bfaf35d1da92942e4f4a1fe7dadb_170810_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;278&#34;
		data-flex-basis=&#34;669px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;# of parameters: $\sim 11\text{M} (\text{TCML})$, $\sim 400 \text{K}(3 \text{ layers GNN})$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled18.png&#34;
	width=&#34;795&#34;
	height=&#34;400&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled18_hu379e1637ed83ad26af73f3539cf37e81_707619_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled18_hu379e1637ed83ad26af73f3539cf37e81_707619_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Mini-ImageNet&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;477px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Mini-ImageNet&lt;/p&gt;
&lt;p&gt;Divided into 64 training, 16 validation, 20 testing classes each containing 600 examples.&lt;/p&gt;
&lt;h3 id=&#34;semi-supervised-learning&#34;&gt;Semi-supervised learning&lt;/h3&gt;
&lt;p&gt;Omniglot&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled19.png&#34;
	width=&#34;1444&#34;
	height=&#34;304&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled19_hu21116ea201fb3837edc6f9ad35132df9_60736_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled19_hu21116ea201fb3837edc6f9ad35132df9_60736_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;475&#34;
		data-flex-basis=&#34;1140px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Mini-ImageNet&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled20.png&#34;
	width=&#34;1666&#34;
	height=&#34;342&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled20_hu1158a4c24840395ac6dcb63f5d7034ea_78788_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled20_hu1158a4c24840395ac6dcb63f5d7034ea_78788_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;487&#34;
		data-flex-basis=&#34;1169px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;active-learning&#34;&gt;Active learning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled21.png&#34;
	width=&#34;1676&#34;
	height=&#34;444&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled21_hu57c3addab4b9d756f0d1227c98fe6ef2_119500_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/imgs/Untitled21_hu57c3addab4b9d756f0d1227c98fe6ef2_119500_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;377&#34;
		data-flex-basis=&#34;905px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Random: Network chooses a random sample to be labeled, instead of one that maximally reduces the loss of the classification task $\mathcal{T}$&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>MAML: Model-Agnostic Meta -Learning for Fast Adaptation of Deep Networks</title>
        <link>https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/</link>
        <pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/</guid>
        <description>&lt;p&gt;ICML, &amp;lsquo;17&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1703.03400&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/thumbnail.png&#34;
	width=&#34;650&#34;
	height=&#34;292&#34;
	srcset=&#34;https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/thumbnail_huac13bc7b7974135f98c493a6d9abadaf_36220_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/maml-model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks/thumbnail_huac13bc7b7974135f98c493a6d9abadaf_36220_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;222&#34;
		data-flex-basis=&#34;534px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MAML is a general and model-agnostic algorithm that can be directly applied to a model trained with &lt;strong&gt;gradient descent&lt;/strong&gt; procedure.&lt;/li&gt;
&lt;li&gt;MAML does not expand the number of learned parameters.&lt;/li&gt;
&lt;li&gt;MAML does not place constraints on the model architecture.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keywords&#34;&gt;Keywords&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Model agnostic&lt;/li&gt;
&lt;li&gt;Fast adaptation&lt;/li&gt;
&lt;li&gt;Optimization based approach&lt;/li&gt;
&lt;li&gt;Learning good model parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Common Approaches of Meta-Learning and MAML&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;MAML is one of the most influential model of optimization-based approaches.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;https://youtu.be/Izqod36syY8&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A few terminologies of meta-learning problems&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Goal of ideal artificial agent:&lt;/p&gt;
&lt;p&gt;Learning and adapting quickly from only a few examples.&lt;/p&gt;
&lt;p&gt;To do so, an agent must..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrate its prior experience with a small amount of new information.&lt;/li&gt;
&lt;li&gt;Avoid overfitting to the new data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ Meta-learning has same goals.&lt;/p&gt;
&lt;p&gt;MAML:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The key idea of MAML is to train the model&amp;rsquo;s &lt;strong&gt;initial parameters&lt;/strong&gt; such that the model has maximal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Learning process of MAML:&lt;/p&gt;
&lt;p&gt;MAML maximizes the sensitivity of the loss functions of new tasks.&lt;/p&gt;
&lt;p&gt;Authors demonstrated the algorithm on three different model types.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Few-shot regression&lt;/li&gt;
&lt;li&gt;Image classification&lt;/li&gt;
&lt;li&gt;Reinforcement learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-model-agnostic-meta-learning&#34;&gt;2. Model-Agnostic Meta Learning&lt;/h2&gt;
&lt;h3 id=&#34;21-meta-learning-problem-set-up&#34;&gt;2.1. Meta-Learning Problem Set-Up&lt;/h3&gt;
&lt;p&gt;To apply MAML to a variety of learning problems, authors introduce a generic notion of a learning task:&lt;/p&gt;
&lt;p&gt;$\mathcal{T} = { \mathcal{L}(\mathbf{x}_1, \mathbf{a}_1, &amp;hellip;, \mathbf{x}_H, \mathbf{a}_H), q(\mathbf{x}_1), q(\mathbf{x}_{t+1}|\mathbf{x}_t, \mathbf{a}_t), H }$&lt;/p&gt;
&lt;p&gt;Each task $\mathcal{T}$ consists of..&lt;/p&gt;
&lt;p&gt;$\mathcal{L}$: a loss function, might be misclassification loss or a cost function in a Markov decision process&lt;/p&gt;
&lt;p&gt;$q(\mathbf{x}_1)$: a distribution over initial observations&lt;/p&gt;
&lt;p&gt;$q(\mathbf{x}_{t+1}|\mathbf{x}_t , \mathbf{a}_t)$: a transition distribution&lt;/p&gt;
&lt;p&gt;$H$: an episode length(e.g. in i.i.d. supervised learning problems, the length $H = 1$.)&lt;/p&gt;
&lt;p&gt;Authors consider a distribution over tasks $p(\mathcal{T})$&lt;/p&gt;
&lt;p&gt;Meta-training:&lt;/p&gt;
&lt;p&gt;A new task $\mathcal{T}_i$ is sampled from $p(\mathcal{T})$.&lt;/p&gt;
&lt;p&gt;The model is trained with only $K$ samples drawn from $q_i$.&lt;/p&gt;
&lt;p&gt;Loss $\mathcal{L}_{\mathcal{T}_i}$ is calculated and feedbacked to model.&lt;/p&gt;
&lt;p&gt;Model $f$ is tested on new samples from $\mathcal{T}_i$.&lt;/p&gt;
&lt;p&gt;The model $f$ is then improved by considering how the $test$ error on new data from $q_i$ changes with respect to the parameters.&lt;/p&gt;
&lt;h3 id=&#34;22-a-model-agnostic-meta-learning-algorithm&#34;&gt;2.2. A Model-Agnostic Meta-Learning Algorithm&lt;/h3&gt;
&lt;p&gt;Intuition: Some internal representations are more transferrable than others. How can we encourage the emergence of such general-purpose representations?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A model $f_\theta$ has paramters $\theta$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each task $\mathcal{T}_i$, $f_\theta$&amp;rsquo;s parameters $\theta$ become $\theta_i&amp;rsquo;$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Algorithm&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled6.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;cf) Terminologies for below description(temporarily defined by JH Gu)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled7.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Divide tasks&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Separate tasks into meta-training task set(${\mathcal{T}_i^{\text{tr}}}$) and meta-test task set(${\mathcal{T}_i^{\text{test}}}$).&lt;/p&gt;
&lt;p&gt;(We can think of ${\mathcal{T}_i^{\text{tr}}}$ as monthly tests(모의고사), and ${\mathcal{T}_i^{\text{test}}}$ as annual tests(수능))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For each task, divide each samples into $\mathcal{D}_{\mathcal{T}_i}^{\text{study}}$(task-specific samples for studying, also called as support set), $\mathcal{D}_{\mathcal{T_i}}^{\text{check}}$(task-specific samples for checking, also called as query set)&lt;/p&gt;
&lt;p&gt;(We can think of $\mathcal{D}_{\mathcal{T}_i}^{\text{study}}$ as 필수예제 in 수학의 정석, and $\mathcal{D}_{\mathcal{T}_i}^{\text{check}}$ as 연습문제 in 수학의 정석)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Meta-training using meta-training task set ${\mathcal{T}_i^{\text{tr}}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Inner loop(task-specific $K$-shot learning)&lt;/p&gt;
&lt;p&gt;For each $\mathcal{T}_i$ in ${\mathcal{T}_i^{\text{tr}}}$, a new parameter $\theta_i&amp;rsquo;$ is created.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Each $\theta_i&amp;rsquo;$ is initialized as $\theta$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With task-specific samples for studying($\mathcal{D}_{\mathcal{T}_i^{\text{tr}}}^{\text{study}}$), each $\theta_i&amp;rsquo;$ is updated by:&lt;/p&gt;
&lt;p&gt;$$
\theta_i&amp;rsquo; = \theta - \alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}(f_\theta)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Outer loop(meta-learning across tasks)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;With task-specific samples for checking($\mathcal{D}_{\mathcal{T_i}^{\text{tr}}}^{\text{check}}$), $\theta$ is updated by:&lt;/p&gt;
&lt;p&gt;$$
\theta = \theta - \beta \nabla_\theta \sum_{\mathcal{T}_i \sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_i} (f_{\theta_i&amp;rsquo;})
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;cf) second-order derivative(Hessian) problem&lt;/p&gt;
&lt;p&gt;The MAML meta-gradient update(outer loop) involves a gradient through a gradient, which can be resource-intensive. This requires an additional backward pass through $f$ to compute Hessian vector products.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*} \textcolor{red}{\theta&amp;rsquo;} &amp;amp;= \theta - \alpha \nabla_\theta \mathcal{L}(\theta) \ \nabla_\theta \mathcal{L}(\textcolor{red}{\theta&amp;rsquo;}) &amp;amp;= (\textcolor{red}\nabla_{\textcolor{red}{\theta&amp;rsquo;}} \mathcal{L}(\textcolor{red}{\theta&amp;rsquo;})) \cdot (\nabla_\theta \textcolor{red}{\theta&amp;rsquo;}) \ &amp;amp;= (\textcolor{red}\nabla_{\textcolor{red}{\theta&amp;rsquo;}} \mathcal{L}(\textcolor{red}{\theta&amp;rsquo;})) \cdot (\nabla_\theta (\theta - \alpha \nabla_\theta \mathcal{L}(\theta)) \ &amp;amp;\approx (\textcolor{red}\nabla_{\textcolor{red}{\theta&amp;rsquo;}} \mathcal{L}(\textcolor{red}{\theta&amp;rsquo;})) \cdot (\nabla_\theta \theta) \ &amp;amp;= (\textcolor{red}\nabla_{\textcolor{red}{\theta&amp;rsquo;}} \mathcal{L}(\textcolor{red}{\theta&amp;rsquo;})) \end{align*}
$$&lt;/p&gt;
&lt;p&gt;Authors included a comparison to drop the backward pass term and using just the first-order approximation, which showed not much difference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled8.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Measure model performance using meta-test task set ${\mathcal{T}_i^{\text{test}}}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each $\mathcal{T}_i$ in ${\mathcal{T}_i^{\text{test}}}$, adjust task-specific parameters with $\mathcal{D}_{\mathcal{T}_i^{\text{test}}}^{\text{study}}$.&lt;/li&gt;
&lt;li&gt;Test the performance with $\mathcal{D}_{\mathcal{T_i}^{\text{test}}}^{\text{check}}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-species-of-maml&#34;&gt;3. Species of MAML&lt;/h2&gt;
&lt;h3 id=&#34;31-supervised-regression-and-classification&#34;&gt;3.1. Supervised Regression and Classification&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Algorithm&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled9.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Formalizing supervised regression and classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Horizon $H = 1$&lt;/li&gt;
&lt;li&gt;Drop the timestep subscript on $\mathbf{x}_t$ (since model accepts a single input and produces a single output)&lt;/li&gt;
&lt;li&gt;The task $\mathcal{T}_i$ generates $K$ i.i.d. observations $\mathbf{x}$ from $q_i$&lt;/li&gt;
&lt;li&gt;Task loss is represented by the error between the model&amp;rsquo;s output for $\mathbf{x}$ and the corresponding target values $\mathbf{y}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loss functions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MSE for regression&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\mathcal{T}_i}(f_\phi) = \sum_{\mathbf{x}^{(j)}, \mathbf{y}^{(j)} \sim \mathcal{T}_i} | f_\phi(\mathbf{x}^{(j)}) - \mathbf{y}^{(j)}|^2_2
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross entropy loss for discrete classification&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\mathcal{T}_i}(f_\phi) = \sum_{\mathbf{x}^{(j)}, \mathbf{y}^{(j)} \sim \mathcal{T}_i} \big\{ \mathbf{y}^{(j)} \log f_\phi (\mathbf{x}^{(j)}) - (1-\mathbf{y}^{(j)})\log(1-f_\phi(\mathbf{x}^{(j)}) \big\}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-reinforcement-learning&#34;&gt;3.2. Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Algorithm&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled10.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Untitled&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Goal of MAML in RL:&lt;/p&gt;
&lt;p&gt;Quickly acquire a policy for a new test task using only a small amount of experience in the test setting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Formalizing RL&lt;/p&gt;
&lt;p&gt;Each RL task $\mathcal{T}_i$ contains..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initial state distribution $q_i(\mathbf{x}_1)$&lt;/li&gt;
&lt;li&gt;Transition distribution $q_i(\mathbf{x}_{t+1}|\mathbf{x}_t, \mathbf{a}_t)$
&lt;ul&gt;
&lt;li&gt;$\mathbf{a}_t$: action&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Loss $\mathcal{L}_{\mathcal{T}_i}$, which corresponds to the negative reward function $R$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, entire task is a Markov decision process(MDP) with horizon $H$&lt;/p&gt;
&lt;p&gt;The model being learned, $f_\theta$, is a policy that maps from states $\mathbf{x}_t$ to a distribution over actions $\mathbf{a}_t$ at each timestep $t \in { 1, &amp;hellip;, H}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loss function for task $\mathcal{T}_i$ and model $f_\phi$:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\mathcal{T}_i}(f_\phi) = -\mathbb{E}_{\mathbf{x}_t, \mathbf{a}_t \sim f_\phi, q_{\mathcal{T}_i}} \bigg [ \sum_{t=1}^H R_i(\mathbf{x}_t, \mathbf{a}_t) \bigg ]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Policy gradient method&lt;/p&gt;
&lt;p&gt;Since the expected reward is generally not differentiable due to unknown dynamics, authors used policy gradient methods to estimate the gradient.&lt;/p&gt;
&lt;p&gt;The policy gradient method is an on-policy algorithm&lt;/p&gt;
&lt;p&gt;→ There are additional sampling procedures in step 5 and 8.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-comparison-with-related-works&#34;&gt;4. Comparison with related works&lt;/h2&gt;
&lt;p&gt;Comparison with other popular approaches&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Training a meta-learner that learns how to update the parameters of the learner&amp;rsquo;s model&lt;/p&gt;
&lt;p&gt;ex) On the optimization of a synaptic learning rule(Bengio et al. 1992)&lt;/p&gt;
&lt;p&gt;→ Requires additional parameters, while MAML does not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training to compare new examples in a learned metric space&lt;/p&gt;
&lt;p&gt;ex) Siamese networks(Koch, 2015), recurrence with attention mechanisms(Vinyals et al. 2016)&lt;/p&gt;
&lt;p&gt;→ Difficult to directly extend to our problems, such as reinforcement learning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training memory-augmented models&lt;/p&gt;
&lt;p&gt;ex) Meta-learning with memory-augmented neural networks(Santoro et al. 2016)&lt;/p&gt;
&lt;p&gt;The recurrent learner is trained to adapt to new tasks as it is rolled out.&lt;/p&gt;
&lt;p&gt;→ Not really straightforward.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-experimental-evaluation&#34;&gt;5. Experimental Evaluation&lt;/h2&gt;
&lt;p&gt;Three questions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can MAML enable fast learning of new tasks?&lt;/li&gt;
&lt;li&gt;Can MAML be used for meta-learning in multiple different domains?&lt;/li&gt;
&lt;li&gt;Can a model learned with MAML continue to improve with additional gradient updates and/or examples?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;51-regression&#34;&gt;5.1. Regression&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled11.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled12.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;52-classification&#34;&gt;5.2. Classification&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled13.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled14.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;53-reinforcement-learning&#34;&gt;5.3. Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled15.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/Untitled16.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/Izqod36syY8&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;KAIST NeuroAI JC_#1 Meta Learning (편집본)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ai.stanford.edu/~cbfinn/_files/dissertation.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://velog.io/@tobigs_xai/10%ec%a3%bc%ec%b0%a8-MAML-Model-agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-%eb%85%bc%eb%ac%b8-%eb%a6%ac%eb%b7%b0&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[10주차] (MAML) Model-agnostic Meta Learning for Fast Adaptation of Deep Networks 논문 리뷰&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Meta-Learning: Learning to Learn Fast&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://gujh14.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Links</title>
        <link>https://gujh14.github.io/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/links/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;links&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ts-logo-128.jpg&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>https://gujh14.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
