<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>ICLR on JH Gu&#39;s Tech Blog</title>
        <link>https://gujh14.github.io/categories/iclr/</link>
        <description>Recent content in ICLR on JH Gu&#39;s Tech Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 27 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gujh14.github.io/categories/iclr/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking</title>
        <link>https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/</link>
        <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/</guid>
        <description>&lt;img src="https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail.png" alt="Featured image of post DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking" /&gt;&lt;p&gt;ICLR, &amp;lsquo;23
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.01776&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This article is one of the first research work that formulated molecular docking as a generative problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Showed very interesting results with decent performance gain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are interested in molecular docking and diffusion models, this is definitely a must-read paper!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is highly recommended to watch &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=gAmTGw601dA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;youtube video&lt;/a&gt; explained by the authors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Molecular docking as a generative problem, not regression!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Problem of learning a distribution over ligand poses conditioned on the target protein structure $p(\mathbf{x} | \mathbf{y})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Used “Diffusion process” for generation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two separate model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Score model: $s(\mathbf{x}, \mathbf{y}, t)$&lt;/p&gt;
&lt;p&gt;Predicts score based on ligand pose $\mathbf{x}$, protein structure $\mathbf{y}$, and timestep $t$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confidence model: $d(\mathbf{x}, \mathbf{y})$&lt;/p&gt;
&lt;p&gt;Predicts whether the ligand pose has RMSD below 2Å compared to ground truth ligand pose&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diffusion on Product space $\mathbb{P}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reduced degrees of freedom $3n \rightarrow (m+6)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;h3 id=&#34;molecular-docking&#34;&gt;Molecular Docking&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled.png&#34;
	width=&#34;1942&#34;
	height=&#34;1116&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled_huf40d7e8c2d3df3da9d828794a85e8161_935838_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled_huf40d7e8c2d3df3da9d828794a85e8161_935838_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;417px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Definition:&lt;/p&gt;
&lt;p&gt;Predicting the position, orientation, and conformation of a ligand when bound to a target protein&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two types of tasks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Known-pocket docking
&lt;ul&gt;
&lt;li&gt;Given: position of the binding pocket&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Blind docking
&lt;ul&gt;
&lt;li&gt;More general setting: no prior knowledge about binding pocket&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;previous-works-search-based--regression-based&#34;&gt;Previous works: Search-based / Regression-based&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1.png&#34;
	width=&#34;1462&#34;
	height=&#34;1104&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1_huc1cd3bae2f7177be1a7cfda9b4564a85_264966_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1_huc1cd3bae2f7177be1a7cfda9b4564a85_264966_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;317px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Search based docking methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Traditional methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consist of parameterized physics-based &lt;strong&gt;scoring function&lt;/strong&gt; and a &lt;strong&gt;search algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scoring function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input: 3D structures&lt;/li&gt;
&lt;li&gt;Output: estimate of the quality/likelihood of the given pose&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stochastically modifies the ligand pose (position, orientation, torsion angles)&lt;/li&gt;
&lt;li&gt;Goal: finding the global optimum of the scoring function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ML has been applied to parameterize the scoring function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But very computationally expensive (large search space)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2.png&#34;
	width=&#34;1776&#34;
	height=&#34;166&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2_huf43150a0e47223ef3cf22aed6633617b_81428_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2_huf43150a0e47223ef3cf22aed6633617b_81428_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1069&#34;
		data-flex-basis=&#34;2567px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression based methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Recent deep learning method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Significant speedup compared to search based methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;No improvements in accuracy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3.png&#34;
	width=&#34;1152&#34;
	height=&#34;144&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3_hu6b088dca52a85205018e5f1b36628350_49655_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3_hu6b088dca52a85205018e5f1b36628350_49655_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;800&#34;
		data-flex-basis=&#34;1920px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.05146&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;EquiBind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4.png&#34;
	width=&#34;2030&#34;
	height=&#34;550&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4_hu663897c2650aaacc73297b198e4cc5b3_753604_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4_hu663897c2650aaacc73297b198e4cc5b3_753604_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;369&#34;
		data-flex-basis=&#34;885px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tried to tackle the blind docking task as a &lt;strong&gt;regression problem&lt;/strong&gt; by directly predicting pocket keypoints on both ligand and protein and aligning them.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.biorxiv.org/content/10.1101/2022.06.06.495043v3&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TANKBind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5.png&#34;
	width=&#34;1988&#34;
	height=&#34;732&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5_hub1f86d8413b43196d2dbec69310f8555_603864_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5_hub1f86d8413b43196d2dbec69310f8555_603864_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;271&#34;
		data-flex-basis=&#34;651px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improved over this by independently predicting a docking pose for each possible pocket and then ranking them.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.06069&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;E3Bind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6.png&#34;
	width=&#34;1536&#34;
	height=&#34;562&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6_hu13edc0df2b71d59ba0b45cb41de53f20_193128_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6_hu13edc0df2b71d59ba0b45cb41de53f20_193128_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used ligand-constrained &amp;amp; protein-constrained update layer to embed ligand atoms and iteratively updated coordinates.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;docking-objective&#34;&gt;Docking objective&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7.png&#34;
	width=&#34;1520&#34;
	height=&#34;1214&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7_hu5f4844bc8df901aef67687f97d63b662_915257_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7_hu5f4844bc8df901aef67687f97d63b662_915257_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;125&#34;
		data-flex-basis=&#34;300px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard evaluation metric:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mathcal{L}_{\epsilon} = \sum_{x, y} I_{\text{RMSD}(y, \hat{y}(x))&amp;lt;\epsilon}$:&lt;/p&gt;
&lt;p&gt;proportion of predictions with $\text{RMSD} &amp;lt; \epsilon$ → Not differentiable!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instead, we use $\text{argmin}_{\hat{y}} \lim_{\epsilon \rightarrow 0} \mathcal{L}_\epsilon$ as objective function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regression is suitable for docking only if it is unimodal.&lt;/li&gt;
&lt;li&gt;Docking has significant aleatoric (irreducible) &amp;amp; epistemic (reducible) uncertainty
&lt;ul&gt;
&lt;li&gt;Regression methods will minimize $\sum \|y - \hat{y}\|^2_2$ → will produce weighted mean of multiple modes&lt;/li&gt;
&lt;li&gt;On the other hand, generative model will populate all/most modes!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8.png&#34;
	width=&#34;2028&#34;
	height=&#34;1108&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8_hu6b313aafde63670bc308ca51bccee486_1187222_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8_hu6b313aafde63670bc308ca51bccee486_1187222_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;439px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression (EquiBind) model set conformer in the middle of the modes.&lt;/li&gt;
&lt;li&gt;Generative samples can populate conformer in most modes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9.png&#34;
	width=&#34;2002&#34;
	height=&#34;1090&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9_hu04edc1aff53bfad5a2de658c9efbf1ee_1153872_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9_hu04edc1aff53bfad5a2de658c9efbf1ee_1153872_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;440px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Much less steric clashes for generative models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;diffdock-overview&#34;&gt;DiffDock Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15.png&#34;
	width=&#34;1110&#34;
	height=&#34;534&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15_hu35359f9550b3436da89f85bd47cc7f53_260405_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15_hu35359f9550b3436da89f85bd47cc7f53_260405_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;498px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two-step approach
&lt;ul&gt;
&lt;li&gt;Score model: Reverse diffusion over translation, rotation, and torsion&lt;/li&gt;
&lt;li&gt;Confidence model: Predict whether or not each ligand pose is $\text{RMSD} &amp;lt; 2\text{Å}$ compared to ground truth ligand pose&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;score-model&#34;&gt;Score model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ligand pose: $\mathbb{R}^{3n}$ ($n$: number of atoms)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But molecular docking needs far less degrees of freedom.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16.png&#34;
	width=&#34;2044&#34;
	height=&#34;1074&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16_hu38cf56affd34313fb9003ca22c483d35_693157_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16_hu38cf56affd34313fb9003ca22c483d35_693157_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;456px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Reduced degree of freedom: $(m+6)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Local structure: Fixed (rigid) after conformer generation with RDKit &lt;code&gt;EmbedMolecule(mol)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bond length, angles, small rings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Position (translation): $\mathbb{R}^3$ - 3D vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17.png&#34;
	width=&#34;698&#34;
	height=&#34;394&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17_hubaace3e56e7acf27bdb1a448af48eb5b_40034_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17_hubaace3e56e7acf27bdb1a448af48eb5b_40034_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;425px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Orientation (rotation): $SO(3)$ - three Euler angle vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a.gif&#34;
	width=&#34;340&#34;
	height=&#34;322&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a_hu98bbb1e40ff0f96440af8978741eb4d9_406254_480x0_resize_box_1.gif 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a_hu98bbb1e40ff0f96440af8978741eb4d9_406254_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Euler2a.gif&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;105&#34;
		data-flex-basis=&#34;253px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Torsion angles: $\mathbb{T}^m$ ($m$: number of rotatable bonds)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim.gif&#34;
	width=&#34;185&#34;
	height=&#34;200&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim_hua8427fe8cdaef099affccff0d8af866f_448496_480x0_resize_box_1.gif 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim_hua8427fe8cdaef099affccff0d8af866f_448496_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Dihedral-angles-anim.gif&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;222px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can perform diffusion on product space $\mathbb{P}: \mathbb{R}^3 \times SO(3) \times \mathbb{T}^m$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For a given seed conformation $\mathbf{c}$, the map $A(\cdot, \mathbf{c}): \mathbb{P} \rightarrow \mathcal{M}_\mathbf{c}$ is a bijection!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18.png&#34;
	width=&#34;2618&#34;
	height=&#34;1122&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18_huf848a016eb7455cba5e115c7211e6f55_618737_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18_huf848a016eb7455cba5e115c7211e6f55_618737_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;560px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;confidence-model&#34;&gt;Confidence Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19.png&#34;
	width=&#34;1740&#34;
	height=&#34;356&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19_hud7e426523596c6c1031c9a3d319cc3e4_305102_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19_hud7e426523596c6c1031c9a3d319cc3e4_305102_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;488&#34;
		data-flex-basis=&#34;1173px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generative model can sample an arbitrary number of poses, but researchers are interested in one or a fixed number of them.&lt;/li&gt;
&lt;li&gt;Confidence predictions are very useful for downstream tasks.&lt;/li&gt;
&lt;li&gt;Confidence model $d(\mathbf{x}, \mathbf{y})$
&lt;ul&gt;
&lt;li&gt;$\mathbf{x}$: pose of a ligand&lt;/li&gt;
&lt;li&gt;$\mathbf{y}$: target protein structure&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Samples are ranked by score and the score of the best is used as overall confidence score.&lt;/li&gt;
&lt;li&gt;Training &amp;amp; Inference
&lt;ul&gt;
&lt;li&gt;Ran the trained diffusion model to obtain a set of candidate poses for every training example and generate binary labels: each pose has RMSD below $2 \text{Å}$ or not.&lt;/li&gt;
&lt;li&gt;Then the confidence model is trained with cross entropy loss to predict the binary label for each pose.&lt;/li&gt;
&lt;li&gt;During inference, diffusion model is run to generate $N$ poses in parallel, and passed to the confidence model that ranks them based on its confidence that they have RMSD below $2\text{Å}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;diffdock-workflow&#34;&gt;DiffDock Workflow&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20.png&#34;
	width=&#34;1786&#34;
	height=&#34;706&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20_huf37ccbd6cea1d3a5450b7473806471ee_503096_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20_huf37ccbd6cea1d3a5450b7473806471ee_503096_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;607px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;diffdock-results&#34;&gt;DiffDock Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Standard benchmark: PDBBind&lt;/p&gt;
&lt;p&gt;19k experimentally determined structures of small molecules + proteins&lt;/p&gt;
&lt;p&gt;Baselines: search-based &amp;amp; deep learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction correctness&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21.png&#34;
	width=&#34;1476&#34;
	height=&#34;782&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21_hu6492e850775254853f2253df277369e5_216895_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21_hu6492e850775254853f2253df277369e5_216895_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;452px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Outperform search-based, deep learning, and pocket prediction + search-based methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22.png&#34;
	width=&#34;1380&#34;
	height=&#34;776&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22_hu1e9d177239c85ffcc71145b4c82f01a0_199858_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22_hu1e9d177239c85ffcc71145b4c82f01a0_199858_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;3 times faster than the most accurate baseline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to unseen receptors&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23.png&#34;
	width=&#34;1494&#34;
	height=&#34;784&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23_hu6b0a4098791f8fb33cad44759483ab95_176122_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23_hu6b0a4098791f8fb33cad44759483ab95_176122_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;457px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Able to generalize: outperform classical method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reverse diffusion process GIF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock.gif&#34;
	width=&#34;500&#34;
	height=&#34;282&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock_huf4cb667dc82f8b11af86b654f4b03337_2904312_480x0_resize_box_1.gif 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock_huf4cb667dc82f8b11af86b654f4b03337_2904312_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;425px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confidence score quality&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24.png&#34;
	width=&#34;1154&#34;
	height=&#34;768&#34;
	srcset=&#34;https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24_hu647c20d3e720e07563131b84bd083487_321886_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24_hu647c20d3e720e07563131b84bd083487_321886_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;High selective accuracy: valuable information for practitioners&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;personal-opinions&#34;&gt;Personal opinions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It is impressive that the authors formulated molecular docking as a generative problem, conditioned on protein structure.&lt;/li&gt;
&lt;li&gt;But it is not an end-to-end approach. And there are some discrepancy between the inputs and output of the confidence model. The input is the predicted ligand pose $\hat{\mathbf{x}}$ and protein structure $\mathbf{y}$, but the output is “whether the RMSE is below 2Å between predicted ligand pose $\hat{\mathbf{x}}$ and ground truth ligand pose $\mathbf{x}$”.&lt;/li&gt;
&lt;li&gt;There are quite a room to improve the performance, but it requires heavy workloads of GPUs.&lt;/li&gt;
&lt;li&gt;I’m skeptical about the generalizability of this model since there are almost no physics informed inductive bias in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Article&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=kKF8_K-mBbS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Youtube&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/gAmTGw601dA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Blog&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://yang-song.net/blog/2021/score/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What are Diffusion Models?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Few-shot Learning with Graph Neural Networks</title>
        <link>https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/</link>
        <pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/</guid>
        <description>&lt;img src="https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/thumbnail.png" alt="Featured image of post Few-shot Learning with Graph Neural Networks" /&gt;&lt;p&gt;ICLR, &amp;lsquo;17,&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1711.04043&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Few-Shot Learning with Graph Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Used similarity value between samples for few shot learning.&lt;/li&gt;
&lt;li&gt;Regard each sample as nodes, and similarity kernel as edges.&lt;/li&gt;
&lt;li&gt;Similarity kernel is trainable. (i.e. Not just simple inner product)&lt;/li&gt;
&lt;li&gt;Can be applied to semi-supervised learning and active learning.&lt;/li&gt;
&lt;li&gt;State-of-the-art performance in Omniglot and Mini-ImageNet in 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keywords&#34;&gt;Keywords&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Few shot learning&lt;/li&gt;
&lt;li&gt;Graph neural network&lt;/li&gt;
&lt;li&gt;Semi-supervised learning&lt;/li&gt;
&lt;li&gt;Active learning with Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks.&lt;/li&gt;
&lt;li&gt;However, there are some tasks(e.g. few shot learning) that cannot achieve high performance with conventional methods.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;New&lt;/strong&gt; supervised learning setup
&lt;ul&gt;
&lt;li&gt;Input-output setup:
&lt;ul&gt;
&lt;li&gt;With i.i.d. samples  of collections of images and their associated label similarity&lt;/li&gt;
&lt;li&gt;cf) conventional setup: i.i.d. samples of images and their associated labels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Authors&amp;rsquo; model can be extended to semi-supervised and active learning
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Semi-supervised learning:&lt;/p&gt;
&lt;p&gt;Learning from a mixture of labeled and unlabeled examples&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled2.png&#34;
	width=&#34;1024&#34;
	height=&#34;428&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled2_huc1bfe4fa7e46f3525a992e0a384e32c8_196077_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled2_huc1bfe4fa7e46f3525a992e0a384e32c8_196077_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;https://blog.est.ai/2020/11/ssl/&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Active learning:&lt;/p&gt;
&lt;p&gt;The learner has the option to request those missing labels that will be most helpful for the prediction task&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled3.png&#34;
	width=&#34;1704&#34;
	height=&#34;610&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled3_huc6f63af45ed0ccc0628e36f08d894c39_473048_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled3_huc6f63af45ed0ccc0628e36f08d894c39_473048_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;ICML 2019 active learning tutorial&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;279&#34;
		data-flex-basis=&#34;670px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled4.png&#34;
	width=&#34;1024&#34;
	height=&#34;428&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled4_hu29a98bb38a9c671de7bd69329e42d18f_525600_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled4_hu29a98bb38a9c671de7bd69329e42d18f_525600_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Annotated by JH Gu&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;574px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;closely-related-works-and-ideas&#34;&gt;Closely related works and ideas&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[Research article] Matching Networks for One shot learning - Vinyals et al.(2016)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mapped support set of images into the desired label.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And developed an end-to-end trainable k-nearest neighbors, accepting those support sets as input via attention LSTM.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled5.png&#34;
	width=&#34;2610&#34;
	height=&#34;1132&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled5_hu0e12fc917e2d1e75fc1d11c369c80fd5_198931_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled5_hu0e12fc917e2d1e75fc1d11c369c80fd5_198931_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Vinyals et al.(2016), cited over 3000 times&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;230&#34;
		data-flex-basis=&#34;553px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled6.png&#34;
	width=&#34;1722&#34;
	height=&#34;1194&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled6_hu682bd0f012da9d625d12e31d1ed30fa9_900563_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled6_hu682bd0f012da9d625d12e31d1ed30fa9_900563_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled7.png&#34;
	width=&#34;2114&#34;
	height=&#34;786&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled7_huc8e7223a5841039c99c90f590a76f75b_236044_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled7_huc8e7223a5841039c99c90f590a76f75b_236044_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;645px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$k$: number of data in support set&lt;/li&gt;
&lt;li&gt;$\hat{x}$: new data&lt;/li&gt;
&lt;li&gt;$\hat{y}$: its class&lt;/li&gt;
&lt;li&gt;$\hat{y}$ is a linear combination of the labels in the support set&lt;/li&gt;
&lt;li&gt;$a$: attention mechanism, which is a kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[Research article] Prototypical Networks for Few-shot Learning - Snell et al.(2017)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled8.png&#34;
	width=&#34;1096&#34;
	height=&#34;320&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled8_hu673a9f33c51a89e6c015e050431013a1_40631_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled8_hu673a9f33c51a89e6c015e050431013a1_40631_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;342&#34;
		data-flex-basis=&#34;822px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blog.kakaocdn.net/dn/QTf75/btqV2blwJop/GPGDedaSftJNpHDXvq2XGk/img.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;https://blog.kakaocdn.net/dn/QTf75/btqV2blwJop/GPGDedaSftJNpHDXvq2XGk/img.gif&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Authors point out the overfitting problem of Matching networks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prototype: Center(mean) of each class cluster&lt;/li&gt;
&lt;li&gt;Similarity: $-\text{Euclidean distance}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[Review article] Geometric deep learning - Bronstein et al.(2017)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled9.png&#34;
	width=&#34;3380&#34;
	height=&#34;760&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled9_huce1372555d0695877b90856ae5320997_165518_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled9_huce1372555d0695877b90856ae5320997_165518_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;444&#34;
		data-flex-basis=&#34;1067px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Geometric deep learning is an umbrella term for emerging techniques attempting to generalize deep models to non-Euclidian domains such as graphs and manifolds&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[Research article] Message passing - Gilmer et al.(2017)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled10.png&#34;
	width=&#34;3232&#34;
	height=&#34;614&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled10_hu1eef0041f9c47cde680bf6ae160fab5e_104258_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled10_hu1eef0041f9c47cde680bf6ae160fab5e_104258_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;526&#34;
		data-flex-basis=&#34;1263px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;$$
m^{t+1}_v = \sum_{w \in N(v)} M_t(h^t_v, h^t_w, e_{vw}) \ h^{t+1}_v = U_t(h^t_v, m^{t+1}_v )
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M_t$: message functions&lt;/li&gt;
&lt;li&gt;$U_t$: vertex update functions&lt;/li&gt;
&lt;li&gt;$h^t_v$: hidden states of node $v$ in the graph at time $t$&lt;/li&gt;
&lt;li&gt;$m^{t+1}_v$: messages of node $v$ in the graph at time $t+1$&lt;/li&gt;
&lt;li&gt;$N(v)$: neighbors of node $v$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*oSQyFjtUkI7_u7lJXWU68Q.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;GIF from https://towardsdatascience.com/introduction-to-message-passing-neural-networks-e670dc103a87&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;problem-set-up&#34;&gt;Problem set-up&lt;/h2&gt;
&lt;p&gt;Authors view the task as a supervised interpolation problem on a graph&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nodes: &lt;strong&gt;Images&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Edges: &lt;strong&gt;Similarity kernels →&lt;/strong&gt; &lt;em&gt;TRAINABLE&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-set-up&#34;&gt;General set-up&lt;/h3&gt;
&lt;p&gt;Input-output pairs $(\mathcal{T}_i, Y_i)_i$ drawn from i.i.d. from a distribution $\mathcal{P}$ of partially labeled image collections&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled11.png&#34;
	width=&#34;4000&#34;
	height=&#34;423&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled11_hu5436287761932c1cedb9fbd5fa84a3d6_135160_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled11_hu5436287761932c1cedb9fbd5fa84a3d6_135160_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;945&#34;
		data-flex-basis=&#34;2269px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s$: # labeled samples&lt;/li&gt;
&lt;li&gt;$r$: # unlabled samples&lt;/li&gt;
&lt;li&gt;$t$: # samples to classify&lt;/li&gt;
&lt;li&gt;$K$: # classes&lt;/li&gt;
&lt;li&gt;$\mathcal{P}_l(\mathbb{R}^N)$: class-specific image distribution over $\mathbb{R}^N$&lt;/li&gt;
&lt;li&gt;targets $Y_i$ are associated with $\bar{x}_1, &amp;hellip;, \bar{x}_t \in \mathcal{T}_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learning objective:&lt;/p&gt;
&lt;p&gt;$\min_\Theta \frac{1}{L} \sum_{i \leq L} \ell(\Phi(\mathcal{T}_i, \Theta), Y_i) + \mathcal{R}(\Theta)$&lt;/p&gt;
&lt;p&gt;($\mathcal{R}$ is the standard regularization objective)&lt;/p&gt;
&lt;h3 id=&#34;few-shot-learning-setting&#34;&gt;Few shot learning setting&lt;/h3&gt;
&lt;p&gt;$r=0, t=1, s=qK$   $\longrightarrow$   $q-\text{shot} , K-\text{way}$&lt;/p&gt;
&lt;h3 id=&#34;semi-supervised-learning-setting&#34;&gt;Semi-supervised learning setting&lt;/h3&gt;
&lt;p&gt;$r &amp;gt; 0, t=1$&lt;/p&gt;
&lt;p&gt;Model can use the auxiliary images(unlabeled set) ${ \tilde{x}_1, &amp;hellip;, \tilde{x}_r }$ to improve the prediction accuracy, by leveraging the fact that these samples are drawn from the common distributions.&lt;/p&gt;
&lt;h3 id=&#34;active-learning-setting&#34;&gt;Active learning setting&lt;/h3&gt;
&lt;p&gt;The learner has the ability to request labels from the auxiliary images ${\tilde{x}_1, &amp;hellip;, \tilde{x}_r}$.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled12.png&#34;
	width=&#34;1188&#34;
	height=&#34;900&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled12_hu518b9887168c60fff5bff4e932a1b548_336596_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled12_hu518b9887168c60fff5bff4e932a1b548_336596_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;316px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\phi(x)$: CNN&lt;/li&gt;
&lt;li&gt;$h(l)$: One-hot encoded label(for labeled set), or uniform distribution(for unlabeled set)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;set-and-graph-input-representations&#34;&gt;Set and Graph Input Representations&lt;/h3&gt;
&lt;p&gt;The goal of few shot learning:&lt;/p&gt;
&lt;p&gt;To propagate label information from labeled samples towards the unlabeled query image&lt;/p&gt;
&lt;p&gt;→ The propagation can be formalized as a posterior inference over a graphical model&lt;/p&gt;
&lt;p&gt;$G_\mathcal{T} = (V,E)$&lt;/p&gt;
&lt;p&gt;Similarity measure is not pre-specified, but learned!&lt;/p&gt;
&lt;p&gt;c.f.) in Siamese network, the similarity measure is fixed(L1 distance)!&lt;/p&gt;
&lt;p&gt;본 논문(Few shot learning with GNN)에 쓰인 문장 구조가 이상해서 헷갈리게 쓰여있음.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled13.png&#34;
	width=&#34;3024&#34;
	height=&#34;1004&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled13_hu195ce8430d176ca0bb51f3bd3cc2cd4a_386759_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled13_hu195ce8430d176ca0bb51f3bd3cc2cd4a_386759_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Koch et al.(2015), https://tyami.github.io/deep learning/Siamese-neural-networks/&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;301&#34;
		data-flex-basis=&#34;722px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;graph-neural-networks&#34;&gt;Graph Neural Networks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled14.png&#34;
	width=&#34;1982&#34;
	height=&#34;838&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled14_hu2b7b1594699112bb43a74ec4c994064f_379402_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled14_hu2b7b1594699112bb43a74ec4c994064f_379402_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;236&#34;
		data-flex-basis=&#34;567px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;We are given an input signal $F \in \mathbb{R}^{V \times d}$ on the vertices of a weighted graph $G$.&lt;/p&gt;
&lt;p&gt;Then we consider a family, or a set &amp;ldquo;$\mathcal{A}$&amp;rdquo; of graph intrinsic linear operators.&lt;/p&gt;
&lt;p&gt;$\mathcal{A} = {\tilde{A}^{(k)}, \mathbf{1}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Linear operator&lt;/p&gt;
&lt;p&gt;e.g.) Simplest linear operator is adjacency operator $A$, where $(AF)&lt;em&gt;i = \sum&lt;/em&gt;{j \sim i} w_{i,j}F_j$ ($w_{i,j}$ is associated weight)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GNN layer&lt;/p&gt;
&lt;p&gt;A GNN layer $\text{Gc}(\cdot)$ receives as input a signal $\mathbf{x}^{(k)} \in \mathbb{R}^{V\times d_k}$ and produces $\mathbf{x}^{(k+1)} \in \mathbb{R}^{V\times d_{k+1}}$&lt;/p&gt;
&lt;p&gt;$$
\mathbf{x}^{(k+1)} = \text{Gc}(\mathbf{x}^{(k)}) = \rho\Big(\sum_{B\in\mathcal{A}} B\mathbf{x}^{(k)}\theta^{(k)}_{B, l}\Big )
$$&lt;/p&gt;
&lt;p&gt;$\mathbf{x}^{(k)}$: representation vector of a certain node at time step $k$&lt;/p&gt;
&lt;p&gt;$\theta$: trainable parameters&lt;/p&gt;
&lt;p&gt;$\rho$: Leaky ReLU&lt;/p&gt;
&lt;p&gt;Construction of edge feature matrix, inspired by message passing algorithm&lt;/p&gt;
&lt;p&gt;$$
\tilde{A}^{(k)}_{i, j} = \varphi_{\tilde{\theta}}(\mathbf{x}^{(k)}_i, \mathbf{x}^{(k)}_j )
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\tilde{A}^{(k)}_{i, j}$: &lt;strong&gt;learned&lt;/strong&gt; edge features from the node&amp;rsquo;s current hidden representation(at time step $k$)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\varphi$: a metric and a symmetric function parameterized with neural network&lt;/p&gt;
&lt;p&gt;$$
\varphi_{\tilde{\theta}}(\mathbf{x}^{(k)}_i, \mathbf{x}^{(k)}_j ) = \text{MLP}_{\tilde{\theta}}(abs(\mathbf{x}^{(k)}_i - \mathbf{x}^{(k)}_j))
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;→ $\tilde{A}^{(k)}$ is then normalized by row-wise softmax&lt;/p&gt;
&lt;p&gt;→ And added to the family $\mathcal{A} = {\tilde{A}^{(k)}, \mathbf{1}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{1}$: Identity matrix, which is the self-edge to aggregate vertex&amp;rsquo;s own features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Construction of initial node features&lt;/p&gt;
&lt;p&gt;$$
\mathbf{x}^{(0)}_i = (\phi(x_i), h(l_i))
$$&lt;/p&gt;
&lt;p&gt;$\phi$: convolutional neural network&lt;/p&gt;
&lt;p&gt;$h(l) \in \mathbb{R}^K_+$ : a one-hot encoding of the label&lt;/p&gt;
&lt;p&gt;For images with unknown label, $\tilde{x}_j$(unlabeled data) and  $\bar{x}_j$(test data), $h(l_j)$ is set with uniform distribution.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;h3 id=&#34;few-shot-and-semi-supervised-learning&#34;&gt;Few-shot and Semi-supervised learning&lt;/h3&gt;
&lt;p&gt;The final layer of GNN is a softmax mapping. We then use cross-entropy loss:&lt;/p&gt;
&lt;p&gt;$$
\ell(\Phi(\mathcal{T}; \Theta), Y) = -\sum_k y_k \log P(Y_* = y_k , |, \mathcal{T})
$$&lt;/p&gt;
&lt;p&gt;The semi-supervised setting is trained identically, but the initial label fields of $\tilde{x}_j$s will be filled with uniform distribution.&lt;/p&gt;
&lt;h3 id=&#34;active-learning-with-attention&#34;&gt;Active learning (with attention)&lt;/h3&gt;
&lt;p&gt;In active learning, the model has the intrinsic ability to query for one of the labels from ${ \tilde{x}_1, &amp;hellip;, \tilde{x}_r }$.&lt;/p&gt;
&lt;p&gt;The network will learn to ask for the most informative label to classify the sample $\bar{x}$.&lt;/p&gt;
&lt;p&gt;The querying is done after the first layer of GNN by using a softmax attention over the unlabeled nodes of the graph.&lt;/p&gt;
&lt;p&gt;Attention&lt;/p&gt;
&lt;p&gt;We apply a function $g(\mathbf{x}^{(1)}_i) \in \mathbb{R}^1$ that maps each unlabeld vector node to a scalar value.&lt;/p&gt;
&lt;p&gt;A softmax is applied over the ${1, &amp;hellip;, r}$ scalar values obtained after applying $g$:&lt;/p&gt;
&lt;p&gt;$r$: # unlabeled samples&lt;/p&gt;
&lt;p&gt;$$
\text{Attention} = \text{Softmax}(g(\mathbf{x}^{(1)}_{{1,&amp;hellip;,r}}))
$$&lt;/p&gt;
&lt;p&gt;To query only one sample we set all elements to zero except for one. → $\text{Attention}&#39;$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At training, model randomly samples one value based on its multinomial probability.&lt;/li&gt;
&lt;li&gt;At test, model just keeps the maximum value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we multiply this with the label vectors&lt;/p&gt;
&lt;p&gt;$$
w \cdot h(l_{i*}) = \langle \text{Attention}&amp;rsquo;, h(l_{{1, &amp;hellip;, r}}) \rangle
$$&lt;/p&gt;
&lt;p&gt;($w$ is scaling factor)&lt;/p&gt;
&lt;p&gt;This value is then summed to the current representation.&lt;/p&gt;
&lt;p&gt;$$
\mathbf{x}^{(1)}_{i*} = [\text{Gc}(\mathbf{x}^{(0)}_{i*}), \mathbf{x}^{(0)}_{i*}] = [\text{Gc}(\mathbf{x}^{(0)}_{i*}), (\phi(x_{i*}), h(l_{i*}))]
$$&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;few-shot-learning&#34;&gt;Few-shot learning&lt;/h3&gt;
&lt;p&gt;Omniglot&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled15.png&#34;
	width=&#34;1744&#34;
	height=&#34;720&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled15_hud1dea68979c0bbf767eb191d1fd1d37e_221954_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled15_hud1dea68979c0bbf767eb191d1fd1d37e_221954_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;581px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;# of parameters: $\sim5\text{M} (\text{TCML})$, $\sim300 \text{K}(3 \text{layers GNN})$&lt;/p&gt;
&lt;p&gt;Omniglot: 1,623 characters  X 20 examples for each characters&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled16.png&#34;
	width=&#34;2170&#34;
	height=&#34;945&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled16_hu734905f970a0a113fc187b3c1a108a68_954773_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled16_hu734905f970a0a113fc187b3c1a108a68_954773_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Omniglot&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;229&#34;
		data-flex-basis=&#34;551px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Mini-ImageNet: Originally introduced by Vinyals et al.(2016)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled17.png&#34;
	width=&#34;1628&#34;
	height=&#34;584&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled17_hu78e7bfaf35d1da92942e4f4a1fe7dadb_170810_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled17_hu78e7bfaf35d1da92942e4f4a1fe7dadb_170810_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;278&#34;
		data-flex-basis=&#34;669px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;# of parameters: $\sim 11\text{M} (\text{TCML})$, $\sim 400 \text{K}(3 \text{ layers GNN})$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled18.png&#34;
	width=&#34;795&#34;
	height=&#34;400&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled18_hu379e1637ed83ad26af73f3539cf37e81_707619_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled18_hu379e1637ed83ad26af73f3539cf37e81_707619_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Mini-ImageNet&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;477px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Mini-ImageNet&lt;/p&gt;
&lt;p&gt;Divided into 64 training, 16 validation, 20 testing classes each containing 600 examples.&lt;/p&gt;
&lt;h3 id=&#34;semi-supervised-learning&#34;&gt;Semi-supervised learning&lt;/h3&gt;
&lt;p&gt;Omniglot&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled19.png&#34;
	width=&#34;1444&#34;
	height=&#34;304&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled19_hu21116ea201fb3837edc6f9ad35132df9_60736_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled19_hu21116ea201fb3837edc6f9ad35132df9_60736_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;475&#34;
		data-flex-basis=&#34;1140px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Mini-ImageNet&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled20.png&#34;
	width=&#34;1666&#34;
	height=&#34;342&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled20_hu1158a4c24840395ac6dcb63f5d7034ea_78788_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled20_hu1158a4c24840395ac6dcb63f5d7034ea_78788_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;487&#34;
		data-flex-basis=&#34;1169px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;active-learning&#34;&gt;Active learning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled21.png&#34;
	width=&#34;1676&#34;
	height=&#34;444&#34;
	srcset=&#34;https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled21_hu57c3addab4b9d756f0d1227c98fe6ef2_119500_480x0_resize_box_3.png 480w, https://gujh14.github.io/p/few-shot-learning-with-graph-neural-networks/Untitled21_hu57c3addab4b9d756f0d1227c98fe6ef2_119500_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;377&#34;
		data-flex-basis=&#34;905px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Random: Network chooses a random sample to be labeled, instead of one that maximally reduces the loss of the classification task $\mathcal{T}$&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
</description>
        </item>
        
    </channel>
</rss>
