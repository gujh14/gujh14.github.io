<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>JH Gu&#39;s Tech Blog</title>
        <link>https://gujh14.github.io/ko/</link>
        <description>Recent content on JH Gu&#39;s Tech Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 27 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gujh14.github.io/ko/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking</title>
        <link>https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/</link>
        <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/</guid>
        <description>&lt;p&gt;ICLR, &amp;lsquo;23&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.01776&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail.png&#34;
	width=&#34;1110&#34;
	height=&#34;534&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail_hu35359f9550b3436da89f85bd47cc7f53_260405_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/thumbnail_hu35359f9550b3436da89f85bd47cc7f53_260405_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;498px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;본 논문은 molecular docking을 generative problem으로 간주한 첫 연구이자, 굉장히 흥미로운 결과들을 보여주었고, 성능 향상을 달성했다.&lt;/li&gt;
&lt;li&gt;Diffusion과 molecular docking에 관심이 있다면 꼭 한 번 읽어봐야 할 논문이다.&lt;/li&gt;
&lt;li&gt;가능하다면 저자들이 본 논문에 대해 설명한 &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=gAmTGw601dA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;youtube 영상&lt;/a&gt;도 보면 이해에 도움이 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Molecular docking을 regression이 아닌, generative problem으로 formulation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Target protein 구조 $\mathbf{y}$를 조건으로 하여, ligand의 위치와 구조$\mathbf{x}$의 distribution을 학습 → $p(\mathbf{x} | \mathbf{y})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generation에 diffusion process 사용&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;두 개의 구분된 모델 사용&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Score model: $s(\mathbf{x}, \mathbf{y}, t)$&lt;/p&gt;
&lt;p&gt;Ligand, protein, timestep을 input으로 하여 score를 prediction하는 model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confidence model: $d(\mathbf{x}, \mathbf{y})$&lt;/p&gt;
&lt;p&gt;Ligand와 protein의 구조를 바탕으로 ground truth ligand pose 대비 RMSD 기준 2Å 이하의 pose일 확률 return&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diffusion on Product space $\mathbb{P}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Degree of freedom을 $3n$에서 $(m+6)$로 축소하여 diffusion 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;h3 id=&#34;molecular-docking&#34;&gt;Molecular Docking&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled.png&#34;
	width=&#34;1942&#34;
	height=&#34;1116&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled_huf40d7e8c2d3df3da9d828794a85e8161_935838_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled_huf40d7e8c2d3df3da9d828794a85e8161_935838_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;417px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Definition:&lt;/p&gt;
&lt;p&gt;Target protein에 결합한 ligand의 position, orientation, conformation을 예측하는 것을 의미&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two types of tasks 크게 두 종류의 task로 구분함&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Known-pocket docking
&lt;ul&gt;
&lt;li&gt;Protein의 binding pocket의 위치 정보를 주고 예측&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Blind docking
&lt;ul&gt;
&lt;li&gt;Binding pocket에 대한 정보 없이 예측하는 더 일반적이고 어려운 상황&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;previous-works-search-based--regression-based&#34;&gt;Previous works: Search-based / Regression-based&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1.png&#34;
	width=&#34;1462&#34;
	height=&#34;1104&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1_huc1cd3bae2f7177be1a7cfda9b4564a85_264966_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled1_huc1cd3bae2f7177be1a7cfda9b4564a85_264966_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;132&#34;
		data-flex-basis=&#34;317px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Search based docking methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Traditional methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parameterize 된 physics 기반의 &lt;strong&gt;scoring function&lt;/strong&gt;과 &lt;strong&gt;search algorithm&lt;/strong&gt;으로 구성&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scoring function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input: 3D 구조&lt;/li&gt;
&lt;li&gt;Output: 특정 pose에 대한 quality나 likelihood 예측&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ligand pose (position, orientation, torsion angles)를 확률론적으로 랜덤하게 (stochastically) 바꿈&lt;/li&gt;
&lt;li&gt;Goal: scoring function의 global optimum을 찾는 것&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scoring function을 parameterize하기 위해 ML이 적용되고 있지만, search space 가 너무 커서 계산량이 매우 많다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;예시&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2.png&#34;
	width=&#34;1776&#34;
	height=&#34;166&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2_huf43150a0e47223ef3cf22aed6633617b_81428_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled2_huf43150a0e47223ef3cf22aed6633617b_81428_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1069&#34;
		data-flex-basis=&#34;2567px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression based methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;최신 딥러닝 기반의 방법들이 ground truth position에 대해 regression으로 formulation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search based method 대비&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;속도는 상당히 빨라짐.&lt;/li&gt;
&lt;li&gt;정확도에 있어서는 부족하거나, 비슷한 정도.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;예시&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3.png&#34;
	width=&#34;1152&#34;
	height=&#34;144&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3_hu6b088dca52a85205018e5f1b36628350_49655_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled3_hu6b088dca52a85205018e5f1b36628350_49655_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;800&#34;
		data-flex-basis=&#34;1920px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2202.05146&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;EquiBind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4.png&#34;
	width=&#34;2030&#34;
	height=&#34;550&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4_hu663897c2650aaacc73297b198e4cc5b3_753604_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled4_hu663897c2650aaacc73297b198e4cc5b3_753604_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;369&#34;
		data-flex-basis=&#34;885px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blind docking을 regression으로 접근하여 binding pocket의 keypoint를 직접 예측함.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.biorxiv.org/content/10.1101/2022.06.06.495043v3&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;TANKBind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5.png&#34;
	width=&#34;1988&#34;
	height=&#34;732&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5_hub1f86d8413b43196d2dbec69310f8555_603864_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled5_hub1f86d8413b43196d2dbec69310f8555_603864_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;271&#34;
		data-flex-basis=&#34;651px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Protein의 각 가능한 pocket에 대해 docking pose를 예측하고 따로 ranking 하여 평가하여 성능 향상.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2210.06069&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;E3Bind&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6.png&#34;
	width=&#34;1536&#34;
	height=&#34;562&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6_hu13edc0df2b71d59ba0b45cb41de53f20_193128_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled6_hu13edc0df2b71d59ba0b45cb41de53f20_193128_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;273&#34;
		data-flex-basis=&#34;655px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ligand 내부의 atom끼리의 상호작용, protein residue와의 상호작용을 모두 고려하여 ligand atom embedding을 만들고, iterative하게 좌표를 업데이트하여 binding pose 예측 수행.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;docking-objective&#34;&gt;Docking objective&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7.png&#34;
	width=&#34;1520&#34;
	height=&#34;1214&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7_hu5f4844bc8df901aef67687f97d63b662_915257_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled7_hu5f4844bc8df901aef67687f97d63b662_915257_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;125&#34;
		data-flex-basis=&#34;300px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard evaluation metric:
&lt;ul&gt;
&lt;li&gt;$\mathcal{L}_{\epsilon} = \sum_{x, y} I_{\text{RMSD}(y, \hat{y}(x))&amp;lt;\epsilon}$: $\text{RMSD} &amp;lt; \epsilon$ 으로 예측한 것들의 비율 → 미분 불가능함.&lt;/li&gt;
&lt;li&gt;대신, $\text{argmin}_{\hat{y}} \lim_{\epsilon \rightarrow 0} \mathcal{L}_\epsilon$ 을 objective function 으로 사용함.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regression은 unimodal일 경우에만 이 objective function과 일맥 상통함.&lt;/li&gt;
&lt;li&gt;그러나 Docking은 상당한 aleatoric (irreducible) &amp;amp; epistemic (reducible) uncertainty 지님.
&lt;ul&gt;
&lt;li&gt;Regression 방법들은 $\sum \|y - \hat{y}\|^2_2$ 를 최소화하는 것이 목적이고, 이는 여러 mode들의 weighted mean을 예측하도록 학습할 것.&lt;/li&gt;
&lt;li&gt;반면 generative model은 대부분의 mode에 대해 generation 가능.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8.png&#34;
	width=&#34;2028&#34;
	height=&#34;1108&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8_hu6b313aafde63670bc308ca51bccee486_1187222_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled8_hu6b313aafde63670bc308ca51bccee486_1187222_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;439px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;실제 결과를 살펴보면, EquiBind 같은 regression model은 여러 mode들의 중간에 위치하도록 예측함.&lt;/li&gt;
&lt;li&gt;반면 generative model은 여러 mode들에 대해 conformer를 위치시킬 수 있음.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9.png&#34;
	width=&#34;2002&#34;
	height=&#34;1090&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9_hu04edc1aff53bfad5a2de658c9efbf1ee_1153872_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled9_hu04edc1aff53bfad5a2de658c9efbf1ee_1153872_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;440px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DiffDock이 steric clash가 덜하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;diffdock-overview&#34;&gt;DiffDock Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15.png&#34;
	width=&#34;1110&#34;
	height=&#34;534&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15_hu35359f9550b3436da89f85bd47cc7f53_260405_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled15_hu35359f9550b3436da89f85bd47cc7f53_260405_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;207&#34;
		data-flex-basis=&#34;498px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;두 단계 model 사용
&lt;ul&gt;
&lt;li&gt;Score model: Translation (평행 이동), rotation (회전), torsion (비틀림)에 대해 reverse diffusion 수행&lt;/li&gt;
&lt;li&gt;Confidence model: 각 ligand pose가 ground truth 와 비교 시 $\text{RMSD} &amp;lt; 2\text{Å}$ 차이로 잘 위치했는지 binary prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;score-model&#34;&gt;Score model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ligand pose: $\mathbb{R}^{3n}$ ($n$: number of atoms)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;하지만 molecular docking은 이보다 더 적은 degree of freedom으로 수행할 수 있다.
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16.png&#34;
	width=&#34;2044&#34;
	height=&#34;1074&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16_hu38cf56affd34313fb9003ca22c483d35_693157_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled16_hu38cf56affd34313fb9003ca22c483d35_693157_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;456px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;축소된 degree of freedom: $(m+6)$
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Local structure: RDKit conformer generation (&lt;code&gt;EmbedMolecule(mol)&lt;/code&gt;) 이후 어느 정도 고정되어 있다고 가정할 수 있음.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bond length, angle, small rings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Position (translation): $\mathbb{R}^3$ - 3D vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17.png&#34;
	width=&#34;698&#34;
	height=&#34;394&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17_hubaace3e56e7acf27bdb1a448af48eb5b_40034_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled17_hubaace3e56e7acf27bdb1a448af48eb5b_40034_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;425px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Orientation (rotation): $SO(3)$ - three Euler angle vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a.gif&#34;
	width=&#34;340&#34;
	height=&#34;322&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a_hu98bbb1e40ff0f96440af8978741eb4d9_406254_480x0_resize_box_1.gif 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Euler2a_hu98bbb1e40ff0f96440af8978741eb4d9_406254_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;105&#34;
		data-flex-basis=&#34;253px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Torsion angles: $\mathbb{T}^m$ ($m$: number of rotatable bonds)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim.gif&#34;
	width=&#34;185&#34;
	height=&#34;200&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim_hua8427fe8cdaef099affccff0d8af866f_448496_480x0_resize_box_1.gif 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Dihedral-angles-anim_hua8427fe8cdaef099affccff0d8af866f_448496_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;222px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Product space $\mathbb{P}: \mathbb{R}^3 \times SO(3) \times \mathbb{T}^m$ 에서 diffusion 수행 가능&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seed conformation $\mathbf{c}$가 주어진 상황이라면, mapping $A(\cdot, \mathbf{c}): \mathbb{P} \rightarrow \mathcal{M}_\mathbf{c}$는 bijection 이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18.png&#34;
	width=&#34;2618&#34;
	height=&#34;1122&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18_huf848a016eb7455cba5e115c7211e6f55_618737_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled18_huf848a016eb7455cba5e115c7211e6f55_618737_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;560px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;confidence-model&#34;&gt;Confidence Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19.png&#34;
	width=&#34;1740&#34;
	height=&#34;356&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19_hud7e426523596c6c1031c9a3d319cc3e4_305102_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled19_hud7e426523596c6c1031c9a3d319cc3e4_305102_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;488&#34;
		data-flex-basis=&#34;1173px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generative model은 여러 pose를 sampling 할 수 있지만, 대부분 연구자들은 1개나 몇 개 정도의 sample만 보기를 원한다.&lt;/li&gt;
&lt;li&gt;따라서 confidence prediction은 downstream task에 있어 유용하게 사용될 수 있다.&lt;/li&gt;
&lt;li&gt;Confidence model $d(\mathbf{x}, \mathbf{y})$
&lt;ul&gt;
&lt;li&gt;$\mathbf{x}$: Ligand의 pose&lt;/li&gt;
&lt;li&gt;$\mathbf{y}$: Target protein의 구조&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample들은 confidence score로 순위가 매겨지고, 가장 좋은 순위의 confidence score가 사용된다.&lt;/li&gt;
&lt;li&gt;Training &amp;amp; Inference
&lt;ul&gt;
&lt;li&gt;학습된 diffusion score model에 대해 후보 pose들을 sampling 하고, binary label (각 pose가 ground truth pose 대비 $\text{RMSD} &amp;lt; 2\text{Å}$인지 아닌지)을 만든다.&lt;/li&gt;
&lt;li&gt;이후 confidence model은 binary label을 맞추도록 cross entropy loss로 학습된다.&lt;/li&gt;
&lt;li&gt;Inference 시에는 diffusion score model이 $N$개의 pose를 생성하고, confidence model에 그 pose가 전달되어 confidence score 기반으로 순위를 매겨 사용한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;diffdock-workflow&#34;&gt;DiffDock Workflow&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20.png&#34;
	width=&#34;1786&#34;
	height=&#34;706&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20_huf37ccbd6cea1d3a5450b7473806471ee_503096_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled20_huf37ccbd6cea1d3a5450b7473806471ee_503096_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;252&#34;
		data-flex-basis=&#34;607px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;diffdock-results&#34;&gt;DiffDock Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Standard benchmark: PDBBind
19k experimentally determined structures of small molecules + proteins&lt;/p&gt;
&lt;p&gt;Baselines: search-based &amp;amp; deep learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction correctness
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21.png&#34;
	width=&#34;1476&#34;
	height=&#34;782&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21_hu6492e850775254853f2253df277369e5_216895_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled21_hu6492e850775254853f2253df277369e5_216895_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;452px&#34;
	
&gt;
Outperform search-based, deep learning, and pocket prediction + search-based methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Runtime
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22.png&#34;
	width=&#34;1380&#34;
	height=&#34;776&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22_hu1e9d177239c85ffcc71145b4c82f01a0_199858_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled22_hu1e9d177239c85ffcc71145b4c82f01a0_199858_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;
3 times faster than the most accurate baseline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization to unseen receptors
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23.png&#34;
	width=&#34;1494&#34;
	height=&#34;784&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23_hu6b0a4098791f8fb33cad44759483ab95_176122_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled23_hu6b0a4098791f8fb33cad44759483ab95_176122_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;457px&#34;
	
&gt;
Able to generalize: outperform classical method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reverse diffusion process GIF
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock.gif&#34;
	width=&#34;500&#34;
	height=&#34;282&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock_huf4cb667dc82f8b11af86b654f4b03337_2904312_480x0_resize_box_1.gif 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/diffdock_huf4cb667dc82f8b11af86b654f4b03337_2904312_1024x0_resize_box_1.gif 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;425px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confidence score quality
&lt;img src=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24.png&#34;
	width=&#34;1154&#34;
	height=&#34;768&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24_hu647c20d3e720e07563131b84bd083487_321886_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/diffdock-diffusion-steps-twists-and-turns-for-molecular-docking/imgs/Untitled24_hu647c20d3e720e07563131b84bd083487_321886_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;
High selective accuracy: valuable information for practitioners&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;personal-opinions&#34;&gt;Personal opinions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Molecular docking을 generation problem으로 formulation 했다는 점이 아주 인상 깊다.&lt;/li&gt;
&lt;li&gt;Conformer generation의 condition으로 protein structure를 사용했다.&lt;/li&gt;
&lt;li&gt;하지만 two step approach라는 점, 그리고 confidence model의 input이 predicted ligand pose $\hat{\mathbf{x}}$와 protein structure $\mathbf{y}$ 인데, 결국 예측하는 것은 “predicted ligand pose $\hat{\mathbf{x}}$와 ground truth ligand pose $\mathbf{x}$ 간의 차이가 2Å 이하인지” 라서 약간의 괴리감이 있는 것 같다.&lt;/li&gt;
&lt;li&gt;아직 성능 개선의 room 이 상당히 남아있지만, GPU workload가 꽤 많이 들어서 우리 연구실에서 수행하기에는 어려울 수도 있을 것 같다는 생각이 든다.&lt;/li&gt;
&lt;li&gt;Docking 이지만 physics 기반의 inductive bias가 들어가 있지는 않은 것 같아서 얼마나 generalization 하기에 적합한지 의문이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Article&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://openreview.net/forum?id=kKF8_K-mBbS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Youtube&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://youtu.be/gAmTGw601dA&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Blog&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://yang-song.net/blog/2021/score/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;What are Diffusion Models?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Neighborhood-aware Scalable Temporal Network Representation Learning</title>
        <link>https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/</link>
        <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/</guid>
        <description>&lt;p&gt;LoG, &amp;lsquo;22&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.01084&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neighborhood-aware Scalable Temporal Network Representation Learning&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail.png&#34;
	width=&#34;914&#34;
	height=&#34;388&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail_hu242d3a243b2ed8387d2472cbaa72b75a_149908_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/thumbnail_hu242d3a243b2ed8387d2472cbaa72b75a_149908_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;565px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;시간에 따라 변하는 temporal network에 network science적인 inference를 잘 할 수 있는 모델들이 부족하다.&lt;/li&gt;
&lt;li&gt;기존의 일반적인 GNN은 structural feature를 잘 반영하기 어렵고, node 간의 distance를 사용한 모델들도 대부분 static network에 적용하는 것이었다.&lt;/li&gt;
&lt;li&gt;Temporal network에 scalable하게 적용하는 방법으로 dictionary-type node representation과 neighborhood cache를 이용하여 다양한 link prediction task에서 좋은 성능을 달성했다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;temporal-network&#34;&gt;Temporal network&lt;/h2&gt;
&lt;p&gt;Temporal network는 시간에 따라 변하는, 복잡한 interactive system의 abstraction임.&lt;/p&gt;
&lt;p&gt;네트워크 구조는 시간에 따라 변한다.&lt;/p&gt;
&lt;p&gt;예시:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;사용자-아이템 네트워크&lt;/li&gt;
&lt;li&gt;소셜 미디어 네트워크&lt;/li&gt;
&lt;li&gt;이메일 네트워크&lt;/li&gt;
&lt;li&gt;모빌리티 네트워크 등&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;저자의 목표:&lt;/p&gt;
&lt;p&gt;Temporal network가 어떻게 변할지 예측하여 link prediction을 수행하는 것.&lt;/p&gt;
&lt;p&gt;적용 가능 분야:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recommendation&lt;/li&gt;
&lt;li&gt;Anomaly detection&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;network-science에서-temporal-network&#34;&gt;Network science에서 temporal network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Network science는 네트워크 구조가 복잡한 시스템 하에서 어떻게 진화하고 변할지에 대한 기본적인 법칙을 연구하는 학문.&lt;/p&gt;
&lt;p&gt;예시:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Triadic closure in social network: 겹치는 친구가 있는 경우 서로 친구가 될 확률이 높다.&lt;/li&gt;
&lt;li&gt;Feed-forward control: positive stimuli 뒤에는 negative stimuli가 따라온다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;선행-연구들&#34;&gt;선행 연구들&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;일반적인 GNN은 여러 node가 관여된 structural feature를 반영할 수 없음.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;예를 들어, triadic closure의 경우,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled.png&#34;
	width=&#34;862&#34;
	height=&#34;444&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled_hu0af0c10cdb137a663b0f78c123bc4e21_292022_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled_hu0af0c10cdb137a663b0f78c123bc4e21_292022_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;timestep $t_3$에서 u와 v가 연결될지, $u$와 $w$가 연결될지 결정할 때, traditional GNN은 $w$와 $v$의 embedding을 동일하게 업데이트 한다. 즉, 두 node의 computation graph가 동일해서 구분하지 못 한다.&lt;/p&gt;
&lt;p&gt;→ 이런 방법을 사용하면 network representation 이 잘 학습되지 않을 것이다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Static graph에서는 이 issue를 해결하기 위한 시도들이 존재.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SEAL (Zhang et al. 2018)&lt;/li&gt;
&lt;li&gt;Distance encoding (Li et al. 2020)&lt;/li&gt;
&lt;li&gt;Labeling trick (Zhang et al. 2021)&lt;/li&gt;
&lt;li&gt;SUREL (Yin et al. 2022)&lt;/li&gt;
&lt;li&gt;ELPH (Chamberlain et al. 2022)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;대부분의 key idea는 structural feature를 만들어서 extra feature로 사용하는 것.&lt;/p&gt;
&lt;p&gt;보통 shortest path distance를 사용.&lt;/p&gt;
&lt;p&gt;이런 idea를 temporal network에 effective하고 scalable하게 적용하는 것이 중요.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal network에 대한 최근 연구&lt;/p&gt;
&lt;p&gt;CAWN (Wang et al. 2021): computation overhead가 높다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;각 node pair마다 random walk이 sampling 되어야 한다.&lt;/li&gt;
&lt;li&gt;Relative positional encoding이 online으로 계산되어야 한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neighborhood-aware-temporal-network-nat&#34;&gt;Neighborhood Aware Temporal Network (NAT)&lt;/h2&gt;
&lt;p&gt;본 연구의 두 가지 특징&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dictionary-type node representation&lt;/p&gt;
&lt;p&gt;구조적인 feature를 효율적으로 만들고, online neighbor sampling이 필요없다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neighborhood caches (N-caches)&lt;/p&gt;
&lt;p&gt;Hashing을 parallel하게 진행해서 dictionary representation을 유지한다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dictionary-representation&#34;&gt;Dictionary representation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;굳이 long-vector representation을 사용하지 않는다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;각 node $u$가 dictionary로 표현된다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keys: 0-hop (self), 1-hop, 2-hop, … 관계에 있는 이웃들을 down-sampling 한 것&lt;/li&gt;
&lt;li&gt;Values: short vector representation (2~8 dim)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled1.png&#34;
	width=&#34;2683&#34;
	height=&#34;803&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled1_hu65974e64d5b0485f6147db68e88da199_223468_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled1_hu65974e64d5b0485f6147db68e88da199_223468_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;334&#34;
		data-flex-basis=&#34;801px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;예시:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled2.png&#34;
	width=&#34;1872&#34;
	height=&#34;880&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled2_huade67fa509dc97d339187804f4125a21_1059953_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled2_huade67fa509dc97d339187804f4125a21_1059953_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;510px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;→ Node $u$와 $v$ 사이의 joint neighborhood 구조를 반영할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled3.png&#34;
	width=&#34;2118&#34;
	height=&#34;1040&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled3_hu66849c16475129caf62098591160e14d_1210153_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled3_hu66849c16475129caf62098591160e14d_1210153_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;488px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;→ NAT는 전통적인 vector representation에서 쓰듯이 그냥 sum 등으로 representation을 aggregate 한다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nat-architecture&#34;&gt;NAT architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Architecture
&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled4.png&#34;
	width=&#34;2492&#34;
	height=&#34;1133&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled4_hu37c1e302144fb2103f270f74417e8020_310888_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled4_hu37c1e302144fb2103f270f74417e8020_310888_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;219&#34;
		data-flex-basis=&#34;527px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Experiments
&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled5.png&#34;
	width=&#34;1800&#34;
	height=&#34;760&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled5_huc6510b21f6b038d5eeef0509f8d82bd4_443546_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled5_huc6510b21f6b038d5eeef0509f8d82bd4_443546_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;236&#34;
		data-flex-basis=&#34;568px&#34;
	
&gt;&lt;br&gt;
Inductive &amp;amp; Transductive (node 기준) 모두 실험.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Performance
&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled6.png&#34;
	width=&#34;1948&#34;
	height=&#34;780&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled6_hu1244281417561286df41d8b587100bd3_1715747_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled6_hu1244281417561286df41d8b587100bd3_1715747_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;249&#34;
		data-flex-basis=&#34;599px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computation &amp;amp; Scalability&lt;br&gt;
&lt;img src=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled7.png&#34;
	width=&#34;1452&#34;
	height=&#34;996&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled7_hua5e9fd5770935b069b9c5fb370894ce1_814749_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/neighborhood-aware-scalable-temporal-network-representation-learning/imgs/Untitled7_hua5e9fd5770935b069b9c5fb370894ce1_814749_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;349px&#34;
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;takeaways&#34;&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;여러 node의 joint neighborhood로부터 얻은 structural feature가 temporal network evolution에 중요하다.&lt;/li&gt;
&lt;li&gt;Dictionary-type representation은 structural feature 구성과 전통적인 vector representation을 모두 반영한다.&lt;/li&gt;
&lt;li&gt;Dictionary-type representation으로 online construction을 효율적으로 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.01084&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Neighborhood-aware Scalable Temporal Network Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/live/wp5S9GHyAgw?feature=share&amp;amp;t=8409&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning on Graphs Conference 2022 - Day 1 Livestream&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>A Generalist Neural Algorithmic Learner</title>
        <link>https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/</link>
        <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/</guid>
        <description>&lt;p&gt;LoG, 22 &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.11142&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Generalist Neural Algorithmic Learner&lt;/a&gt;
&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/thumbnail.png&#34;
	width=&#34;1042&#34;
	height=&#34;528&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/thumbnail_hudd1318377aff45244e3590338f52153c_112022_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/thumbnail_hudd1318377aff45244e3590338f52153c_112022_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;473px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Neural network, 그 중에서도 GNN은 전통적인 컴퓨터과학 알고리즘(CLRS 책에 기재된)을 학습할 수 있다.&lt;/li&gt;
&lt;li&gt;필요한 algorithm이 간단하지 않을 경우, generalist neural algorithmic learner가 필요하다.&lt;/li&gt;
&lt;li&gt;안정적인 multi-task learning에 있어서 chunking mechanism이 중요했다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;starting-point-a-benchmark-to-train-neural-computer-scientists&#34;&gt;Starting point: A benchmark to train neural computer scientists&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.15659&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;The CLRS Algorithmic Reasoning Benchmark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/deepmind/clrs&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/deepmind/clrs&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neural network가 전통적인 CS algorithm을 풀도록 학습시킬 수 있을까?&lt;/li&gt;
&lt;li&gt;이후에는 이 지식을 natural input에 적용해서 real-world problem을 풀 수 있을 것이다.&lt;/li&gt;
&lt;li&gt;그리고, 그 algorithm을 여러 개 학습시킬 수 있을까?&lt;/li&gt;
&lt;li&gt;이런 문제들은 recurrent architecture로 modeling 할 수 있다.
&lt;ul&gt;
&lt;li&gt;LSTM, Transformer, ConvNet, GNN 등.&lt;/li&gt;
&lt;li&gt;저자들은 GNN을 사용.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Benchmark: Introduction to Algorithms: CLRS&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled.png&#34;
	width=&#34;442&#34;
	height=&#34;500&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled_hua63265e8260ecb294a7579e9be70a8cf_138413_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled_hua63265e8260ecb294a7579e9be70a8cf_138413_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;88&#34;
		data-flex-basis=&#34;212px&#34;
	
&gt;
&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled1.png&#34;
	width=&#34;1352&#34;
	height=&#34;2296&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled1_hucd8b39bbcea98e1dc70ae8292aef00b9_466801_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled1_hucd8b39bbcea98e1dc70ae8292aef00b9_466801_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;List of algorithms included in the benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;58&#34;
		data-flex-basis=&#34;141px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;representation&#34;&gt;Representation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;모든 알고리즘은 graph 형태로 표현되었다.&lt;/li&gt;
&lt;li&gt;각 알고리즘은 정해진 숫자만큼의 “probe”로 표현된다.&lt;/li&gt;
&lt;li&gt;예를 들어, insertion sort 알고리즘은 아래 6가지의 probe로 구성된다.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&#39;pos&#39;: (Stage. INPUT, Location.NODE, Type.SCALAR)&lt;/code&gt; → 각 node의 ID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;key&#39;: (Stage.INPUT, Location. NODE, Type.SCALAR)&lt;/code&gt; → 정렬할 value&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;pred&#39;: (Stage.OUTPUT, Location. NODE, Type.POINTER)&lt;/code&gt; → 최종 node 순서&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;pred h&#39;: (Stage. HINT, Location. NODE, Type. POINTER)&lt;/code&gt; → 실행하면서 바뀌는 node 순서&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;i&#39;: (Stage.HINT, Location.NODE, Type.MASK_ONE)&lt;/code&gt; → insertion할 index&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;j&#39;: (Stage.HINT, Location.NODE, Type.MASK_ONE)&lt;/code&gt; → tracking 할 index&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probe는 input, output, hint 중 하나.&lt;/li&gt;
&lt;li&gt;Input과 output은 알고리즘 실행 시 고정, hint는 실행되면서 바뀜.
&lt;ul&gt;
&lt;li&gt;따라서, 모든 sorting algorithm은 input과 output이 같고 hint만 다름.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;representation-encoding&#34;&gt;Representation: Encoding&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled2.png&#34;
	width=&#34;1890&#34;
	height=&#34;978&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled2_hu7e853881e8b757dd1a85a210730154a4_566098_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled2_hu7e853881e8b757dd1a85a210730154a4_566098_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;193&#34;
		data-flex-basis=&#34;463px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;처음 positional ID (node의 ID)는 encoder에 의해 vector 형태로 표현됨.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled3.png&#34;
	width=&#34;1876&#34;
	height=&#34;966&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled3_hu7baf8378b39f5fa240e98e584947107a_603402_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled3_hu7baf8378b39f5fa240e98e584947107a_603402_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;466px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Algorithm의 대상이 되는 value도 encoder에 의해 vector 형태로 표현되고, pos에 더해짐.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled4.png&#34;
	width=&#34;1890&#34;
	height=&#34;974&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled4_hucd1cc34f62c703accc3d07c639180e51_633524_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled4_hucd1cc34f62c703accc3d07c639180e51_633524_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;465px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Hint로 사용되는 pred_h도 encoder에 의해 pointer로 mapping됨.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled5.png&#34;
	width=&#34;1886&#34;
	height=&#34;988&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled5_huaaecf81d46faf81870899897fc493e3b_810218_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled5_huaaecf81d46faf81870899897fc493e3b_810218_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;190&#34;
		data-flex-basis=&#34;458px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Insertion에 필요한 index도 encoder에 의해 바뀌어 더해짐.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled6.png&#34;
	width=&#34;1408&#34;
	height=&#34;872&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled6_hue476ddce194debfa21fdc58a72e3d857_332819_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled6_hue476ddce194debfa21fdc58a72e3d857_332819_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;161&#34;
		data-flex-basis=&#34;387px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled7.png&#34;
	width=&#34;1420&#34;
	height=&#34;832&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled7_hua63bee05ca500a02321ab8fba0e28fac_373127_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled7_hua63bee05ca500a02321ab8fba0e28fac_373127_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled8.png&#34;
	width=&#34;1388&#34;
	height=&#34;838&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled8_hu182e9048a4d33125a9f3cad1eff4208e_440343_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled8_hu182e9048a4d33125a9f3cad1eff4208e_440343_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;representation-decoding&#34;&gt;Representation: Decoding&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled9.png&#34;
	width=&#34;2160&#34;
	height=&#34;1006&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled9_hu1c5ed84ce779c0009f05f3a5c7d8805a_1301394_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled9_hu1c5ed84ce779c0009f05f3a5c7d8805a_1301394_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;515px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Processing step은 algorithm에 따라 바뀌지 않는다! Processing parameter는 공유됨.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled10.png&#34;
	width=&#34;1910&#34;
	height=&#34;1120&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled10_hu780304005a57f8d4da75acf6761a1447_1221193_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled10_hu780304005a57f8d4da75acf6761a1447_1221193_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Train할 때는 hint를 사용하고, test할 때는 hint를 사용하지 않고 input만 주고 output을 맞춘다.&lt;/p&gt;
&lt;p&gt;Train할 때는 output loss 뿐만 아니라 중간 과정인 hint loss에 대해서도 loss를 준다.&lt;/p&gt;
&lt;h2 id=&#34;details&#34;&gt;Details&lt;/h2&gt;
&lt;p&gt;몇 가지 training detail이 실제 학습에 매우 중요했다.&lt;/p&gt;
&lt;p&gt;Node 개수가 16이하인 graph에 대해서 train하고, node 개수가 64인 sample에 대해 test 했다.&lt;/p&gt;
&lt;p&gt;Train, test 시 모두 trajectory의 length도 정해져있다.&lt;/p&gt;
&lt;p&gt;In-distribution score에 따라 early stopping을 적용했다.&lt;/p&gt;
&lt;h2 id=&#34;why-even-care-about-building-a-generalist&#34;&gt;Why even care about building a generalist?&lt;/h2&gt;
&lt;p&gt;왜 generalist algorithmic solver가 필요한 것일까?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled11.png&#34;
	width=&#34;1788&#34;
	height=&#34;920&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled11_hud75bfcc42b7bb40e9cc645bc00eace8d_751629_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled11_hud75bfcc42b7bb40e9cc645bc00eace8d_751629_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;466px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;어떤 input에 대해 output을 맞추는 것은 problem solving 인데, 우리가 보통 어떻게 real-world에서 문제를 해결하고 있는지 살펴보자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Example: Route recommendation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled12.png&#34;
	width=&#34;2190&#34;
	height=&#34;1214&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled12_hu7e6e9af231f964be6a6ec86c54c77193_1572950_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled12_hu7e6e9af231f964be6a6ec86c54c77193_1572950_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Google map에서 최적의 route 추천을 해야하는 상황을 가정해보자. 그냥 raw map 상태로는 문제를 풀 수 없다. Graph 형태로 문제를 추상화한다음, CLRS에서 공부했던 알고리즘을 적용해서, (아마 대부분 Dijkstra’s algorithm을 써서) 최단 거리를 알아내고, 그것을 정답으로 하여 raw map으로 다시 변환할 것이다.&lt;/p&gt;
&lt;p&gt;하지만 real-world에서는 꼭 ‘최단거리’가 최적의 route는 아닐 수 있다. 교통상황, 높이, 등등 다양한 factor가 존재하고, 이럴 경우에는 algorithm이 obvious하지 않다.&lt;/p&gt;
&lt;h3 id=&#34;details-1&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled13.png&#34;
	width=&#34;1136&#34;
	height=&#34;572&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled13_hue16f66050313a9e43df5d697661818d7_432973_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled13_hue16f66050313a9e43df5d697661818d7_432973_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;476px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;저자들은 neural algorithmic reasoning으로 파란 부분의 bottleneck을 해결할 수 있다고 주장한다.&lt;/p&gt;
&lt;p&gt;그리고 잘 학습된 generalist processor가 빨간 bottleneck을 해결할 수 있다고 주장한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이 generalist processor는 중요한 알고리즘의 latent space를 잘 공유한다면, 더 이상 특정 알고리즘을 고르지 않아도 되고, 최적의 정답을 추천해줄 수 있을 것이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;to-get-a-generalist-first-we-need-a-good-specialist&#34;&gt;To get a generalist, first we need a good specialist!&lt;/h2&gt;
&lt;p&gt;그러나 CLRS의 30개의 알고리즘에 대해 모두 다 잘 하는 generalist를 naïve하게 학습하는 것은 어려웠다.&lt;/p&gt;
&lt;p&gt;최근 NE++ (Xhonneux et al., NeurIPS&#39;21) 에서, 이런 알고리즘들은 서로 연관이 높은 것들끼리 (e.g. Prim + Dijkstra) 학습될 때만 잘 학습된다는 얘기가 있었다.&lt;/p&gt;
&lt;p&gt;DeepMind 저자들이 발견한 학습 양상:&lt;/p&gt;
&lt;p&gt;학습이 불안정한 task 들은 다른 task에도 크게 영향을 준다.&lt;/p&gt;
&lt;p&gt;따라서, single-task 단위로 안정성을 먼저 높여 놓아야 한다.&lt;/p&gt;
&lt;h3 id=&#34;bucket-list-of-improvements&#34;&gt;Bucket list of improvements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Teacher forcing 없애기&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training data augmentation (16개 이하의 node size 기준)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Soft hint propagation (hint에 대해 argmax를 취하지 않고, softmax를 취한다)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Static hint elimination (hint가 바뀌지 않으면, input으로 사용한다)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoder initialization (Xavier) + Gradient clipping&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Randomized positional embedding&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sinkhorn operator를 사용한 Permutation decoder&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Processor에 gating mechanism&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Triplet reasoning&lt;/p&gt;
&lt;p&gt;$t_{ijk} = \psi_t (h_i, h_j, h_k, e_{ij}, e_{ik}, e_{kj}, g)$&lt;/p&gt;
&lt;p&gt;$h_{ij} = \phi_t(\max_k t_{ijk})$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled14.png&#34;
	width=&#34;1870&#34;
	height=&#34;888&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled14_huf219c2988f878053e5a1b857a2f9f297_1096341_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled14_huf219c2988f878053e5a1b857a2f9f297_1096341_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;210&#34;
		data-flex-basis=&#34;505px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;final-step-to-the-generalist&#34;&gt;Final step to the generalist&lt;/h2&gt;
&lt;p&gt;Multi-task learning을 위해 &lt;strong&gt;chunking mechanism&lt;/strong&gt;이 중요했다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Trajectory의 length는 16으로 설정한다.&lt;/li&gt;
&lt;li&gt;16보다 짧은 sample들은 padding 되지 않고 그냥 다음 task가 concat 된다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;즉, padding 없이 쭉 연결된 형태이며, 이렇게 구성하면 memory-efficient 하고 학습이 안정화되는 효과가 있었다.&lt;/p&gt;
&lt;p&gt;이렇게 해도 되는 이유는 CLRS-30 task는 Markovian 이기 때문이다.&lt;/p&gt;
&lt;h3 id=&#34;single-generalist-that-matches-the-thirty-specialists&#34;&gt;Single generalist that matches the thirty specialists&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled15.png&#34;
	width=&#34;2232&#34;
	height=&#34;730&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled15_hu1f7b1e05436ecf853aeb420a00a1cb90_1553889_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled15_hu1f7b1e05436ecf853aeb420a00a1cb90_1553889_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;733px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;chunking-helps-significantly&#34;&gt;Chunking helps significantly&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled16.png&#34;
	width=&#34;2196&#34;
	height=&#34;818&#34;
	srcset=&#34;https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled16_hue2c0561d8ac9d7e91097433a7ef8d5ad_959613_480x0_resize_box_3.png 480w, https://gujh14.github.io/ko/p/a-generalist-neural-algorithmic-learner/imgs/Untitled16_hue2c0561d8ac9d7e91097433a7ef8d5ad_959613_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;644px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/live/wp5S9GHyAgw?feature=share&amp;amp;t=10170&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning on Graphs Conference 2022 - Day 1 Livestream&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2209.11142&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Generalist Neural Algorithmic Learner&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
